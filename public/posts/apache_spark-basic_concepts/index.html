<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Apache Spark: Basic Concepts | Nancy's Notebook</title><meta name=keywords content="Spark"><meta name=description content="
Study note of Big Data Essentials: HDFS, MapReduce and Spark RDD
"><meta name=author content="Me"><link rel=canonical href=https://nancyyanyu.github.io/posts/apache_spark-basic_concepts/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Apache Spark: Basic Concepts"><meta property="og:description" content="
Study note of Big Data Essentials: HDFS, MapReduce and Spark RDD
"><meta property="og:type" content="article"><meta property="og:url" content="https://nancyyanyu.github.io/posts/apache_spark-basic_concepts/"><meta property="og:image" content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Apache Spark: Basic Concepts"><meta name=twitter:description content="
Study note of Big Data Essentials: HDFS, MapReduce and Spark RDD
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nancyyanyu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Apache Spark: Basic Concepts","item":"https://nancyyanyu.github.io/posts/apache_spark-basic_concepts/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Apache Spark: Basic Concepts","name":"Apache Spark: Basic Concepts","description":" Study note of Big Data Essentials: HDFS, MapReduce and Spark RDD\n","keywords":["Spark"],"articleBody":" Study note of Big Data Essentials: HDFS, MapReduce and Spark RDD\nApache Spark Apache Spark:a modern distributed fault tolerant computation platform.\nHistory of Apache Spark:\nRDDs RDD(Resilient Distributed Dataset): a core abstraction enabling both an efficient execution for a computation, and a flexible and convenient formalism to define computations.\nResilient — able to withstand failures Distributed — spanning across multiple machines Formally, RDD is a read-only, partitioned collection of records To say that the dataset is an RDD, the dataset must adhere to the RDD interface. The dataset must be:\nable to enumerate its partitions by implementing the partition’s function.\npartitions() -\u003e Array[Partition] The partition is an opaque object for the framework. It is passed back to the iterator function of the RDD, when the framework needs to read the data from the partition.\niterator(p: Partition, parents: Array[Iterator]) -\u003e Iterator able to enumerate its dependencies and provide an array of dependency objects. dependencies() -\u003e Array[Dependency] The dependency object maps partitions of the dataset to the dependencies that are partitions of the parent dataset.\nThose parent partitions are injected into the iterator call when creating a reader.\nTyped. every item in RDD has the same, known type.\ntypedness is an important property to catch bugs early on before the actual execution\ne.g. RDD[strings], or an RDD[integers]. Why do we need a new abstraction? Example: iterative computations (K-means, PageRank, …)\nrelation between consequent steps is known only to the user code not the framework framework has no capabilities to optimize the whole computation framework must reliably persist data between steps thus generating excessive I/O (even if it is temporary data) In this scenario,Spark is trying to keep the data in the memory, effectively eliminates an intermediate disk persistence, and thus improving the completion time.\nExample: joins\njoin operation is used in many MapReduce applications not-so-easy to reuse code Example: a binary file in HDFS Implement the necessary functions to make the binary file in RDD\nTo implement the partition’s function: lookup the blocks for NameNode, create a partition for every block, and return the partitions. To implement the iterator’s function: extract the block information from the partition, and use it to create a reader from HDFS. To implement the dependencies’ function: File reading does not depend on any other RDD, nor on any other partition. So implementing the dependencies function is trivial. It returns an empty array Example: a data* file in HDFS reading records from the file rather than a raw bytes\nExample: an in-memory array The simplest way to make the array in RDD is to pretend that there is a single partition with the whole array. In this case, the partition object keeps a reference to the array, and the iterator function uses this reference to create an iterator.\nExample: a sliced* in-memory array slice the array into chunks to gain parallelism\nThe partition corresponds to the chunk of the source array.\npartitions are handled in parallel.\nSummary RDD is a read-only, partitioned collection of records The collection of the dataset must provide information about its partitions, and provide means to create an iterator over the partition. a developer can access the partitions and create iterators over them RDD tracks dependencies (to be explained in the next video) Examples of RDDs Hadoop files with the proper file format In-memory arrays Transformation Two ways to construct RDDs:\nData in a stable storage Example: files in HDFS, objects in Amazon S3 bucket, lines in a text file, … RDD for data in a stable storage has no dependencies From existing RDDs by applying a transformation Example: filtered file, grouped records, … RDD for a transformed data depends on the source data Transformation:\nAllow you to create new RDDs from the existing RDDs by specifying how to obtain new items from the existing items The transformed RDD depends implicitly on the source RDD Note: Datasets are immutable in Spark, and you cannot modify data in-place.\nExample: map, flatMap map:\nDef: map(f: T -\u003e U): RDD[T] -\u003e RDD[U] returns a mapped RDD with items f(x) for every x in the source RDD flatMap:\nDef: flatMap(f: T -\u003e Array[U]): RDD[T] -\u003e RDD[U] same as map but flattens the result of f generalizes map and filter Example: filter filter:\nDef: filter(p: T -\u003e Boolean): RDD[T] -\u003e RDD[T] returns a filtered RDD with items satisfying the predicate p Filtered RDD:\npartitions: transformed RDD’s mostly the same as source one–\u003ereuse partitions of the source RDD as there is no need to change the partitioning. dependencies: every field of partition depends on the source partition. You can establish this relation by providing a dependency object that establishes one-to-one correspondence between the filtered and the source partitions. iterator: when creating an iterator over the filter partition: Spark would inject an iterator of the source partition into the iterator function called reusing the parent iterator. When requested for the next value, you can pull values from the parent iterator until it returns you an item that satisfies the predicate. Lazy iterator Actual filtering happens not at the creation time of Y, but at the access time to the iterator over a partition of Y.The filtering starts to happen only when you start to pull items from the iterator.\nSame holds for other transformations – they are lazy,i.e. they compute the result only when accessed.\nOn closures (Partition)Dependency graph Whenever you apply a transformation to the RDD, you implicitly construct a dependency graph on the RDDs\nThis graph is used by the framework to schedule jobs Keyed Transformation Grouped RDD partition the key space\nusing the hash partition then to compute the values in the resulting partition\nShuffle: redistribute all the values between all the partitions.\nscan over the entire source RDD to select only the pairs that belong to the output partition.\nDiffernt from flatMap like transformation, a single output partition depends on all the input partitions.\nNarrow \u0026 Wide dependencies Cogroup Transformation The cogroup transformation allows you to compute any kind of a join between two data sets.\nInner join That is all triples (k, x, y) where (k, x) is in X and (k, y) is in Y\n–\u003e apply the `flatMap transformation on top of the result of the cogroup transformation.\nJoins Transformation The join transformation produces the inner join of two data sets.\ninner join: If the key is present only in the one side of the join that is in one data set. Then it is omitted from the result\nouter join: one-sided keys are added to the result with appropriate null values.\nMapReduce in Spark You can express any MapReduce computation in Spark as the flatMap followed by the groupByKey, followed by one more flatMap.\nSummary Transformation\nis a description of how to obtain a new RDD from existing RDDs is the primary way to “modify” data (given that RDDs are immutable) Properties:\nTransformations are lazy, i.e. no work is done until data is explicitly requested There are transformations with narrow and wide dependencies MapReduce can be expressed with a couple of transformations Complex transformations (like joins, cogroup) are available Actions Driver \u0026 executors:\nWhen you write and invoke your Spark application, driver runs within the driver program. Driver program drives the execution of your Spark application Driver delegates tasks to executors to use cluster resources When something must be done with the data, the driver schedules tasks to be executed by executors. In local mode, executors are collocated with the driver In cluster mode, executors are located on cluster machines–\u003e allowing you to use the cluster for a computation Actions:\nTriggers data to be materialized and processed on the executors and then passes the outcome to the driver Actions, together with a transformation code, are executed elsewhere, not in your driver program. Your driver program receives only the outcome.\nSpark does a great job of abstracting away the execution details, and that improves your developer productivity and code readability.\nExample: actions are used to collect, print and save data Frequently used actions collect action collects the result into the memory of the driver program. intended to be used when the output is small enough to fit into the driver’s memory. take action takes the given number of items from a data set and passes them back to the driver. tries to use only the first partition of a data set to optimize the completion time. When the result is large enough, you may want to save it to HDFS for example. Doing so by collecting items in the driver will quickly run out of memory. There are special family of safe actions that do heavy work on the executor side and return a confirmation to the driver.\nSaveAsText file is used for full debugging or for simple applications SaveAsHadoopFile leverages Hadoop file formats to serialize data–\u003ecommon way to save data to HDFS. If you need to run your own code over a data set, there are foreach and foreachPartition actions that invoke your function on the executor side.\nYou can use this function to persist your data in your custom database for example, or to send data over the wire to an external service, or anything else. Summary Actions trigger computation and processing of the dataset Actions are executed on executors and they pass results back to the driver Actions are used to collect, save, print and fold data Resiliency How it is possible to continue operation despite machine failures in the cluster?\nFault-tolerance in MapReduce Two key aspects:\nreliable storage for input and output data once data is stored in HDFS it is safe deterministic and side-effect free execution of mappers and reducers if the computation is deterministic, and has no side effects, the framework can restart it multiple times, and get the same result every time Fault-tolerance in Spark Same two key aspects:\nreliable storage for input and output data deterministic and side-effect free execution of transformations(including closures) Spark assumes that every transformation is deterministic, and free of side-effects, to be able to restart any failed computation Determinism — every invocation of the function results in the same returned value\ne. g. do not use random numbers, do not depend on a hash value order Freedom of side-effects — an invocation of the function does not change anything in the external world\ne. g. do not commit to a database, do not rely on global variables If your function happens to commit to the database, there is no way to roll back changes in case of failure. Spark is allowed to restart the failed parts of the computation. To decide what to restart, Spark keeps track of the lineage. ```\nFault-tolerance \u0026 transformations Lineage — a dependency graph for all partitions of all RDDs involved in a computation up to the data source\nMachine failure renders some partitions in the lineage unavailable. To cope with the failure, you must detect which partitions become unavailable, and decide what to restart.\nDetection is done in the Driver Program, because the Driver Program orchestrates the execution, and already tracks the partitions. Deciding what to restart: Given the failed partition, you look at its dependencies, and if they are alive, restart the computation. If the dependencies are failed as well, you recursively try to recover them. Restarts are slightly more fragile in the case of wide dependencies, because to recompute an output partition, all dependencies must be alive, and there are many of them. If dependencies you evicted out of a cache for example, the restart will be expensive. Fault-tolerance \u0026 actions Actions are side-effects in Spark (communicate with external services)\nActions have to be idempotent幂等（自己重複運算的結果等於它自己的元素） that is safe to be re-executed multiple times given the same input Example: collect()\nall transformations are deterministic, the final data set isn’t changed in case of restarts. Therefore, even if the collect action fails, it could be safely re-executed Example: saveAsTextFile()\nsince the data set is the same, you can safely override the output file Summary Resiliency is implemented by\ntracking lineage assuming deterministic \u0026 side-effect free execution of transformations(including closures) all the closures pass to Spark, must be deterministic and side effect free assuming idempotency for actions These properties cannot be checked or enforced at the compile time, and may lead to obscure bugs in your application that are hard to debug and hard to reproduce.\n","wordCount":"2049","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nancyyanyu.github.io/posts/apache_spark-basic_concepts/"},"publisher":{"@type":"Organization","name":"Nancy's Notebook","logo":{"@type":"ImageObject","url":"https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nancyyanyu.github.io/ accesskey=h title="Nancy's Notebook (Alt + H)"><img src=https://nancyyanyu.github.io/apple-touch-icon.png alt aria-label=logo height=35>Nancy's Notebook</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nancyyanyu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://nancyyanyu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://nancyyanyu.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nancyyanyu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nancyyanyu.github.io/posts/>Posts</a></div><h1 class=post-title>Apache Spark: Basic Concepts</h1><div class=post-meta>10 min&nbsp;·&nbsp;2049 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/Apache_Spark-Basic_Concepts/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><blockquote><p>Study note of <a href=https://www.coursera.org/learn/big-data-essentials>Big Data Essentials: HDFS, MapReduce and Spark RDD</a></p></blockquote><h1 id=apache-spark>Apache Spark<a hidden class=anchor aria-hidden=true href=#apache-spark>#</a></h1><p><strong>Apache Spark</strong>:a modern distributed fault tolerant computation platform.</p><p><strong>History of Apache Spark</strong>:</p><p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/week4_1_huffc2b93cfb6886d9be7a4f237abfd8d7_160985_500x0_resize_box_3.png width=500 height=297><figcaption><small></small></figcaption></figure><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/week4_2_huc1aefe33ed2b50bb43b887ea6773dff1_35166_500x0_resize_box_3.png width=500 height=89><figcaption><small></small></figcaption></figure><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/week4_3_huf2fd6f5ed3c7b7da15f5295bbe12c283_131750_500x0_resize_box_3.png width=500 height=291><figcaption><small></small></figcaption></figure><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/week4_4_hua69e55dd2773c3953d435855d18d2a20_108151_500x0_resize_box_3.png width=500 height=265><figcaption><small></small></figcaption></figure></p><h1 id=rdds>RDDs<a hidden class=anchor aria-hidden=true href=#rdds>#</a></h1><p><strong>RDD(Resilient Distributed Dataset)</strong>: a core abstraction enabling both an <em>efficient execution for a computation</em>, and a <em>flexible and convenient formalism to define computations</em>.</p><ul><li><strong>Resilient</strong> — able to withstand failures</li><li><strong>Distributed</strong> — spanning across multiple machines</li><li>Formally, RDD is a read-only, partitioned collection of records</li><li>To say that the dataset is an RDD, the dataset must adhere to the RDD interface.</li></ul><p><strong>The dataset must be:</strong></p><ul><li><p>able to enumerate its partitions by implementing the partition&rsquo;s function.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>partitions<span class=o>()</span> -&gt; Array<span class=o>[</span>Partition<span class=o>]</span>
</span></span></code></pre></div></li><li><p>The partition is an opaque object for the framework. It is passed back to the iterator function of the RDD, when the framework needs to read the data from the partition.</p></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>iterator<span class=o>(</span>p: Partition, parents: Array<span class=o>[</span>Iterator<span class=o>])</span> -&gt; Iterator
</span></span></code></pre></div><ul><li>able to enumerate its dependencies and provide an array of dependency objects.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>dependencies<span class=o>()</span> -&gt; Array<span class=o>[</span>Dependency<span class=o>]</span>
</span></span></code></pre></div><ul><li><p>The dependency object maps partitions of the dataset to the dependencies that are partitions of the parent dataset.</p></li><li><p>Those parent partitions are injected into the iterator call when creating a reader.</p></li><li><p><strong>Typed.</strong> every item in RDD has the same, known type.</p></li><li><p>typedness is an important property to catch bugs early on before the actual execution</p></li><li><p>e.g. RDD[strings], or an RDD[integers].<figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/rdd1_hu3dff5fe79a6f52ba49930ceabef8f742_120437_500x0_resize_box_3.png width=500 height=156><figcaption><small></small></figcaption></figure></p></li></ul><h2 id=why-do-we-need-a-new-abstraction>Why do we need a new abstraction?<a hidden class=anchor aria-hidden=true href=#why-do-we-need-a-new-abstraction>#</a></h2><p><strong>Example</strong>: iterative computations (K-means, PageRank, …)</p><ul><li>relation between consequent steps is known only to the user code not the framework</li><li>framework has no capabilities to optimize the whole computation</li><li>framework must reliably persist data between steps thus generating excessive I/O (even if it is temporary data)</li></ul><blockquote><p>In this scenario,Spark is trying to keep the data in the memory, effectively eliminates an intermediate disk persistence, and thus improving the completion time.</p></blockquote><p><strong>Example</strong>: joins</p><ul><li>join operation is used in many MapReduce applications</li><li>not-so-easy to reuse code</li></ul><h2 id=example-a-binary-file-in-hdfs>Example: a binary file in HDFS<a hidden class=anchor aria-hidden=true href=#example-a-binary-file-in-hdfs>#</a></h2><blockquote><p>Implement the necessary functions to make the binary file in RDD</p></blockquote><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/rdd2_hu75407efc9485835edc808d22e7bb183c_170374_500x0_resize_box_3.png width=500 height=283><figcaption><small></small></figcaption></figure><ul><li>To implement the <strong>partition</strong>&rsquo;s function: lookup the blocks for NameNode, create a partition for every block, and return the partitions.</li><li>To implement the <strong>iterator</strong>&rsquo;s function: extract the block information from the partition, and use it to create a reader from HDFS.</li><li>To implement the <strong>dependencies</strong>&rsquo; function: File reading does not depend on any other RDD, nor on any other partition. So implementing the dependencies function is trivial. It returns an empty array</li></ul><h2 id=example-a-data-file-in-hdfs>Example: a data* file in HDFS<a hidden class=anchor aria-hidden=true href=#example-a-data-file-in-hdfs>#</a></h2><blockquote><p>reading records from the file rather than a raw bytes</p></blockquote><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/rdd3_hu66ae20dae180eacc0d139216cbecaae2_225273_500x0_resize_box_3.png width=500 height=296><figcaption><small></small></figcaption></figure><h2 id=example-an-in-memory-array>Example: an in-memory array<a hidden class=anchor aria-hidden=true href=#example-an-in-memory-array>#</a></h2><p>The simplest way to make the array in RDD is to pretend that there is a single partition with the whole array. In this case, the partition object keeps a <em><strong>reference</strong></em> to the array, and the iterator function uses this reference to create an iterator.</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/rdd4_hub66e7495a75b4ed31c404c1ab32065b6_152731_500x0_resize_box_3.png width=500 height=255><figcaption><small></small></figcaption></figure><h2 id=example-a-sliced-in-memory-array>Example: a sliced* in-memory array<a hidden class=anchor aria-hidden=true href=#example-a-sliced-in-memory-array>#</a></h2><blockquote><p>slice the array into chunks to gain parallelism</p></blockquote><p>The partition corresponds to the chunk of the source array.</p><p><em>partitions are handled in parallel.</em></p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/rdd5_hucfae71c29b80acdef0782ffbeaacb250_184401_500x0_resize_box_3.png width=500 height=267><figcaption><small></small></figcaption></figure><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><ul><li><strong>RDD is a read-only, partitioned collection of records</strong></li><li>The collection of the dataset must provide information about its partitions, and provide means to create an iterator over the partition.</li><li>a developer can access the partitions and create iterators over them</li><li>RDD tracks dependencies (to be explained in the next video)</li><li>Examples of RDDs</li><li>Hadoop files with the proper file format</li><li>In-memory arrays</li></ul><h1 id=transformation>Transformation<a hidden class=anchor aria-hidden=true href=#transformation>#</a></h1><p><strong>Two ways to construct RDDs:</strong></p><ul><li>Data in a stable storage</li><li>Example: files in HDFS, objects in Amazon S3 bucket, lines in a text file, …</li><li>RDD for data in a stable storage has no dependencies</li><li>From existing RDDs by applying a transformation</li><li>Example: filtered file, grouped records, …</li><li>RDD for a transformed data depends on the source data</li></ul><p><strong>Transformation</strong>:</p><ul><li>Allow you to create new RDDs from the existing RDDs by specifying how
to obtain new items from the existing items</li><li>The transformed RDD depends implicitly on the source RDD</li></ul><p>Note: Datasets are immutable in Spark, and you cannot modify data in-place.</p><h2 id=example-map-flatmap>Example: map, flatMap<a hidden class=anchor aria-hidden=true href=#example-map-flatmap>#</a></h2><p><strong>map</strong>:</p><ul><li>Def: map(f: T -> U): RDD[T] -> RDD[U]</li><li>returns a mapped RDD with items f(x) for every x in the source RDD<figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/trans2_hu1aa1798068220627fad9be650d348340_48948_500x0_resize_box_3.png width=500 height=103><figcaption><small></small></figcaption></figure></li></ul><p><strong>flatMap</strong>:</p><ul><li>Def: flatMap(f: T -> Array[U]): RDD[T] -> RDD[U]</li><li>same as map but flattens the result of <em>f</em></li><li>generalizes map and filter</li></ul><h2 id=example-filter>Example: filter<a hidden class=anchor aria-hidden=true href=#example-filter>#</a></h2><p><strong>filter</strong>:</p><ul><li>Def: filter(p: T -> Boolean): RDD[T] -> RDD[T]</li><li>returns a filtered RDD with items satisfying the predicate <em>p</em><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/trans1_hu6920467350a6de804c20175c3ebe2857_40129_500x0_resize_box_3.png width=500 height=140><figcaption><small></small></figcaption></figure></li></ul><p><strong>Filtered RDD</strong>:</p><ul><li><strong>partitions</strong>: transformed RDD&rsquo;s mostly the same as source one&ndash;><em>reuse partitions of the source RDD</em> as there is no need to change the partitioning.</li><li><strong>dependencies</strong>: every field of partition depends on the source partition. You can establish this relation by providing a dependency object that establishes one-to-one correspondence between the filtered and the source partitions.</li><li><strong>iterator</strong>: when creating an iterator over the filter partition:</li><li>Spark would inject an iterator of the source partition into the iterator function called</li><li>reusing the parent iterator.</li><li>When requested for the next value, you can pull values from the parent iterator until it returns you an item that satisfies the predicate.<figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/trans3_hu8426c79edf133750b2f93f0f5a44a577_237427_500x0_resize_box_3.png width=500 height=267><figcaption><small></small></figcaption></figure></li></ul><h3 id=lazy-iterator>Lazy iterator<a hidden class=anchor aria-hidden=true href=#lazy-iterator>#</a></h3><p>Actual filtering happens not at the creation time of Y, but at the access time to the iterator over a partition of Y.The filtering starts to happen only when you start to pull items from the iterator.</p><p>Same holds for other transformations – they are lazy,i.e. they compute the result only when accessed.</p><h3 id=on-closures>On closures<a hidden class=anchor aria-hidden=true href=#on-closures>#</a></h3><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/trans4_huc5b3388f430da8ca3112506f438d1ba1_113957_500x0_resize_box_3.png width=500 height=86><figcaption><small></small></figcaption></figure><h3 id=partitiondependency-graph>(Partition)Dependency graph<a hidden class=anchor aria-hidden=true href=#partitiondependency-graph>#</a></h3><p>Whenever you apply a transformation to the RDD, you implicitly construct a dependency graph on the RDDs</p><ul><li>This graph is used by the framework to schedule jobs</li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/trans5_huf79c993e3b87abb247fb2298e6b4b61f_125118_500x0_resize_box_3.png width=500 height=240><figcaption><small></small></figcaption></figure><h2 id=keyed-transformation>Keyed Transformation<a hidden class=anchor aria-hidden=true href=#keyed-transformation>#</a></h2><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/trans6_hu2c49ab8e41132e70ea4368a948ecbd97_232903_500x0_resize_box_3.png width=500 height=278><figcaption><small></small></figcaption></figure><h3 id=grouped-rdd>Grouped RDD<a hidden class=anchor aria-hidden=true href=#grouped-rdd>#</a></h3><ul><li><p>partition the key space</p></li><li><p>using the hash partition then to compute the values in the resulting partition</p></li><li><p><strong>Shuffle</strong>: redistribute all the values between all the partitions.</p></li><li><p>scan over the entire source RDD to select only the pairs that belong to the output partition.</p></li></ul><blockquote><p>Differnt from flatMap like transformation, a single output partition depends on all the input partitions.</p></blockquote><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/trans9_hu2d4daecabfb7618424d0c5a29eb37dc7_271644_500x0_resize_box_3.png width=500 height=196><figcaption><small></small></figcaption></figure><h4 id=narrow--wide-dependencies>Narrow & Wide dependencies<a hidden class=anchor aria-hidden=true href=#narrow--wide-dependencies>#</a></h4><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/trans10_hu257564f8957003dcefda09e4c05e1f3c_249456_500x0_resize_box_3.png width=500 height=270><figcaption><small></small></figcaption></figure><h2 id=cogroup-transformation>Cogroup Transformation<a hidden class=anchor aria-hidden=true href=#cogroup-transformation>#</a></h2><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/trans7_hu8df7ba8145b766cec4a20fa0f0da4d35_196063_500x0_resize_box_3.png width=500 height=255><figcaption><small></small></figcaption></figure><p>The cogroup transformation allows you to compute any kind of a join between two data sets.</p><h3 id=inner-join>Inner join<a hidden class=anchor aria-hidden=true href=#inner-join>#</a></h3><p>That is all triples (k, x, y) where (k, x) is in X and (k, y) is in Y</p><p>&ndash;> apply the <strong>`flatMap</strong> transformation on top of the result of the cogroup transformation.</p><h2 id=joins-transformation>Joins Transformation<a hidden class=anchor aria-hidden=true href=#joins-transformation>#</a></h2><p>The join transformation produces the inner join of two data sets.</p><p><strong>inner join:</strong> If the key is present only in the one side of the join that is in one data set. Then it is omitted from the result</p><p><strong>outer join:</strong> one-sided keys are added to the result with appropriate null values.</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/trans8_hu19dc23bdda9ed0a72bd8116b4c7a708d_162809_500x0_resize_box_3.png width=500 height=256><figcaption><small></small></figcaption></figure><h2 id=mapreduce-in-spark>MapReduce in Spark<a hidden class=anchor aria-hidden=true href=#mapreduce-in-spark>#</a></h2><p>You can express any MapReduce computation in Spark as the <strong>flatMap</strong> followed by the <strong>groupByKey</strong>, followed by one more <strong>flatMap</strong>.</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/trans11_hub01310bbd0fc1ac4f0a6b82479c82227_182078_500x0_resize_box_3.png width=500 height=278><figcaption><small></small></figcaption></figure><h2 id=summary-1>Summary<a hidden class=anchor aria-hidden=true href=#summary-1>#</a></h2><p>Transformation</p><ul><li>is a description of how to obtain a new RDD from existing RDDs</li><li>is the primary way to “modify” data (given that RDDs are immutable)</li></ul><p>Properties:</p><ul><li>Transformations are lazy, i.e. no work is done until data is explicitly requested</li><li>There are transformations with narrow and wide dependencies</li><li>MapReduce can be expressed with a couple of transformations</li><li>Complex transformations (like joins, cogroup) are available</li></ul><h1 id=actions>Actions<a hidden class=anchor aria-hidden=true href=#actions>#</a></h1><p><strong>Driver & executors</strong>:</p><ul><li>When you write and invoke your Spark application, driver <em><strong>runs within the driver program.</strong></em></li><li>Driver program drives the execution of your Spark application</li><li>Driver delegates tasks to executors to use cluster resources</li><li>When something must be done with the data, the driver schedules tasks to be executed by executors.</li><li>In <em>local</em> mode, executors are collocated with the driver</li><li>In <em>cluster</em> mode, executors are located on cluster machines&ndash;> allowing you to use the cluster for a computation</li></ul><p><strong>Actions</strong>:</p><ul><li>Triggers data to be materialized and processed on the executors and
then passes the outcome to the driver</li></ul><blockquote><p>Actions, together with a transformation code, are executed elsewhere, not in your driver program. Your driver program receives only the outcome.</p></blockquote><p>Spark does a great job of abstracting away the execution details, and that improves your developer productivity and code readability.</p><ul><li><strong>Example</strong>: actions are used to collect, print and save data</li></ul><h2 id=frequently-used-actions>Frequently used actions<a hidden class=anchor aria-hidden=true href=#frequently-used-actions>#</a></h2><ul><li><strong>collect</strong> action collects the result into the memory of the driver program.</li><li>intended to be used when the output is small enough to fit into the driver&rsquo;s memory.</li><li><strong>take</strong> action takes the given number of items from a data set and passes them back to the driver.</li><li>tries to use only the first partition of a data set to optimize the completion time.<figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/action1_hudf5ebd6101daec6efb6b93ee24cca1d0_236557_500x0_resize_box_3.png width=500 height=281><figcaption><small></small></figcaption></figure>When the result is large enough, you may want to save it to HDFS for example. Doing so by collecting items in the driver will quickly run out of memory.</li></ul><p>There are special family of safe actions that do heavy work on the executor side and return a confirmation to the driver.</p><ul><li>SaveAsText file is used for full debugging or for simple applications</li><li>SaveAsHadoopFile leverages Hadoop file formats to serialize data&ndash;>common way to save data to HDFS.<figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/action2_hu3d5089b55311f2b9fa17683adc6351dd_171254_500x0_resize_box_3.png width=500 height=140><figcaption><small></small></figcaption></figure></li></ul><p>If you need to run your own code over a data set, there are <em>foreach</em> and <em>foreachPartition</em> actions that invoke your function on the executor side.</p><p>You can use this function to persist your data in your custom database for example, or to send data over the wire to an external service, or anything else.<figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/action3_huf152b8e37f35d767444d2cf0b144deae_96259_500x0_resize_box_3.png width=500 height=104><figcaption><small></small></figcaption></figure></p><h2 id=summary-2>Summary<a hidden class=anchor aria-hidden=true href=#summary-2>#</a></h2><ul><li>Actions trigger computation and processing of the dataset</li><li>Actions are executed on executors and they pass results back to the driver</li><li>Actions are used to collect, save, print and fold data</li></ul><h1 id=resiliency>Resiliency<a hidden class=anchor aria-hidden=true href=#resiliency>#</a></h1><p>How it is possible to continue operation despite machine failures in the cluster?</p><h2 id=fault-tolerance-in-mapreduce>Fault-tolerance in MapReduce<a hidden class=anchor aria-hidden=true href=#fault-tolerance-in-mapreduce>#</a></h2><p>Two key aspects:</p><ul><li>reliable storage for input and output data</li><li>once data is stored in HDFS it is safe</li><li>deterministic and side-effect free execution of mappers and reducers</li><li>if the computation is deterministic, and has no side effects, the framework can restart it multiple times, and get the same result every time<figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/resi1_hu6d8b1a8de64b9d8e2d180c14b3998d53_118884_500x0_resize_box_3.png width=500 height=217><figcaption><small></small></figcaption></figure></li></ul><h2 id=fault-tolerance-in-spark>Fault-tolerance in Spark<a hidden class=anchor aria-hidden=true href=#fault-tolerance-in-spark>#</a></h2><p>Same two key aspects:</p><ul><li>reliable storage for input and output data</li><li>deterministic and side-effect free execution of
transformations(including closures)</li><li>Spark assumes that every transformation is deterministic, and free of side-effects, to be able to restart any failed computation</li></ul><p><strong>Determinism</strong> — every invocation of the function results in the same returned value</p><ul><li>e. g. do not use random numbers, do not depend on a hash value order</li></ul><p><strong>Freedom of side-effects</strong> — an invocation of the function does not change anything in the external world</p><ul><li>e. g. do not commit to a database, do not rely on global variables</li><li>If your function happens to commit to the database, there is no way to roll back changes in case of failure.</li></ul><p>Spark is allowed to restart the failed parts of the computation. To decide what to restart, Spark keeps track of the lineage. ```</p><h2 id=fault-tolerance--transformations>Fault-tolerance & transformations<a hidden class=anchor aria-hidden=true href=#fault-tolerance--transformations>#</a></h2><p><strong>Lineage</strong> — a dependency graph for all partitions of all RDDs involved in a
computation up to the data source</p><p>Machine failure renders some partitions in the lineage unavailable. To cope with the failure, you must detect <em>which partitions become unavailable</em>, and decide <em>what to restart</em>.</p><ul><li>Detection is done in the Driver Program, because the Driver Program orchestrates the execution, and already tracks the partitions.</li><li><strong>Deciding what to restart</strong>: Given the failed partition, you look at its dependencies, and if they are alive, restart the computation. If the dependencies are failed as well, you recursively try to recover them.</li><li><strong>Restarts</strong> are slightly more fragile in the case of wide dependencies, because to recompute an output partition, all dependencies must be alive, and there are many of them. If dependencies you evicted out of a cache for example, the restart will be expensive.</li></ul><p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/resi2_hu6f262b989a799dc814f5acfe31f8fcd0_121627_500x0_resize_box_3.png width=500 height=207><figcaption><small></small></figcaption></figure><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-basic_concepts/resi3_huc20dbbe880e0f92960a988cf86036740_119760_500x0_resize_box_3.png width=500 height=217><figcaption><small></small></figcaption></figure></p><h2 id=fault-tolerance--actions>Fault-tolerance & actions<a hidden class=anchor aria-hidden=true href=#fault-tolerance--actions>#</a></h2><p>Actions are side-effects in Spark (communicate with external services)</p><ul><li>Actions have to be <strong>idempotent</strong>幂等（自己重複運算的結果等於它自己的元素） that is safe to be re-executed multiple times given the same input</li></ul><p>Example: <strong>collect()</strong></p><ul><li>all transformations are deterministic, the final data set isn&rsquo;t changed in case of restarts. Therefore, even if the collect action fails, it could be safely re-executed</li></ul><p>Example: <strong>saveAsTextFile()</strong></p><ul><li>since the data set is the same, you can safely override the output file</li></ul><h2 id=summary-3>Summary<a hidden class=anchor aria-hidden=true href=#summary-3>#</a></h2><p>Resiliency is implemented by</p><ul><li>tracking lineage</li><li>assuming deterministic & side-effect free execution of transformations(including closures)</li><li>all the closures pass to Spark, must be deterministic and side effect free</li><li>assuming idempotency for actions</li></ul><p>These properties cannot be checked or enforced at the compile time, and may lead to obscure bugs in your application that are hard to debug and hard to reproduce.</p><pre tabindex=0><code></code></pre></div><footer class=post-footer><ul class=post-tags><li><a href=https://nancyyanyu.github.io/tags/spark/>Spark</a></li></ul><nav class=paginav><a class=prev href=https://nancyyanyu.github.io/posts/apache_spark-advanced_topics/><span class=title>« Prev</span><br><span>Apache Spark: Advanced Topics</span></a>
<a class=next href=https://nancyyanyu.github.io/posts/ml-linear_regression_models/><span class=title>Next »</span><br><span>Study Note: Linear Regression Part I - Linear Regression Models</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark: Basic Concepts on twitter" href="https://twitter.com/intent/tweet/?text=Apache%20Spark%3a%20Basic%20Concepts&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-basic_concepts%2f&amp;hashtags=Spark"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark: Basic Concepts on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-basic_concepts%2f&amp;title=Apache%20Spark%3a%20Basic%20Concepts&amp;summary=Apache%20Spark%3a%20Basic%20Concepts&amp;source=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-basic_concepts%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark: Basic Concepts on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-basic_concepts%2f&title=Apache%20Spark%3a%20Basic%20Concepts"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark: Basic Concepts on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-basic_concepts%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark: Basic Concepts on whatsapp" href="https://api.whatsapp.com/send?text=Apache%20Spark%3a%20Basic%20Concepts%20-%20https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-basic_concepts%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark: Basic Concepts on telegram" href="https://telegram.me/share/url?text=Apache%20Spark%3a%20Basic%20Concepts&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-basic_concepts%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark: Basic Concepts on ycombinator" href="https://news.ycombinator.com/submitlink?t=Apache%20Spark%3a%20Basic%20Concepts&u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-basic_concepts%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://nancyyanyu.github.io/>Nancy's Notebook</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>