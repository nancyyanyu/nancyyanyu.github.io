<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Study Note: Linear Discriminant Analysis, ROC & AUC, Confusion Matrix | Nancy's Notebook</title><meta name=keywords content="LDA,Classification,Model Evaluation"><meta name=description content="LDA V.S. Logistic Regression:

When the classes are well-separated, the parameter estimates for the
logistic regression model are surprisingly unstable. Linear discriminant
analysis does not suffer from this problem.
If n is small and the distribution of the predictors X is approximately
normal in each of the classes, the linear discriminant model is again
more stable than the logistic regression model.
Linear discriminant analysis is popular
when we have more than two response classes.
"><meta name=author content="Me"><link rel=canonical href=https://nancyyanyu.github.io/posts/ml-linear_discriminant_analysis/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Study Note: Linear Discriminant Analysis, ROC & AUC, Confusion Matrix"><meta property="og:description" content="LDA V.S. Logistic Regression:

When the classes are well-separated, the parameter estimates for the
logistic regression model are surprisingly unstable. Linear discriminant
analysis does not suffer from this problem.
If n is small and the distribution of the predictors X is approximately
normal in each of the classes, the linear discriminant model is again
more stable than the logistic regression model.
Linear discriminant analysis is popular
when we have more than two response classes.
"><meta property="og:type" content="article"><meta property="og:url" content="https://nancyyanyu.github.io/posts/ml-linear_discriminant_analysis/"><meta property="og:image" content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-06-09T14:49:45+00:00"><meta property="article:modified_time" content="2019-06-09T14:49:45+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Study Note: Linear Discriminant Analysis, ROC & AUC, Confusion Matrix"><meta name=twitter:description content="LDA V.S. Logistic Regression:

When the classes are well-separated, the parameter estimates for the
logistic regression model are surprisingly unstable. Linear discriminant
analysis does not suffer from this problem.
If n is small and the distribution of the predictors X is approximately
normal in each of the classes, the linear discriminant model is again
more stable than the logistic regression model.
Linear discriminant analysis is popular
when we have more than two response classes.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nancyyanyu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Study Note: Linear Discriminant Analysis, ROC \u0026 AUC, Confusion Matrix","item":"https://nancyyanyu.github.io/posts/ml-linear_discriminant_analysis/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Study Note: Linear Discriminant Analysis, ROC \u0026 AUC, Confusion Matrix","name":"Study Note: Linear Discriminant Analysis, ROC \u0026 AUC, Confusion Matrix","description":"LDA V.S. Logistic Regression:\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem. If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model. Linear discriminant analysis is popular when we have more than two response classes. ","keywords":["LDA","Classification","Model Evaluation"],"articleBody":"LDA V.S. Logistic Regression:\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem. If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model. Linear discriminant analysis is popular when we have more than two response classes. Using Bayes’ Theorem for Classification Suppose that we wish to classify an observation into one of K classes, where K ≥ 2.\nPrior:Let $\\pi_k=Pr(Y=k)$ represent the overall or prior probability that a randomly chosen observation comes from the kth class. This is the probability that a given observation is associated with the kth category of the response variable Y .\nLet $f_k(X) ≡ Pr(X = x|Y = k)$ denote the density function of X for an observation that comes from the kth class. In other words, fk(x) is relatively large if there is a high probability that an observation in the kth class has X ≈ x.\nBayes’ theorem states that $$ \\begin{align} Pr(Y=k|X=x)=\\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)} \\end{align} $$ Posterior:$p_k(X) = Pr(Y = k|X)$ an observation X = x belongs to the kth class, given the predictor value for that observation\nEstimating $π_k$: simply compute the fraction of the training observations that belong to the kth class.\nEstimating $f_k(X)$: more challenging\nLinear Discriminant Analysis for p = 1 Assume p = 1—that is, we have only one predictor. We would like to obtain an estimate for $f_k(x)$ that we can estimate $p_k(x)$. We will then classify an observation to the class for which $p_k(x)$ is greatest.\nAssumptions In order to estimate $f_k(x)$, we will first make some assumptions about its form:\nAssume that $f_k(x)$ is normal or Gaussian. $$ \\begin{align} f_k(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp{\\left( -\\frac{1}{2\\sigma_k^2}(x-\\mu_k)^2 \\right)} \\end{align} $$ where $μ_k$ and $σ_k^2$ are the mean and variance parameters for the kth class.\nAssume that $\\sigma_1^2=…=\\sigma_k^2$ : that is, there is a shared variance term across all K classes, which for simplicity we can denote by $\\sigma^2$. So $$ \\begin{align} p_k(x)=\\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left( -\\frac{1}{2\\sigma^2}(x-\\mu_k)^2 \\right)}}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left( -\\frac{1}{2\\sigma^2}(x-\\mu_l)^2 \\right)}} \\end{align} $$ The Bayes classifier involves assigning an observation X = x to the class for which $p_k(x)$ is largest. Taking the log of $p_k(x)$ and rearranging the terms, it is not hard to show that this is equivalent to assigning the observation to the class for which $$ \\begin{align} \\delta_k(x)=x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\log(\\pi_k) \\quad\\quad (4.13) \\end{align} $$ is largest.\nFor instance, if K = 2 and π1 = π2, then the Bayes classifier assigns an observation to class 1 if $2x (μ_1 − μ_2) \u003e μ^2_1 − μ^2_2$, and to class 2 otherwise. In this case, the Bayes decision boundary corresponds to the point where $$ \\begin{align} x=\\frac{\\mu_1^2-\\mu_2^2}{2(\\mu_1-\\mu_2)}=\\frac{\\mu_1+\\mu_2}{2} \\end{align} $$\nParameters Estimation In practice, even if we are quite certain of our assumption that X is drawn from a Gaussian distribution within each class, we still have to estimate the parameters $μ_1, . . . , μ_K, π_1, . . . , π_K$, and $σ^2$.\nLinear discriminant analysis (LDA) method approximates the Bayes classifier by plugging estimates for $μ_1, . . . , μ_K, π_1, . . . , π_K$, and $σ^2$ into (4.13)\n$$ \\begin{align} \\hat{\\mu}k=\\frac{1}{n_k}\\sum{i:y_i=k}x_i \\quad (4.15) \\ \\hat{\\sigma}^2=\\frac{1}{n-K}\\sum_{k=1}^K\\sum_{i:y_i=k}(x_i-\\hat{\\mu_k})^2 \\quad (4.16)\\ \\hat{\\pi_k}=\\frac{n_k}{n} \\end{align} $$\nwhere n is the total number of training observations, and $n_k$ is the number of training observations in the kth class.\n$\\hat{\\mu}_k$: average of all the training observations from the kth class;\n$\\hat{\\sigma}^2$: a weighted average of the sample variances for each of the K classes.\n$\\hat{\\pi_k}$: the proportion of the training observations that belong to the kth class\nLDA classifier The LDA classifier assigns an observation X = x to the class for which\n$$ \\begin{align} \\hat{\\delta}_k(x)=x\\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2}+\\log(\\hat{\\pi}_k) \\end{align} $$ is largest.\nThe word linear in the classifier’s name stems from the fact that the discriminant functions $\\hat{\\delta}_k(x)$ are linear functions of x.\nThe right-hand panel of Figure 4.4 displays a histogram of a random sample of 20 observations from each class.\nTo implement LDA,\nEstimating πk, μk, and σ2 using (4.15) and (4.16). Compute the decision boundary, shown as a black solid line, that results from assigning an observation to the class for which $\\hat{\\delta}_k(x)$ is largest. In this case, since n1 = n2 = 20, we have $\\hat{\\pi_1}$ = $\\hat{\\pi_2}$. As a result, the decision boundary corresponds to the midpoint between the sample means for the two classes,$\\frac{\\mu_1+\\mu_2}{2}$\nLinear Discriminant Analysis for p \u003e1 Assume that X = (X1,X2, . . .,Xp) is drawn from a multivariate Gaussian (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix.\nMultivariate Gaussian Distribution Assumes that each individual predictor follows a one-dimensional normal distribution with some correlation between each pair of predictors.\nTo indicate that a p-dimensional random variable X has a multivariate Gaussian distribution, we write X ∼ N(μ,Σ). Here E(X) = μ is the mean of X (a vector with p components), and Cov(X) = Σ is the p × p covariance matrix of X. Formally, the multivariate Gaussian density is defined as $$ \\begin{align} f(x)=\\frac{1}{\\sqrt{(2\\pi)^{p}|Σ|}}\\exp{\\left( \\frac{1}{2}(x-\\mu)^TΣ^{-1}(x-\\mu) \\right)} \\end{align} $$ In the case of p \u003e 1 predictors, the LDA classifier assumes that the observations in the kth class are drawn from a multivariate Gaussian distribution $N(μ_k,Σ)$, where $μ_k$ is a class-specific mean vector, and Σ is a covariance matrix that is common to all K classes.\nPlugging the density function for the kth class, $f_k(X = x)$, into $Pr(Y = k|X = x)$, the Bayes classifier assigns an observation X = x to the class for which $$ \\begin{align} \\delta_k(x)=x^TΣ^{-1}\\mu_k-\\frac{1}{2}\\mu_k^TΣ^{-1}\\mu_k+\\log{\\pi_k} \\quad \\quad (4.19) \\end{align} $$ is largest.\nOnce again, we need to estimate the unknown parameters $μ_1, . . . , μ_K$, $π_1, . . . , π_K$, and Σ; the formulas are similar to those used in the one dimensional case, given in (4.15). To assign a new observation X = x, LDA plugs these estimates into (4.19) and classifies to the class for which $\\hat{\\delta}_k(x)$ is largest.\nOverall, the LDA decision boundaries are pretty close to the Bayes decision boundaries, shown again as dashed lines.\nCaveats Training error rates will usually be lower than test error rates. The higher the ratio of parameters p to number of samples n, the more we expect this overfitting to play a role.\nSecond, since only 3.33% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that each individual will not default, regardless of his or her credit card balance and student status, will result in an error rate of 3.33%. In other words, the trivial null classifier will achieve an error rate that is only a bit higher than the LDA training set error rate.\nTwo Types of Error, Confusion Matrix Binary classifier can make two types of errors:\nit can incorrectly assign an individual who defaults to the no default category; it can incorrectly assign an individual who does not default to the default category. Confusion Matrix\n*注意这张图不是标准的confusion matrix，看下面那张 Explanation： The matrix table reveals that LDA predicted that a total of 104 people would default. Of these people, 81 actually defaulted and 23 did not.\nType I Error： Of the 333 individuals who defaulted, 252 (or 75.7%) were missed by LDA. So while the overall error rate is low, the error rate among individuals who defaulted is very high. From the perspective of a credit card company that is trying to identify high-risk individuals, an error rate of 252/333 = 75.7% among individuals who default may well be unacceptable.\nType II Error：Only 23 out of 9, 667 of the individuals who did not default were incorrectly labeled. This looks like a pretty low error rate!\nSensitivity:the percentage of true defaulters that are identified, a low 24.3% in this case.\nSpecificity:the percentage of non-defaulters that are correctly identified, here (1 − 23/9, 667)× 100 = 99.8%.\nWhy does LDA do such a poor job of classifying the customers who default? In other words, why does it have such a low sensitivity?\nLDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers (if the Gaussian model is correct). That is, the Bayes classifier will yield the smallest possible total number of misclassified observations, irrespective of which class the errors come from.\nThe Bayes classifier works by assigning an observation to the class for which the posterior probability pk(X) is greatest. In the two-class case, this amounts to assigning an observation to the default class if $Pr(default = Yes|X = x) \u003e 0.5.$\nThus, the Bayes classifier, and by extension LDA, uses a threshold of 50% for the posterior probability of default in order to assign an observation to the default class.\nModify LDA\nIf we are concerned about incorrectly predicting the default status for individuals who default, then we can consider lowering this threshold.\n$$P(default = Yes|X = x) \u003e 0.2$$\nFigure 4.7 illustrates the trade-off that results from modifying the threshold value for the posterior probability of default\nHow can we decide which threshold value is best? Such a decision must be based on domain knowledge, such as detailed information about the costs associated with default.\nROC \u0026 AUC ROC:The ROC curve is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds.\nAUC: The overall performance of a classifier, summarized over all possible thresholds, is given by the area under the (ROC) curve (AUC).\nAn ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. We expect a classifier that performs no better than chance to have an AUC of 0.5\nROC curves are useful for comparing different classifiers, since they take into account all possible thresholds.\nQuadratic Discriminant Analysis Quadratic discriminant analysis (QDA) classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction.\nHowever, unlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the kth class is of the form $X ∼ N(μ_k,Σ_k)$, where $Σ_k$ is a covariance matrix for the kth class. Under this assumption, the Bayes classifier assigns an observation $X = x$ to the class for which\n$$ \\begin{align} \\delta_k(x)\u0026=-\\frac{1}{2}(x-\\mu_k)^TΣ_k^{-1}(x-\\mu_k)-\\frac{1}{2}\\log{|Σ_k|}+\\log{\\pi_k} \\\\ \u0026=-\\frac{1}{2}x^TΣ_k^{-1}x+x^TΣ_k^{-1}\\mu_k-\\frac{1}{2}\\mu_k^TΣ_k^{-1}\\mu_k-\\frac{1}{2}\\log{|Σ_k|}+\\log{\\pi_k} \\end{align} $$\nis largest.\nSo the QDA classifier involves plugging estimates for $Σ_k, μ_k, π_k$ into $\\delta_k(x)$, and then assigning an observation $X = x$ to the class for which this quantity is largest. Unlike LDA, the quantity $x$ appears as a quadratic function.\nWhy does it matter whether or not we assume that the K classes share a common covariance matrix? The answer lies in the bias-variance trade-off:\nWhen there are p predictors, then estimating a covariance matrix requires estimating p(p+1)/2 parameters. QDA estimates a separate covariance matrix for each class, for a total of Kp(p+1)/2 parameters. Consequently, LDA is a much less flexible classifier than QDA, and so has substantially lower variance. But there is a trade-off: if LDA’s assumption that the K classes share a common covariance matrix is badly off, then LDA can suffer from high bias. Conclusion\nLDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the K classes is clearly untenable Ref:\nJames, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.\nHastie, Trevor, et al. “The elements of statistical learning: data mining, inference and prediction.” The Mathematical Intelligencer 27.2 (2005): 83-85\n","wordCount":"1968","inLanguage":"en","datePublished":"2019-06-09T14:49:45Z","dateModified":"2019-06-09T14:49:45Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nancyyanyu.github.io/posts/ml-linear_discriminant_analysis/"},"publisher":{"@type":"Organization","name":"Nancy's Notebook","logo":{"@type":"ImageObject","url":"https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nancyyanyu.github.io/ accesskey=h title="Nancy's Notebook (Alt + H)"><img src=https://nancyyanyu.github.io/apple-touch-icon.png alt aria-label=logo height=35>Nancy's Notebook</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nancyyanyu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://nancyyanyu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://nancyyanyu.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nancyyanyu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nancyyanyu.github.io/posts/>Posts</a></div><h1 class=post-title>Study Note: Linear Discriminant Analysis, ROC & AUC, Confusion Matrix</h1><div class=post-meta><span title='2019-06-09 14:49:45 +0000 UTC'>June 9, 2019</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;1968 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/ML-Linear_Discriminant_Analysis/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p><strong>LDA V.S. Logistic Regression</strong>:</p><ol><li>When the classes are well-separated, the parameter estimates for the
logistic regression model are surprisingly unstable. Linear discriminant
analysis does not suffer from this problem.</li><li>If n is small and the distribution of the predictors X is approximately
normal in each of the classes, the linear discriminant model is again
more stable than the logistic regression model.</li><li>Linear discriminant analysis is popular
when we have more than two response classes.</li></ol><h1 id=using-bayes-theorem-for-classification>Using Bayes’ Theorem for Classification<a hidden class=anchor aria-hidden=true href=#using-bayes-theorem-for-classification>#</a></h1><p>Suppose that we wish to classify an observation into one of K classes, where
K ≥ 2.</p><p><strong>Prior</strong>:Let $\pi_k=Pr(Y=k)$ represent the overall or <em><strong>prior</strong></em>
probability that a randomly chosen observation comes from the kth class. This is the probability that a given observation is associated with the kth
category of the response variable Y .</p><p>Let $f_k(X) ≡ Pr(X = x|Y = k)$ denote
the <em><strong>density function</strong></em> of X for an observation that comes from the kth class. In other words, fk(x) is relatively large if there is a high probability that an observation in the kth class has X ≈ x.</p><p><strong>Bayes’ theorem</strong> states that
$$
\begin{align}
Pr(Y=k|X=x)=\frac{\pi_k f_k(x)}{\sum_{l=1}^K\pi_lf_l(x)}
\end{align}
$$
<strong>Posterior</strong>:$p_k(X)
= Pr(Y = k|X)$ an observation X = x belongs to the kth class, given the predictor value for that
observation</p><p><strong>Estimating $π_k$:</strong> simply compute the fraction of the training
observations that belong to the kth class.</p><p><strong>Estimating $f_k(X)$:</strong> more challenging</p><h1 id=linear-discriminant-analysis-for-p--1>Linear Discriminant Analysis for p = 1<a hidden class=anchor aria-hidden=true href=#linear-discriminant-analysis-for-p--1>#</a></h1><p>Assume p = 1—that is, we have only one predictor. We
would like to obtain an estimate for $f_k(x)$ that we can estimate $p_k(x)$. We will then classify an observation to the class
for which $p_k(x)$ is greatest.</p><h2 id=assumptions>Assumptions<a hidden class=anchor aria-hidden=true href=#assumptions>#</a></h2><p>In order to estimate $f_k(x)$, we will first make
some assumptions about its form:</p><ol><li>Assume that $f_k(x)$ is normal or Gaussian.
$$
\begin{align}
f_k(x)=\frac{1}{\sqrt{2\pi}\sigma_k}\exp{\left( -\frac{1}{2\sigma_k^2}(x-\mu_k)^2 \right)}
\end{align}
$$</li></ol><p>where $μ_k$ and $σ_k^2$ are the mean and variance parameters for the kth class.</p><ol start=2><li>Assume that $\sigma_1^2=&mldr;=\sigma_k^2$
: that is, there is a shared
variance term across all K classes, which for simplicity we can denote by
$\sigma^2$.</li></ol><p>So
$$
\begin{align}
p_k(x)=\frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_k)^2 \right)}}{\sum_{l=1}^K\pi_l\frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_l)^2 \right)}}
\end{align}
$$
The <strong>Bayes classifier</strong> involves assigning an observation X = x to the class for which $p_k(x)$ is largest. Taking the log of $p_k(x)$
and rearranging the terms, it is not hard to show that this is equivalent to
assigning the observation to the class for which
$$
\begin{align}
\delta_k(x)=x\frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log(\pi_k) \quad\quad (4.13)
\end{align}
$$
is largest.</p><p>For instance, if K = 2 and π1 = π2, then the Bayes classifier
assigns an observation to class 1 if $2x (μ_1 − μ_2) > μ^2_1
− μ^2_2$, and to class
2 otherwise. In this case, the Bayes decision boundary corresponds to the
point where
$$
\begin{align}
x=\frac{\mu_1^2-\mu_2^2}{2(\mu_1-\mu_2)}=\frac{\mu_1+\mu_2}{2}
\end{align}
$$</p><h2 id=parameters-estimation>Parameters Estimation<a hidden class=anchor aria-hidden=true href=#parameters-estimation>#</a></h2><p>In practice, even if we are quite certain of our assumption that X is drawn
from a Gaussian distribution within each class, we still have to estimate
the parameters $μ_1, . . . , μ_K, π_1, . . . , π_K$, and $σ^2$.</p><p><strong>Linear discriminant analysis (LDA)</strong> method approximates the Bayes classifier by plugging estimates for $μ_1, . . . , μ_K, π_1, . . . , π_K$, and $σ^2$ into (4.13)</p><p>$$
\begin{align}
\hat{\mu}<em>k=\frac{1}{n_k}\sum</em>{i:y_i=k}x_i \quad (4.15) \
\hat{\sigma}^2=\frac{1}{n-K}\sum_{k=1}^K\sum_{i:y_i=k}(x_i-\hat{\mu_k})^2 \quad (4.16)\
\hat{\pi_k}=\frac{n_k}{n}
\end{align}
$$</p><p>where n is the total number of training observations, and $n_k$ is the number
of training observations in the kth class.</p><p>$\hat{\mu}_k$: average of all the training observations from the kth class;</p><p>$\hat{\sigma}^2$: a weighted average of the sample variances for each of the K classes.</p><p>$\hat{\pi_k}$: the proportion of the training observations
that belong to the kth class</p><h2 id=lda-classifier>LDA classifier<a hidden class=anchor aria-hidden=true href=#lda-classifier>#</a></h2><p>The LDA classifier assigns an observation X = x to the class for which</p><p>$$
\begin{align}
\hat{\delta}_k(x)=x\frac{\hat{\mu}_k}{\hat{\sigma}^2}-\frac{\hat{\mu}_k^2}{2\hat{\sigma}^2}+\log(\hat{\pi}_k)
\end{align}
$$
is largest.</p><p>The word <em><strong>linear</strong></em> in the classifier’s name stems from the fact
that the <em><strong>discriminant functions</strong></em> $\hat{\delta}_k(x)$ are linear functions of x.</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_discriminant_analysis/16_hu833a910696322272e30a562d84cc9ab7_273195_600x0_resize_box_3.png width=600 height=400><figcaption><small></small></figcaption></figure><p>The right-hand panel of Figure 4.4 displays a histogram of a random
sample of 20 observations from each class.</p><p>To implement LDA,</p><ol><li>Estimating πk, μk, and σ2 using (4.15) and (4.16).</li><li>Compute the decision boundary, shown as a black solid line, that results from assigning an observation to the class for which $\hat{\delta}_k(x)$ is largest.</li></ol><p>In this case, since n1 = n2 = 20,
we have $\hat{\pi_1}$ = $\hat{\pi_2}$. As a result, the decision boundary corresponds to the
midpoint between the sample means for the two classes,$\frac{\mu_1+\mu_2}{2}$</p><h1 id=linear-discriminant-analysis-for-p-1>Linear Discriminant Analysis for p >1<a hidden class=anchor aria-hidden=true href=#linear-discriminant-analysis-for-p-1>#</a></h1><p>Assume that X = (X1,X2, . . .,Xp) is drawn from a <strong>multivariate Gaussian</strong> (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix.</p><h2 id=multivariate-gaussian-distribution>Multivariate Gaussian Distribution<a hidden class=anchor aria-hidden=true href=#multivariate-gaussian-distribution>#</a></h2><p>Assumes that each individual predictor
follows a one-dimensional normal distribution with some
correlation between each pair of predictors.</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_discriminant_analysis/17_hu8311a094f77dc7c2c4e3fdc2bfa21bf3_64264_600x0_resize_box_3.png width=600 height=121><figcaption><small></small></figcaption></figure><p>To indicate that a p-dimensional random variable X has a multivariate
Gaussian distribution, we write X ∼ N(μ,Σ). Here E(X) = μ is
the mean of X (a vector with p components), and Cov(X) = Σ is the
p × p <strong>covariance matrix</strong> of X. Formally, the <strong>multivariate Gaussian density</strong>
is defined as
$$
\begin{align}
f(x)=\frac{1}{\sqrt{(2\pi)^{p}|Σ|}}\exp{\left( \frac{1}{2}(x-\mu)^TΣ^{-1}(x-\mu) \right)}
\end{align}
$$
In the case of p > 1 predictors, the <strong>LDA classifier</strong> assumes that the
observations in the kth class are drawn from a multivariate Gaussian distribution
$N(μ_k,Σ)$, where $μ_k$ is a class-specific mean vector, and Σ is a
covariance matrix that is common to all K classes.</p><p>Plugging the density function for the kth class, $f_k(X = x)$, into $Pr(Y = k|X = x)$, the Bayes classifier assigns an observation X = x
to the class for which
$$
\begin{align}
\delta_k(x)=x^TΣ^{-1}\mu_k-\frac{1}{2}\mu_k^TΣ^{-1}\mu_k+\log{\pi_k} \quad \quad (4.19)
\end{align}
$$
is largest.</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_discriminant_analysis/18_hu1d6561f500ae721a7ec01a6f000b787f_275828_600x0_resize_box_3.png width=600 height=417><figcaption><small></small></figcaption></figure><p>Once again, we need to estimate the unknown parameters $μ_1, . . . , μ_K$,
$π_1, . . . , π_K$, and Σ; the formulas are similar to those used in the one dimensional
case, given in (4.15). To assign a new observation X = x,
<strong>LDA</strong> plugs these estimates into (4.19) and classifies to the class for which
$\hat{\delta}_k(x)$ is largest.</p><blockquote><p>Overall, the LDA decision boundaries are
pretty close to the Bayes decision boundaries, shown again as dashed lines.</p></blockquote><h2 id=caveats>Caveats<a hidden class=anchor aria-hidden=true href=#caveats>#</a></h2><ol><li><p>Training error rates will usually be lower than test error
rates. The higher the ratio of parameters p to number
of samples n, the more we expect this overfitting to play a role.</p></li><li><p>Second, since only 3.33% of the individuals in the training sample
defaulted, a simple but useless classifier that always predicts that each individual will not default, regardless of his or her credit card
balance and student status, will result in an error rate of 3.33%. In
other words, the trivial <strong>null classifier</strong> will achieve an error rate that is only a bit higher than the LDA training set error rate.</p></li></ol><h3 id=two-types-of-error-confusion-matrix>Two Types of Error, Confusion Matrix<a hidden class=anchor aria-hidden=true href=#two-types-of-error-confusion-matrix>#</a></h3><p>Binary classifier can make two types of
errors:</p><ol><li>it can incorrectly assign an individual who defaults to the no default
category;</li><li>it can incorrectly assign an individual who does not default to
the default category.</li></ol><p><strong>Confusion Matrix</strong></p><p>*注意这张图不是标准的confusion matrix，看下面那张<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_discriminant_analysis/10_hu6e5446caf80cb9f272d633596c0c19ba_181527_600x0_resize_box_3.png width=600 height=268><figcaption><small></small></figcaption></figure></p><p><strong>Explanation</strong>：
The matrix
table reveals that LDA predicted that a total of 104 people would default.
Of these people, 81 actually defaulted and 23 did not.</p><p><strong>Type I Error</strong>： Of the 333 individuals who defaulted, 252 (or 75.7%) were missed by LDA. So while the overall error
rate is low, the error rate among individuals who defaulted is very high.
From the perspective of a credit card company that is trying to identify
high-risk individuals, an error rate of 252/333 = 75.7% among individuals
who default may well be unacceptable.</p><p><strong>Type II Error</strong>：Only 23 out
of 9, 667 of the individuals who did not default were incorrectly labeled.
This looks like a pretty low error rate!</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_discriminant_analysis/11_hua81a192e347b747900794136cbd40a2d_343718_1000x0_resize_box_3.png width=1000 height=294><figcaption><small></small></figcaption></figure><p><strong>Sensitivity</strong>:the percentage of
true defaulters that are identified, a low 24.3% in this case.</p><p><strong>Specificity</strong>:the percentage of non-defaulters that are correctly identified, here (1 −
23/9, 667)× 100 = 99.8%.</p><h3 id=why-does-lda-do-such-a-poor-job-of-classifying-the-customers-who-default>Why does LDA do such a poor job of classifying the customers who default?<a hidden class=anchor aria-hidden=true href=#why-does-lda-do-such-a-poor-job-of-classifying-the-customers-who-default>#</a></h3><blockquote><p>In other words, why does it have such a low sensitivity?</p></blockquote><p>LDA is trying to approximate the Bayes classifier, which has the lowest
total error rate out of all classifiers (if the Gaussian model is correct).
That is, the Bayes classifier will yield the smallest possible total number
of misclassified observations, irrespective of which class the errors come
from.</p><p>The Bayes classifier works by assigning an observation to the class for
which the posterior probability pk(X) is greatest. In the two-class case, this
amounts to assigning an observation to the default class if $Pr(default = Yes|X = x) > 0.5.$</p><p>Thus, the Bayes classifier, and by extension LDA, uses a threshold of 50%
for the posterior probability of default in order to assign an observation
to the default class.</p><p><strong>Modify LDA</strong></p><p>If we are concerned about incorrectly predicting
the default status for individuals who default, then we can consider
lowering this threshold.</p><p>$$P(default = Yes|X = x) > 0.2$$</p><p>Figure 4.7 illustrates the trade-off that results from modifying the threshold
value for the posterior probability of default</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_discriminant_analysis/12_hu114fcb30357cb88d2a79fd63dcf818c6_162276_700x0_resize_box_3.png width=700 height=449><figcaption><small></small></figcaption></figure><p>How can we decide which threshold value is
best? Such a decision must be based on <strong>domain knowledge</strong>, such as detailed
information about the costs associated with default.</p><h3 id=roc--auc>ROC & AUC<a hidden class=anchor aria-hidden=true href=#roc--auc>#</a></h3><p><strong>ROC</strong>:The ROC curve is a popular graphic for simultaneously displaying the
two types of errors for all possible thresholds.</p><p><strong>AUC</strong>: The overall performance of a classifier, summarized
over all possible thresholds, is given by the area under the (ROC)
curve (AUC).</p><ul><li><p>An ideal ROC curve will hug the top left corner, so the larger
the AUC the better the classifier. We expect
a classifier that performs no better than chance to have an AUC of 0.5</p></li><li><p>ROC curves are useful for comparing different classifiers, since they take
into account all possible thresholds.</p></li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_discriminant_analysis/13_hu12eed1ec59869d940b6507c8ff5970ec_236196_700x0_resize_box_3.png width=700 height=681><figcaption><small></small></figcaption></figure><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_discriminant_analysis/14_hu75f35f0a3ac11f2b1f49cd2d67e0f185_78889_700x0_resize_box_3.png width=700 height=208><figcaption><small></small></figcaption></figure><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_discriminant_analysis/15_hu35d7fec76043afb1f6450325425f61eb_95961_700x0_resize_box_3.png width=700 height=206><figcaption><small></small></figcaption></figure><h1 id=quadratic-discriminant-analysis>Quadratic Discriminant Analysis<a hidden class=anchor aria-hidden=true href=#quadratic-discriminant-analysis>#</a></h1><p><strong>Quadratic discriminant analysis (QDA)</strong> classifier results from assuming that the
observations from each class are drawn from a Gaussian distribution, and
plugging estimates for the parameters into Bayes’ theorem in order to perform
prediction.</p><p>However, unlike LDA, <strong>QDA assumes that each class has
its own covariance matrix</strong>. That is, it assumes that an observation from the
kth class is of the form $X ∼ N(μ_k,Σ_k)$, where $Σ_k$ is a covariance matrix
for the kth class. Under this assumption, the Bayes classifier assigns an
observation $X = x$ to the class for which</p><p>$$
\begin{align}
\delta_k(x)&=-\frac{1}{2}(x-\mu_k)^TΣ_k^{-1}(x-\mu_k)-\frac{1}{2}\log{|Σ_k|}+\log{\pi_k} \\
&=-\frac{1}{2}x^TΣ_k^{-1}x+x^TΣ_k^{-1}\mu_k-\frac{1}{2}\mu_k^TΣ_k^{-1}\mu_k-\frac{1}{2}\log{|Σ_k|}+\log{\pi_k}
\end{align}
$$</p><p>is largest.</p><p>So the QDA classifier involves plugging estimates for $Σ_k, μ_k, π_k$ into $\delta_k(x)$, and then assigning an observation $X = x$ to the class
for which this quantity is largest. Unlike LDA, the quantity $x$ appears
as a quadratic function.</p><h2 id=why-does-it-matter-whether-or-not-we-assume-that-the-k-classes-share-a-common-covariance-matrix>Why does it matter whether or not we assume that the K classes share a common covariance matrix?<a hidden class=anchor aria-hidden=true href=#why-does-it-matter-whether-or-not-we-assume-that-the-k-classes-share-a-common-covariance-matrix>#</a></h2><p>The answer lies in the <strong>bias-variance trade-off</strong>:</p><ul><li>When there are p predictors, then estimating a covariance matrix requires estimating
p(p+1)/2 parameters. QDA estimates a separate covariance matrix
for each class, for a total of Kp(p+1)/2 parameters.</li><li>Consequently, LDA is a much less flexible classifier than QDA, and
so has substantially lower variance.</li><li>But there is a trade-off: if LDA’s assumption that
the K classes share a common covariance matrix is badly off, then LDA
can suffer from high bias.</li></ul><p><strong>Conclusion</strong></p><ul><li>LDA tends to be a better bet
than QDA if there are relatively few training observations and so reducing
variance is crucial.</li><li>QDA is recommended if the training set is
very large, so that the variance of the classifier is not a major concern, or if
the assumption of a common covariance matrix for the K classes is clearly
untenable</li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_discriminant_analysis/16_hu833a910696322272e30a562d84cc9ab7_273195_700x0_resize_box_3.png width=700 height=467><figcaption><small></small></figcaption></figure><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &ldquo;The elements of statistical learning: data mining, inference and prediction.&rdquo; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://nancyyanyu.github.io/tags/lda/>LDA</a></li><li><a href=https://nancyyanyu.github.io/tags/classification/>Classification</a></li><li><a href=https://nancyyanyu.github.io/tags/model-evaluation/>Model Evaluation</a></li></ul><nav class=paginav><a class=prev href=https://nancyyanyu.github.io/posts/ml-comparing-logistic-regression-lda-qda-and-knn/><span class=title>« Prev</span><br><span>Study Note: Comparing Logistic Regression, LDA, QDA, and KNN</span></a>
<a class=next href=https://nancyyanyu.github.io/posts/ml-bias-variance/><span class=title>Next »</span><br><span>Study Note: Bias, Variance and Model Complexity</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Discriminant Analysis, ROC & AUC, Confusion Matrix on twitter" href="https://twitter.com/intent/tweet/?text=Study%20Note%3a%20Linear%20Discriminant%20Analysis%2c%20ROC%20%26%20AUC%2c%20Confusion%20Matrix&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_discriminant_analysis%2f&amp;hashtags=LDA%2cClassification%2cModelEvaluation"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Discriminant Analysis, ROC & AUC, Confusion Matrix on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_discriminant_analysis%2f&amp;title=Study%20Note%3a%20Linear%20Discriminant%20Analysis%2c%20ROC%20%26%20AUC%2c%20Confusion%20Matrix&amp;summary=Study%20Note%3a%20Linear%20Discriminant%20Analysis%2c%20ROC%20%26%20AUC%2c%20Confusion%20Matrix&amp;source=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_discriminant_analysis%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Discriminant Analysis, ROC & AUC, Confusion Matrix on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_discriminant_analysis%2f&title=Study%20Note%3a%20Linear%20Discriminant%20Analysis%2c%20ROC%20%26%20AUC%2c%20Confusion%20Matrix"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Discriminant Analysis, ROC & AUC, Confusion Matrix on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_discriminant_analysis%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Discriminant Analysis, ROC & AUC, Confusion Matrix on whatsapp" href="https://api.whatsapp.com/send?text=Study%20Note%3a%20Linear%20Discriminant%20Analysis%2c%20ROC%20%26%20AUC%2c%20Confusion%20Matrix%20-%20https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_discriminant_analysis%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Discriminant Analysis, ROC & AUC, Confusion Matrix on telegram" href="https://telegram.me/share/url?text=Study%20Note%3a%20Linear%20Discriminant%20Analysis%2c%20ROC%20%26%20AUC%2c%20Confusion%20Matrix&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_discriminant_analysis%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Discriminant Analysis, ROC & AUC, Confusion Matrix on ycombinator" href="https://news.ycombinator.com/submitlink?t=Study%20Note%3a%20Linear%20Discriminant%20Analysis%2c%20ROC%20%26%20AUC%2c%20Confusion%20Matrix&u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_discriminant_analysis%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://nancyyanyu.github.io/>Nancy's Notebook</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>