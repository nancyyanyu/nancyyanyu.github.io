<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Study Note: SVM | Nancy's Notebook</title><meta name=keywords content="SVM"><meta name=description content="Maximal Margin Classifier
What Is a Hyperplane?
Hyperplane: In a p-dimensional space, a hyperplane is a flat affine subspace of dimension $p − 1$.

e.g. in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line.

Mathematical definition of a hyperplane:
$$
\beta_0+\beta_1X_1+\beta_2X_2,&mldr;+\beta_pX_p=0, \quad (9.1)
$$

Any $X = (X_1,X_2,…X_p)^T$ for which (9.1) holds is a point on the hyperplane.
"><meta name=author content="Me"><link rel=canonical href=https://nancyyanyu.github.io/posts/ml-svm/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Study Note: SVM"><meta property="og:description" content="Maximal Margin Classifier
What Is a Hyperplane?
Hyperplane: In a p-dimensional space, a hyperplane is a flat affine subspace of dimension $p − 1$.

e.g. in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line.

Mathematical definition of a hyperplane:
$$
\beta_0+\beta_1X_1+\beta_2X_2,&mldr;+\beta_pX_p=0, \quad (9.1)
$$

Any $X = (X_1,X_2,…X_p)^T$ for which (9.1) holds is a point on the hyperplane.
"><meta property="og:type" content="article"><meta property="og:url" content="https://nancyyanyu.github.io/posts/ml-svm/"><meta property="og:image" content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-06-12T01:00:13+00:00"><meta property="article:modified_time" content="2019-06-12T01:00:13+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Study Note: SVM"><meta name=twitter:description content="Maximal Margin Classifier
What Is a Hyperplane?
Hyperplane: In a p-dimensional space, a hyperplane is a flat affine subspace of dimension $p − 1$.

e.g. in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line.

Mathematical definition of a hyperplane:
$$
\beta_0+\beta_1X_1+\beta_2X_2,&mldr;+\beta_pX_p=0, \quad (9.1)
$$

Any $X = (X_1,X_2,…X_p)^T$ for which (9.1) holds is a point on the hyperplane.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nancyyanyu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Study Note: SVM","item":"https://nancyyanyu.github.io/posts/ml-svm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Study Note: SVM","name":"Study Note: SVM","description":"Maximal Margin Classifier What Is a Hyperplane? Hyperplane: In a p-dimensional space, a hyperplane is a flat affine subspace of dimension $p − 1$.\ne.g. in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line. Mathematical definition of a hyperplane: $$ \\beta_0+\\beta_1X_1+\\beta_2X_2,\u0026hellip;+\\beta_pX_p=0, \\quad (9.1) $$\nAny $X = (X_1,X_2,…X_p)^T$ for which (9.1) holds is a point on the hyperplane. ","keywords":["SVM"],"articleBody":"Maximal Margin Classifier What Is a Hyperplane? Hyperplane: In a p-dimensional space, a hyperplane is a flat affine subspace of dimension $p − 1$.\ne.g. in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line. Mathematical definition of a hyperplane: $$ \\beta_0+\\beta_1X_1+\\beta_2X_2,…+\\beta_pX_p=0, \\quad (9.1) $$\nAny $X = (X_1,X_2,…X_p)^T$ for which (9.1) holds is a point on the hyperplane. Classification Using a Separating Hyperplane Setting:\n$n \\times p$ data matrix $X$ that consists of $n$ training observations in p-dimensional space These observations fall into two classes—that is, $y_1, . . . , y_n \\in {−1, 1}$. Test observation: a p-vector of observed features $x^∗ ={x^∗_1 . . . x^∗_p}^T$. Separating hyperplane has the property that: $$ y_i(\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2},…+\\beta_px_{ip})\u003e0, \\quad (9.8) $$\nfor all i=1,…,n We classify the test observation $x^{*}$ based on the sign of $f(x^*) = \\beta_0+\\beta_1x_{1}^{*}+\\beta_2x_{2}^*,…+\\beta_px_{p}^{*}$.\nIf $f(x^∗)$ is positive, then we assign the test observation to class 1, and if f(x∗) is negative, then we assign it to class −1. Magnitude of $f(x^∗)$. If $f(x^∗)$is far from zero, then this means that $x^∗$ lies far from the hyperplane,and so we can be confident about our class assignment for $x^∗$. The Maximal Margin Classifier Margin: the smallest (perpendicular) distance from each training observation to a given separating hyperplane $\\Rightarrow$ the minimal distance from the observations to the hyperplane.\nMaximal margin hyperplane: the separating hyperplane that is farthest from the training observations.\nThe maximal margin hyperplane is the separating hyperplane for which the margin is largest Overfitting when $p$ is large. Maximal margin classifier: classify a test observation based on which side of the maximal margin hyperplane it lies.\nSupport vectors: training observations that are equidistant from the maximal margin hyperplane that indicate the width of the margin.\nThey “support” the maximal margin hyperplane in the sense vector that if these points were moved slightly then the maximal margin hyperplanewould move as well. The maximal margin hyperplane depends directly on the support vectors, but not on the other observations Construction of the Maximal Margin Classifier The maximal margin hyperplane is the solution to the optimization problem $$ \\max_{\\beta_0,…\\beta_p} M \\\\ s.t. \\sum_{j=1}^p \\beta_j^2=1, \\quad (9.10) \\\\ y_i(\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2},…+\\beta_px_{ip})\u003eM \\quad \\forall i=1,..,n. \\quad (9.11) $$\nThe constraint in (9.11) in fact requires that each observation be on the correct side of the hyperplane, with some cushion, provided that margin $M$ is positive.) The constraint in (9.10) makes sure the perpendicular distance from the i-th observation to the hyperplane is given by $$ y_i(\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2},…+\\beta_px_{ip}) $$\nSupport Vector Classifiers Overview of the Support Vector Classifier Maximal margin hyperplane is extremely sensitive to a change in a single observation suggests that it may have overfit the training data.\nIn this case, we might be willing to consider a classifier based on a hyperplane that does not perfectly separate the two classes, in the interest of\nGreater robustness to individual observations, and Better classification of most of the training observations. Support Vector Classifier (Soft Margin Classifier): Rather than seeking the largest possible margin that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observationsto be on the incorrect side of the margin, or even the incorrect side of the hyperplane.\nDetails of the Support Vector Classifier Optimization problem: $$ \\max_{\\beta_0,…\\beta_p,\\epsilon_1,..,\\epsilon_n} M \\\\ s.t. \\sum_{j=1}^p \\beta_j^2=1, \\quad (9.13) \\\\ y_i(\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2},…+\\beta_px_{ip})\u003eM(1-\\epsilon_i) \\quad \\forall i=1,..,n. \\quad (9.14) \\\\ \\epsilon_i\\geq0,\\sum_{i=1}^p\\epsilon_i \\leq C, \\quad (9.15) $$\nSlack variables: $\\epsilon_1,..,\\epsilon_n$ - allow individual observations to be on the wrong side of the margin or the hyperplane $\\epsilon_i=0$: the i-th observation is on the correct side of the margin $\\epsilon_i \u003e0$: the i-th observation is on the wrong side of the margin $\\Rightarrow$ i-th observation violated the margin. $\\epsilon_i \u003e1$: the i-th observation is on the wrong side of the hyperplane Classify the test observation based on the sign of $f(x^{*}) = \\beta_0+\\beta_1x_{1}^*+\\beta_2x_{2}^*,…+\\beta_px_{p}^*$. Tuning parameter C: $C$ bounds the sum of the $\\epsilon_i$’s, and so it determines the number and severity of the violationsto the margin(and to the hyperplane) that we will tolerate. $ budget for the amount that the margin can be violated by the $n$ observations. Generally chosen via cross-validation. $C$ controls the bias-variance trade-off of the support vector classifier. C is small: a classifier highly fit to the data, fewersupport vectors $\\Rightarrow$ low bias , high variance; C is large: margin wider, many support vectors $\\Rightarrow$ high bias , low variance; Properties:\nOnly observations that either lie on the margin or that violate the margin (support vectors) will affect the hyperplane, and hence the classifier obtained. The fact that the support vector classifier’s decision rule is based only on a potentially small subset of the training observations (the support vectors) means that it is quite robust to the behavior of observations that are far away from the hyperplane. Different from LDA which depends on the mean of all of the observations within each class, as well as the within-class covariance matrix computed using all of the observations Support Vector Machines SVMs with Kernel The support vector machine (SVM) is an extension of the support vector classifier that results from enlarging the feature space using kernels.\nThe solution to the support vector classifier problem involves only the inner products of the observations: $$ \\langle x_i,x_{i^{’}} \\rangle =\\sum_{j=1}^px_{ij}x_{i^{’}j} $$ (Details won’t be discussed in this note)\nThe linear support vector classifier can be represented as $$ f(x)=\\beta_0+\\sum_{i=1}^n \\alpha_i \\langle x,x_i \\rangle $$\n$α_i$ is nonzero only for the support vectors in the solution—that is, if a training observation is not a support vector, then its $α_i$equals zero. So if $S$ is the collection of indices of these support points: $$ f(x)=\\beta_0+\\sum_{i \\in S}^n \\alpha_i \\langle x,x_i \\rangle $$ Generalization: Kernel $$ K(x_i,x_{i^{’}}) $$ Kernel: Kernel is a function that quantifies the similarity of two observations.\nLinear kernel: $K(x_i,x_{i^{’}})=\\sum_{j=1}^px_{ij}x_{i^{’}j}$ Linear kernel essentially quantifies the similarity of a pair of observations using Pearson (standard) correlation. Polynomial kernel of degree d: $K(x_i,x_{i^{’}})=(1+\\sum_{j=1}^px_{ij}x_{i^{’}j})^d$ fitting a support vector classifier in a higher-dimensional space involving polynomials of degree $d$. Radial kernel: $K(x_i,x_{i^{’}})=\\exp(-\\gamma \\sum_{j=1}^p(x_{ij}-x_{i^{’}j})^2)$ Radial kernel has very local behavior: only nearby training observations have an effect on the class label of a test observation If a given test observation $x^∗ = (x^∗_1 . . .x^∗_p)^T$ is far from a training observation $x_i$ in terms of Euclidean distance; $\\Rightarrow$ $ \\sum_{j=1}^p(x_{ij}-x_{i^{’}j})^2 $ will be large $\\Rightarrow$ $K(x_i,x_{i^{’}})=\\exp(-\\gamma \\sum_{j=1}^p(x_{ij}-x_{i^{’}j})^2)$ will be very tiny. $\\Rightarrow$ $x_i$ will play virtually no role in $f(x^∗)$. Support Vector Machine: When the support vector classifieris combined with a non-linear kernel, the resulting classifier is known as a support vector machine. $$ f(x)=\\beta_0+\\sum_{i \\in S}^n \\alpha_i K(x,x_i) $$ **Advantage of Kernel over enlarging the feature space using functions of the original features: **\nComputational: one need only compute $K(x_i,x_{i^{’}})$ for all $\\left(\\begin{array}{c}n\\\\ 2\\end{array}\\right)$ distinct pairs $i, i^{’}$. This can bedone without explicitly working in the enlarged feature space. Curse of dimensionality: for some kernels, such as the radial kernel, the feature space is implicit and infinite-dimensional. SVMs with More than Two Classes One-Versus-One Classification A one-versus-one or all-pairs approach constructs $\\left(\\begin{array}{c}K\\\\ 2\\end{array}\\right)$ SVMs, each of which compares a pair of classes\nOne such SVM might compare the $k$-th class, coded as +1, to the $k^{’}$-th class, codedas −1. We classify a test observation using each of the $\\left(\\begin{array}{c}K\\\\ 2\\end{array}\\right)$ classifiers. We tally the number of times that the test observation is assigned to each of the K classes. The final classification is performed by assigning the test observation to the class to which it was most frequently assigned in these $\\left(\\begin{array}{c}K\\\\ 2\\end{array}\\right)$ pairwise classifications. One-Versus-All Classification The one-versus-all approach:\nWe fit $K$ SVMs, each time comparing one of all the K classes to the remaining K − 1 classes. Let $β_{0k}, β_{1k}, . . . , β_{pk}$ denote the parameters that result from fitting an SVM comparing the kth class(coded as +1) to the others (coded as −1). Let $ x^∗$ denote a test observation. We assign the observation to the class for which $β_{0k}x_1^+β_{1k}x_2^+, . . . ,+ β_{pk}x_p^*$ is largest, as this amounts to a high level of confidence that the test observation belongs to the kth class rather than to any of the other classes. Relationship to Logistic Regression Rewrite the criterion (9.12)–(9.15) [look at last post] for fitting the support vector classifier $f(X) = β_0 + β_1X_1 + . . . + β_pX_p$ as $$ \\min_{\\beta_0,…,\\beta_p}\\left( \\sum_{i=1}^n\\max[0,1-y_If(x_i)]+\\lambda\\sum_{j=1}^p\\beta_j^2 \\right) $$\nλ is small: few violations to the margin ; high-variance, low-bias; $\\Leftrightarrow$ small $C$; “Loss + Penalty” form: $$ \\min_{\\beta_0,…,\\beta_p}\\left( L(\\mathbf{X},\\mathbf{y},\\beta)+\\lambda P(\\beta) \\right) $$\n$L(\\mathbf{X},\\mathbf{y},\\beta)$ : loss function $P(\\beta)$: penalty function Ridge regression and the lasso: $$ L(\\mathbf{X},\\mathbf{y},\\beta)=\\sum_{i=1}^n \\left( y_i-\\beta_0-\\sum_{j=1}^p x_{ij}\\beta_j \\right)^2 \\\\ P(\\beta) = \\sum_{j=1}^p \\beta_j^2 \\quad ridge , regression \\\\ P(\\beta) = \\sum_{j=1}^p |\\beta_j| \\quad lasso $$ SVM: hindge loss $$ L(\\mathbf{X},\\mathbf{y},\\beta)=\\sum_{i=1}^n \\max[0,1-y_i(\\beta_0+\\beta_1x_{i1}+,,,+\\beta_px_{ip})] $$ Due to thesimilarities between their loss functions, logistic regression and the supportvector classifier often give very similar results. When the classes are well separated, SVMs tend to behave better than logistic regression Ref:\nJames, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.\nHastie, Trevor, et al. “The elements of statistical learning: data mining, inference and prediction.” The Mathematical Intelligencer 27.2 (2005): 83-85\n","wordCount":"1541","inLanguage":"en","datePublished":"2019-06-12T01:00:13Z","dateModified":"2019-06-12T01:00:13Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nancyyanyu.github.io/posts/ml-svm/"},"publisher":{"@type":"Organization","name":"Nancy's Notebook","logo":{"@type":"ImageObject","url":"https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nancyyanyu.github.io/ accesskey=h title="Nancy's Notebook (Alt + H)"><img src=https://nancyyanyu.github.io/apple-touch-icon.png alt aria-label=logo height=35>Nancy's Notebook</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nancyyanyu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://nancyyanyu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://nancyyanyu.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nancyyanyu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nancyyanyu.github.io/posts/>Posts</a></div><h1 class=post-title>Study Note: SVM</h1><div class=post-meta><span title='2019-06-12 01:00:13 +0000 UTC'>June 12, 2019</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1541 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/ML-SVM/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h1 id=maximal-margin-classifier>Maximal Margin Classifier<a hidden class=anchor aria-hidden=true href=#maximal-margin-classifier>#</a></h1><h2 id=what-is-a-hyperplane>What Is a Hyperplane?<a hidden class=anchor aria-hidden=true href=#what-is-a-hyperplane>#</a></h2><p><strong>Hyperplane</strong>: In a p-dimensional space, a hyperplane is a flat affine subspace of dimension $p − 1$.</p><ul><li>e.g. in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line.</li></ul><p><strong>Mathematical definition of a hyperplane</strong>:
$$
\beta_0+\beta_1X_1+\beta_2X_2,&mldr;+\beta_pX_p=0, \quad (9.1)
$$</p><ul><li>Any $X = (X_1,X_2,…X_p)^T$ for which (9.1) holds is a point on the hyperplane.</li></ul><h2 id=classification-using-a-separating-hyperplane>Classification Using a Separating Hyperplane<a hidden class=anchor aria-hidden=true href=#classification-using-a-separating-hyperplane>#</a></h2><p><strong>Setting</strong>:</p><ul><li>$n \times p$ data matrix $X$ that consists of $n$ training observations in p-dimensional space</li><li>These observations fall into two classes—that is, $y_1, . . . , y_n \in {−1, 1}$.</li><li>Test observation: a p-vector of observed features $x^∗ ={x^∗_1 . . . x^∗_p}^T$.</li></ul><p><strong>Separating hyperplane</strong> has the property that:
$$
y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},&mldr;+\beta_px_{ip})>0, \quad (9.8)
$$</p><ul><li>for all i=1,…,n</li></ul><p>We classify the test observation $x^{*}$ based on the sign of $f(x^*) = \beta_0+\beta_1x_{1}^{*}+\beta_2x_{2}^*,&mldr;+\beta_px_{p}^{*}$.</p><ul><li>If $f(x^∗)$ is positive, then we assign the test observation to class 1, and if f(x∗) is negative, then we assign it to class −1.</li><li><strong>Magnitude</strong> of $f(x^∗)$. If $f(x^∗)$is far from zero, then this means that $x^∗$ lies far from the hyperplane,and so we can be confident about our class assignment for $x^∗$.</li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-svm/S1_hube95143e5871b74c3623caee0168904b_246826_700x0_resize_box_3.png width=700 height=542><figcaption><small></small></figcaption></figure><h2 id=the-maximal-margin-classifier>The Maximal Margin Classifier<a hidden class=anchor aria-hidden=true href=#the-maximal-margin-classifier>#</a></h2><p><em><strong>Margin</strong></em>: the smallest (perpendicular) distance from each training observation to a given separating hyperplane $\Rightarrow$ the minimal distance from the observations to the hyperplane.</p><p><em><strong>Maximal margin hyperplane</strong></em>: the separating hyperplane that is farthest from the training observations.</p><ul><li>The maximal margin hyperplane is the separating hyperplane for which the <em>margin</em> is <strong>largest</strong></li><li>Overfitting when $p$ is large.</li></ul><p><em><strong>Maximal margin classifier</strong></em>: classify a test observation based on which side of the maximal margin hyperplane it lies.</p><p><em><strong>Support vectors</strong></em>: training observations that are equidistant from the maximal margin hyperplane that indicate <em>the width of the margin</em>.</p><ul><li>They “support” the maximal margin hyperplane in the sense vector that if these points were moved slightly then the maximal margin hyperplanewould move as well.</li><li><em>The maximal margin hyperplane depends directly on the support vectors, but not on the other observations</em></li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-svm/S2_huaacbb4b5c5ef1d2bec8a2ca4b072043b_229537_700x0_resize_box_3.png width=700 height=588><figcaption><small></small></figcaption></figure><h2 id=construction-of-the-maximal-margin-classifier>Construction of the Maximal Margin Classifier<a hidden class=anchor aria-hidden=true href=#construction-of-the-maximal-margin-classifier>#</a></h2><p>The maximal margin hyperplane is the solution to the optimization problem
$$
\max_{\beta_0,&mldr;\beta_p} M \\
s.t. \sum_{j=1}^p \beta_j^2=1, \quad (9.10) \\
y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},&mldr;+\beta_px_{ip})>M \quad \forall i=1,..,n. \quad (9.11)
$$</p><ul><li>The constraint in (9.11) in fact requires that each observation be on the correct side of the hyperplane, with some cushion, provided that <strong>margin</strong> $M$ is positive.)</li><li>The constraint in (9.10) makes sure the perpendicular distance from the i-th observation to the hyperplane is given by</li></ul><p>$$
y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},&mldr;+\beta_px_{ip})
$$</p><h1 id=support-vector-classifiers>Support Vector Classifiers<a hidden class=anchor aria-hidden=true href=#support-vector-classifiers>#</a></h1><h2 id=overview-of-the-support-vector-classifier>Overview of the Support Vector Classifier<a hidden class=anchor aria-hidden=true href=#overview-of-the-support-vector-classifier>#</a></h2><p><strong>Maximal margin hyperplane</strong> is extremely sensitive to a change in a single observation suggests that it may have <em><strong>overfit</strong></em> the training data.</p><p>In this case, we might be willing to consider a classifier based on a hyperplane that does not perfectly separate the two classes, in the interest of</p><ul><li>Greater <em>robustness</em> to individual observations, and</li><li>Better classification of most of the training observations.</li></ul><p><em><strong>Support Vector Classifier (Soft Margin Classifier)</strong></em>: Rather than seeking the largest possible margin that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observationsto be on the incorrect side of the margin, or even the incorrect side of the hyperplane.</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-svm/SVC1_hu86c77bb06adcf1a51c82d367fd8893de_236121_700x0_resize_box_3.png width=700 height=512><figcaption><small></small></figcaption></figure><h2 id=details-of-the-support-vector-classifier>Details of the Support Vector Classifier<a hidden class=anchor aria-hidden=true href=#details-of-the-support-vector-classifier>#</a></h2><p><strong>Optimization problem</strong>:
$$
\max_{\beta_0,&mldr;\beta_p,\epsilon_1,..,\epsilon_n} M \\
s.t. \sum_{j=1}^p \beta_j^2=1, \quad (9.13) \\
y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2},&mldr;+\beta_px_{ip})>M(1-\epsilon_i) \quad \forall i=1,..,n. \quad (9.14) \\
\epsilon_i\geq0,\sum_{i=1}^p\epsilon_i \leq C, \quad (9.15)
$$</p><ul><li><em><strong>Slack variables</strong></em>: $\epsilon_1,..,\epsilon_n$ - allow individual observations to be on the wrong side of the margin or the hyperplane<ul><li>$\epsilon_i=0$: the i-th observation is on the correct side of the <em>margin</em></li><li>$\epsilon_i >0$: the i-th observation is on the wrong side of the <em>margin</em> $\Rightarrow$ i-th observation <em><strong>violated</strong></em> the margin.</li><li>$\epsilon_i >1$: the i-th observation is on the wrong side of the <em>hyperplane</em></li></ul></li><li>Classify the test observation based on the sign of $f(x^{*}) = \beta_0+\beta_1x_{1}^*+\beta_2x_{2}^*,&mldr;+\beta_px_{p}^*$.</li><li><em><strong>Tuning parameter C</strong></em>: $C$ bounds the sum of the $\epsilon_i$&rsquo;s, and so it determines the number and severity of the violationsto the margin(and to the hyperplane) that we will tolerate. $<ul><li><em><strong>budget</strong></em> for the amount that the margin can be violated by the $n$ observations.</li><li>Generally chosen via <em>cross-validation</em>.</li><li>$C$ controls the <strong>bias-variance trade-off</strong> of the support vector classifier.<ul><li>C is small: a classifier highly fit to the data, fewersupport vectors $\Rightarrow$ low bias , high variance;</li><li>C is large: margin wider, many support vectors $\Rightarrow$ high bias , low variance;</li></ul></li></ul></li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-svm/SVC2_hu15c8100fd357ab271e8e26f9a13d357e_254413_700x0_resize_box_3.png width=700 height=856><figcaption><small></small></figcaption></figure><p><strong>Properties</strong>:</p><ul><li>Only observations that either <em>lie on the margin or that violate the margin</em> (<strong>support vectors</strong>) will affect the hyperplane, and hence the classifier obtained.</li><li>The fact that the support vector classifier’s decision rule is based only on a potentially small subset of the training observations (the <em><strong>support vectors</strong></em>) means that it is quite <strong>robust</strong> to the behavior of observations that are far away from the hyperplane.<ul><li>Different from LDA which depends on the mean of <em>all</em> of the observations within each class, as well as the <em>within-class covariance matrix</em> computed using all of the observations</li></ul></li></ul><h1 id=support-vector-machines>Support Vector Machines<a hidden class=anchor aria-hidden=true href=#support-vector-machines>#</a></h1><h2 id=svms-with-kernel>SVMs with Kernel<a hidden class=anchor aria-hidden=true href=#svms-with-kernel>#</a></h2><p>The <em><strong>support vector machine (SVM)</strong></em> is an extension of the support vector classifier that results from enlarging the feature space using <strong>kernels</strong>.</p><p>The <em><strong>solution to the support vector classifier problem</strong></em> involves only the <em><strong>inner products</strong></em> of the observations:
$$
\langle x_i,x_{i^{&rsquo;}} \rangle =\sum_{j=1}^px_{ij}x_{i^{&rsquo;}j}
$$
(Details won&rsquo;t be discussed in this note)</p><p>The <strong>linear support vector classifier</strong> can be represented as
$$
f(x)=\beta_0+\sum_{i=1}^n \alpha_i \langle x,x_i \rangle
$$</p><ul><li>$α_i$ is nonzero only for the support vectors in the solution—that is, if a training observation is not a support vector, then its $α_i$equals zero.</li></ul><p>So if $S$ is the collection of indices of these support points:
$$
f(x)=\beta_0+\sum_{i \in S}^n \alpha_i \langle x,x_i \rangle
$$
<strong>Generalization</strong>: <em>Kernel</em>
$$
K(x_i,x_{i^{&rsquo;}})
$$
<strong>Kernel</strong>: Kernel is a function that quantifies the similarity of two observations.</p><ul><li><em><strong>Linear kernel</strong></em>: $K(x_i,x_{i^{&rsquo;}})=\sum_{j=1}^px_{ij}x_{i^{&rsquo;}j}$<ul><li>Linear kernel essentially quantifies the similarity of a pair of observations using <strong>Pearson</strong> (standard) correlation.</li></ul></li><li><em><strong>Polynomial kernel</strong></em> of degree d: $K(x_i,x_{i^{&rsquo;}})=(1+\sum_{j=1}^px_{ij}x_{i^{&rsquo;}j})^d$<ul><li>fitting a support vector classifier in a higher-dimensional space involving polynomials of degree $d$.</li></ul></li><li><em><strong>Radial kernel</strong></em>: $K(x_i,x_{i^{&rsquo;}})=\exp(-\gamma \sum_{j=1}^p(x_{ij}-x_{i^{&rsquo;}j})^2)$<ul><li>Radial kernel has very <em>local</em> behavior: only nearby training observations have an effect on the class label of a test observation<ul><li>If a given test observation $x^∗ = (x^∗_1 . . .x^∗_p)^T$ is far from a training observation $x_i$ in terms of <em><strong>Euclidean distance</strong></em>; $\Rightarrow$ $ \sum_{j=1}^p(x_{ij}-x_{i^{&rsquo;}j})^2 $ will be large $\Rightarrow$ $K(x_i,x_{i^{&rsquo;}})=\exp(-\gamma \sum_{j=1}^p(x_{ij}-x_{i^{&rsquo;}j})^2)$ will be very tiny. $\Rightarrow$ $x_i$ will play virtually no role in $f(x^∗)$.</li></ul></li></ul></li></ul><p><em><strong>Support Vector Machine</strong></em>: When the support vector classifieris combined with a non-linear kernel, the resulting classifier is known as a support vector machine.
$$
f(x)=\beta_0+\sum_{i \in S}^n \alpha_i K(x,x_i)
$$<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-svm/SVM1_huadb8995410894b4ec08853cc663784b2_217245_700x0_resize_box_3.png width=700 height=453><figcaption><small></small></figcaption></figure></p><p>**Advantage of Kernel over enlarging the feature space using functions of the original features: **</p><ul><li><em><strong>Computational</strong></em>: one need only compute $K(x_i,x_{i^{&rsquo;}})$ for all $\left(\begin{array}{c}n\\ 2\end{array}\right)$ distinct pairs $i, i^{&rsquo;}$. This can bedone without explicitly working in the <em>enlarged feature space.</em><ul><li><strong>Curse of dimensionality</strong>: for some kernels, such as the radial kernel, the feature space is implicit and infinite-dimensional.</li></ul></li></ul><h2 id=svms-with-more-than-two-classes>SVMs with More than Two Classes<a hidden class=anchor aria-hidden=true href=#svms-with-more-than-two-classes>#</a></h2><h3 id=one-versus-one-classification>One-Versus-One Classification<a hidden class=anchor aria-hidden=true href=#one-versus-one-classification>#</a></h3><p>A <em><strong>one-versus-one</strong></em> or <em><strong>all-pairs</strong></em> approach constructs $\left(\begin{array}{c}K\\ 2\end{array}\right)$ SVMs, each of which compares a pair of classes</p><ol><li>One such SVM might compare the $k$-th class, coded as +1, to the $k^{&rsquo;}$-th class, codedas −1.</li><li>We classify a test observation using each of the $\left(\begin{array}{c}K\\ 2\end{array}\right)$ classifiers.</li><li>We tally the number of times that the test observation is assigned to each of the K classes.</li><li>The final classification is performed by assigning the test observation to the class to which it was most frequently assigned in these $\left(\begin{array}{c}K\\ 2\end{array}\right)$ pairwise classifications.</li></ol><h3 id=one-versus-all-classification>One-Versus-All Classification<a hidden class=anchor aria-hidden=true href=#one-versus-all-classification>#</a></h3><p>The <em><strong>one-versus-all</strong></em> approach:</p><ol><li>We fit $K$ SVMs, each time comparing one of all the K classes to the remaining K − 1 classes.</li><li>Let $β_{0k}, β_{1k}, . . . , β_{pk}$ denote the parameters that result from fitting an SVM comparing the kth class(coded as +1) to the others (coded as −1).</li><li>Let $ x^∗$ denote a test observation. We assign the observation to the class for which $β_{0k}x_1^<em>+β_{1k}x_2^</em>+, . . . ,+ β_{pk}x_p^*$ is largest, as this amounts to a high level of confidence that the test observation belongs to the kth class rather than to any of the other classes.</li></ol><h2 id=relationship-to-logistic-regression>Relationship to Logistic Regression<a hidden class=anchor aria-hidden=true href=#relationship-to-logistic-regression>#</a></h2><p>Rewrite the criterion (9.12)–(9.15) [look at last post] for fitting the support vector classifier $f(X) = β_0 + β_1X_1 + . . . + β_pX_p$ as
$$
\min_{\beta_0,&mldr;,\beta_p}\left( \sum_{i=1}^n\max[0,1-y_If(x_i)]+\lambda\sum_{j=1}^p\beta_j^2 \right)
$$</p><ul><li>λ is small: few violations to the margin ; high-variance, low-bias; $\Leftrightarrow$ small $C$;</li></ul><p><strong>“Loss + Penalty” form</strong>:
$$
\min_{\beta_0,&mldr;,\beta_p}\left( L(\mathbf{X},\mathbf{y},\beta)+\lambda P(\beta) \right)
$$</p><ul><li>$L(\mathbf{X},\mathbf{y},\beta)$ : loss function</li><li>$P(\beta)$: penalty function</li></ul><p><strong>Ridge regression and the lasso</strong>:
$$
L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n \left( y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j \right)^2 \\
P(\beta) = \sum_{j=1}^p \beta_j^2 \quad ridge , regression \\
P(\beta) = \sum_{j=1}^p |\beta_j| \quad lasso
$$
<strong>SVM</strong>: <em><strong>hindge loss</strong></em>
$$
L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n \max[0,1-y_i(\beta_0+\beta_1x_{i1}+,,,+\beta_px_{ip})]
$$<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-svm/SVM2_hu0c4050889f065f43c6499ff9b42655dc_143489_700x0_resize_box_3.png width=700 height=481><figcaption><small></small></figcaption></figure></p><ul><li>Due to thesimilarities between their loss functions, logistic regression and the supportvector classifier often give very similar results.</li><li>When the classes are well separated, SVMs tend to behave better than logistic regression</li></ul><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &ldquo;The elements of statistical learning: data mining, inference and prediction.&rdquo; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://nancyyanyu.github.io/tags/svm/>SVM</a></li></ul><nav class=paginav><a class=prev href=https://nancyyanyu.github.io/posts/ml-dimension_reduction-pca/><span class=title>« Prev</span><br><span>Study Note: Dimension Reduction - PCA, PCR</span></a>
<a class=next href=https://nancyyanyu.github.io/posts/ml-resampling-methods-cross-validation-bootstrap/><span class=title>Next »</span><br><span>Study Note: Resampling Methods - Cross Validation, Bootstrap</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: SVM on twitter" href="https://twitter.com/intent/tweet/?text=Study%20Note%3a%20SVM&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-svm%2f&amp;hashtags=SVM"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: SVM on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-svm%2f&amp;title=Study%20Note%3a%20SVM&amp;summary=Study%20Note%3a%20SVM&amp;source=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-svm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: SVM on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-svm%2f&title=Study%20Note%3a%20SVM"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: SVM on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-svm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: SVM on whatsapp" href="https://api.whatsapp.com/send?text=Study%20Note%3a%20SVM%20-%20https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-svm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: SVM on telegram" href="https://telegram.me/share/url?text=Study%20Note%3a%20SVM&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-svm%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: SVM on ycombinator" href="https://news.ycombinator.com/submitlink?t=Study%20Note%3a%20SVM&u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-svm%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://nancyyanyu.github.io/>Nancy's Notebook</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>