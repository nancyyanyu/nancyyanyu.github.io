<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Nancy&#39;s Notebook</title>
    <link>https://nancyyanyu.github.io/posts/</link>
    <description>Recent content in Posts on Nancy&#39;s Notebook</description>
    <image>
      <title>Nancy&#39;s Notebook</title>
      <url>https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 15 Aug 2023 22:36:17 +0000</lastBuildDate><atom:link href="https://nancyyanyu.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Paper Note: CLIP</title>
      <link>https://nancyyanyu.github.io/posts/paper-clip/</link>
      <pubDate>Tue, 15 Aug 2023 22:36:17 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/paper-clip/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;The simple pre-training task of &lt;strong&gt;predicting which caption goes with which image&lt;/strong&gt; is an efficient and scalable way to learn SOTA image representations from scratch. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling &lt;strong&gt;zero-shot transfer&lt;/strong&gt; of the model to downstream tasks.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper Note: Swin Transformer</title>
      <link>https://nancyyanyu.github.io/posts/paper-swintransformer/</link>
      <pubDate>Tue, 15 Aug 2023 22:20:38 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/paper-swintransformer/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;A new ViT &lt;em&gt;whose representation is computed with &lt;strong&gt;S&lt;/strong&gt;hifted &lt;strong&gt;win&lt;/strong&gt;dows&lt;/em&gt;*!***&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper Note: Masked autoencoders(MAE) (very short)</title>
      <link>https://nancyyanyu.github.io/posts/paper-mae/</link>
      <pubDate>Tue, 15 Aug 2023 22:19:31 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/paper-mae/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Masked autoencoders (MAE)&lt;/strong&gt; are scalable &lt;strong&gt;self-supervised&lt;/strong&gt; learners for &lt;strong&gt;computer vision&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper Note: ViT</title>
      <link>https://nancyyanyu.github.io/posts/paper-vit/</link>
      <pubDate>Tue, 15 Aug 2023 22:08:51 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/paper-vit/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;ViT &lt;em&gt;&lt;strong&gt;applies a standard Transformer directly to images&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper Note: Attention is All You Need</title>
      <link>https://nancyyanyu.github.io/posts/paper-transformer/</link>
      <pubDate>Tue, 15 Aug 2023 21:54:35 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/paper-transformer/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;The Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper Note: BERT</title>
      <link>https://nancyyanyu.github.io/posts/paper-bert/</link>
      <pubDate>Tue, 15 Aug 2023 21:39:56 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/paper-bert/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;BERT is designed to &lt;strong&gt;pre-train deep bidirectional representations from unlabeled text&lt;/strong&gt; by &lt;strong&gt;jointly conditioning on both left and right context in all layers&lt;/strong&gt;. As a result, the &lt;strong&gt;pre-trained BERT model&lt;/strong&gt; can be &lt;strong&gt;fine-tuned with just one additional output layer&lt;/strong&gt; to create state-of-the-art models for a wide range of tasks.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>A/B Testing Final Project</title>
      <link>https://nancyyanyu.github.io/posts/ab_testing_final_project/</link>
      <pubDate>Fri, 03 Jul 2020 14:28:54 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/ab_testing_final_project/</guid>
      <description>&lt;p&gt;Implementation of Udacity A/B Testing course final project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Study Notes of Udacity A/B Testing</title>
      <link>https://nancyyanyu.github.io/posts/ab_testing_udacity/</link>
      <pubDate>Fri, 03 Jul 2020 14:24:21 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/ab_testing_udacity/</guid>
      <description>&lt;p&gt;Study note of Udacity &lt;a href=&#34;https://classroom.udacity.com/courses/ud257&#34;&gt;A/B Testing course&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Spark SQL &amp; DataFrame, SparkETL</title>
      <link>https://nancyyanyu.github.io/posts/apache_pyspark-sql-and-dataframe/</link>
      <pubDate>Tue, 09 Jul 2019 14:49:45 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/apache_pyspark-sql-and-dataframe/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Pyspark code of &lt;a href=&#34;https://www.coursera.org/learn/big-data-essentials&#34;&gt;Big Data Essentials: HDFS, MapReduce and Spark RDD&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Apache Spark: Advanced Topics</title>
      <link>https://nancyyanyu.github.io/posts/apache_spark-advanced_topics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/apache_spark-advanced_topics/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Study note of &lt;a href=&#34;https://www.coursera.org/learn/big-data-essentials&#34;&gt;Big Data Essentials: HDFS, MapReduce and Spark RDD&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Apache Spark: Basic Concepts</title>
      <link>https://nancyyanyu.github.io/posts/apache_spark-basic_concepts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/apache_spark-basic_concepts/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Study note of &lt;a href=&#34;https://www.coursera.org/learn/big-data-essentials&#34;&gt;Big Data Essentials: HDFS, MapReduce and Spark RDD&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Study Note: Assessing Model Accuracy</title>
      <link>https://nancyyanyu.github.io/posts/ml-assessing-model-accuracy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/ml-assessing-model-accuracy/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;MSE/Bias-Variance Trade-Off/K-Nearest Neighbors&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Study Note: Bias, Variance and Model Complexity</title>
      <link>https://nancyyanyu.github.io/posts/ml-bias-variance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/ml-bias-variance/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Study Note: Clustering</title>
      <link>https://nancyyanyu.github.io/posts/ml-clustering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/ml-clustering/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;K-Means Clustering/Hierarchical Clustering Algorithm&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Study Note: Comparing Logistic Regression, LDA, QDA, and KNN</title>
      <link>https://nancyyanyu.github.io/posts/ml-comparing-logistic-regression-lda-qda-and-knn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/ml-comparing-logistic-regression-lda-qda-and-knn/</guid>
      <description>&lt;h2 id=&#34;logistic-regression-and-lda-methods-are-closely-connected&#34;&gt;Logistic regression and LDA methods are closely connected.&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Setting&lt;/strong&gt;: Consider the two-class setting with \(p = 1\) predictor, and let \(p_1(x)\) and \(p_2(x) = 1−p_1(x)\) be the probabilities that the observation \(X = x\) belongs to class 1 and class 2, respectively.&lt;/p&gt;
&lt;p&gt;In LDA, from&lt;/p&gt;
&lt;p&gt;$$
\begin{align} p_k(x)=\frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_k)^2 \right)}}{\sum_{l=1}^K\pi_l\frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_l)^2 \right)}} \end{align}
$$&lt;/p&gt;
&lt;p&gt;$$
\begin{align} \delta_k(x)=x\frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log(\pi_k) \end{align}
$$
The &lt;strong&gt;log odds&lt;/strong&gt; is given by&lt;/p&gt;
&lt;p&gt;$$
\begin{align}\log{\frac{p_1(x)}{1-p_1(x)}}=\log{\frac{p_1(x)}{p_2(x)}}=c_0+c_1x \end{align}
$$
where c0 and c1 are functions of μ1, μ2, and σ2.&lt;/p&gt;
&lt;p&gt;In Logistic Regression,&lt;/p&gt;
&lt;p&gt;$$
\begin{align} \log{\frac{p_1}{1-p_1}}=\beta_0+\beta_1x \end{align}
$$&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Study Note: Dimension Reduction - PCA, PCR</title>
      <link>https://nancyyanyu.github.io/posts/ml-dimension_reduction-pca/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/ml-dimension_reduction-pca/</guid>
      <description>&lt;h1 id=&#34;dimension-reduction-methods&#34;&gt;Dimension Reduction Methods&lt;/h1&gt;
&lt;p&gt;Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.&lt;/p&gt;
&lt;p&gt;Dimension Reduction Methods &lt;em&gt;&lt;strong&gt;transform&lt;/strong&gt;&lt;/em&gt; the predictors and then fit a least
squares model using the transformed variables.&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;p&gt;Let $Z_1,Z_2, . . . ,Z_M$ represent $M &amp;lt; p$ linear combinations of our original $p$ predictors. That is,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
Z_m=\sum_{j=1}^p\phi_{jm}X_j
\end{align}
$$&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
