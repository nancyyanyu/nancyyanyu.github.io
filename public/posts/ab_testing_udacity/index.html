<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Study Notes of Udacity A/B Testing | Nancy's Notebook</title><meta name=keywords content="ab testing"><meta name=description content="Study note of Udacity A/B Testing course."><meta name=author content="Me"><link rel=canonical href=https://nancyyanyu.github.io/posts/ab_testing_udacity/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Study Notes of Udacity A/B Testing"><meta property="og:description" content="Study note of Udacity A/B Testing course."><meta property="og:type" content="article"><meta property="og:url" content="https://nancyyanyu.github.io/posts/ab_testing_udacity/"><meta property="og:image" content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-07-03T14:24:21+00:00"><meta property="article:modified_time" content="2020-07-03T14:24:21+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Study Notes of Udacity A/B Testing"><meta name=twitter:description content="Study note of Udacity A/B Testing course."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nancyyanyu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Study Notes of Udacity A/B Testing","item":"https://nancyyanyu.github.io/posts/ab_testing_udacity/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Study Notes of Udacity A/B Testing","name":"Study Notes of Udacity A\/B Testing","description":"Study note of Udacity A/B Testing course.\n","keywords":["ab testing"],"articleBody":"Study note of Udacity A/B Testing course.\nOverview A/B testing is a methodology for testing product changes. You split your users to two groups - the control group which sees the default feature, and an experimental group that sees the new features.\nWhat A/B testing isn’t good for A/B testing is not good for testing new experiences. It may result in change aversion (where users don’t like changes to the norm), or a novelty effect (where users see something new and test out everything).\nThe two things with new experiences is\nhaving a baseline and how much time needs to be allowed for the users to adapt to the new experience, so you can say what is going to be the plateaud experience and make a robust decision. Finally, A/B testing cannot tell you if you are missing something.\nIn these cases, user logs can be used to develop hypothesis that can then be used in an A/B test. A/B testing gives broad quantitiative data, while other techniques such as user research, focus groups, human evaluation give you deep qualitative data\nMetric Choice Click-Through Rate: $\\frac{Number \\ of \\ clicks }{Number \\ of \\ page views}$\nUse a rate when you want to measure the usability (how often do they actually find that button.) Click-Through Probability $\\frac{Unique \\ visitors \\ who \\ click }{Unique \\ visitors \\ to \\ page}$\nUse a probability when you want to measure the total impact (how often users went to the second level page on your site) We’re interested in whether users are progressing to the second level of the funnel, which is why we picked a probability.\nHow to compute rate \u0026 probability ? Rate: on every page view you capture the event, and then whenever a user clicks you also capture that click event\nSum the page views, you sum the clicks and you divide. Probability: match each page view with all of the child clicks, so that you count, at most, one child click per page view.\nReview Distribution Binomial Distribution For a binomial distribution with probability $p$ , the mean is given by $p$ and the standard deviation is $\\sqrt{p \\times (1−p)/N}$ where $N$ is the number of trials.\nA binomial distribution can be used when\nThe outcomes are of 2 types Each event is independent of the other Each event has an identical distribution (i.e. $p$ is the same for all) We expect click-through probability to follow a binomial distribution\nConfidence Interval Click or non-click\n$x$: # of users who clicked\n$N$: # of users\n$\\hat{p}=\\frac{x}{N}$ : estimate of probability\n$\\hat{p}=\\frac{100}{1000}=0.1$ To use normal: check $N \\cdot \\hat{p}\u003e5$ and $N \\cdot (1-\\hat{p})\u003e5$\n$z$- distribution: standard normal distribution with $\\mu=0$ and $\\sigma=1$\nWith 95% confidence, the true value would be within $1.96 $ and $-1.96$ $m=z \\cdot SE=z \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N}}$: margin of error\n$m=0.019$, Confidence interval: $[0.081,0.119]$ This means, if you’d run the experiment again with another 1000 page views, you’d maybe expect to see between 80 and 120 clicks, but more or less than that would be pretty surprising.\nStandard deviation of binomial If you look up a binomial distribution elsewhere, you may find that it has a mean of $np$ and a standard deviation of $\\sqrt{np(1-p)}$. This is for a binomial distribution defined as the total number of successes, whereas we will use the fraction or proportion of successes throughout this class. In this cas, the mean is $p$ and standard deviation is $\\sqrt{\\frac{p(1-p)}{n}}$.\nUseful equations You may find these equations helpful in solving the quiz: p_hat = X/N SE = sqrt(p_hat (1-p_hat) / N) m = z* SE\nHypothesis Testing null hypothesis: there’s no difference in click-through probability between our control, and our experiment.\nalternative hypothesis: whether the click-through rate is different? Or it’s higher, or lower? Or are we interested in any kind of difference at all?\nTwo-tailed vs. one-tailed tests Two-tailed: The null hypothesis and alternative hypothesis allows you to distinguish between three cases:\nA statistically significant positive result A statistically significant negative result No statistically significant difference. one-tailed: allows you to distinguish between two cases:\nA statistically significant positive result No statistically significant result Which one you should use depends on what action you will take based on the results. If you’re going to launch the experiment for a statistically significant positive change, and otherwise not, then you don’t need to distinguish between a negative result and no result, so a one-tailed test is good enough. If you want to learn the direction of the difference, then a two-tailed test is necessary.\nComparing two samples - Pooled Standard Error For comparing two samples, we calculate the pooled standard error. For e.g., suppose $Xcont$ and $Ncont$ are the control number of users that click, and the total number of users in the control group. Let $X_{exp}$ and $N_{exp}$ be the values for the experiment.\nThe pooled probability is given by\n$\\hat{p_{pool}}=\\frac{X_{cont}+X_{exp}}{N_{cont}+N_{exp}}$\n$$SE_{pool}=\\sqrt{\\hat{p_{pool}}∗(1−\\hat{p_{pool}})∗(\\frac{1}{N_{cont}}+\\frac{1}{N_{exp}})}$$\n$$\\hat{d}=\\hat{p_{exp}}−\\hat{p_{cont}}$$\n$$H_0:d=0 \\ where \\ \\hat{d} ∼N(0,SE_{pool})$$\n$d$ is practical significance boundary. If $\\hat{d} \u003e1.96∗SE_{pool}$ or $ \\hat{d}\u003c−1.96∗SE_{pool}$ then we can reject the null hypothesis and state that our difference represents a statistically significant difference\nPractical or Substantive Significance Practical significance is the level of change that you would expect to see from a business standpoint for the change to be valuable.\nThe differences in the magnitudes for what’s consider practically significant can be quite different. What you really want to observe is repeatability.\nStatistical significance is about repeatability. And you want to make sure when you setup your experiment that you get that guarantee that yes, these results are repeatable so it’s statistically significant.\nThe statistical significance bar is often lower than the practical significance bar, so that if the outcome is practically significance, it is also statistically significant.\nSize vs Power trade-off Statistical power: given that we have control over how many page views go into our control and our experiment, we have to decide how many page views we need in order to get a statistically significant result. -\u003e determine the number of data points needed to get a statistically significant result.\nPower has an inverse trade-off with size. The smaller the change you want to detect or the increased confidence you want to have in the result, means you have to run a larger experiment.\nAs you increase the number of samples, the confidence interval moves closer to the mean.\n$$α=P(reject\\ null | null\\ true)$$ - Falsely concluding there is a difference.\n$$β=P(fail\\ to\\ reject\\ null | null\\ false)$$ - Falsely concluding there is no difference\n$1−β$ is referred to as the sensitivity of the experiment, or statistical power. In general, you want your experiment to have a high level of sensitivity at the practical significance boundary. People often choose high sensitivity, typically around 80%.\nFor a small sample,\n$α$ is low (you are unlikely to launch a bad experiment) $β$ is high (you are likely to fail to launch an experiment that actually did have a difference you care about). For a large sample,\n$α$ remains the same $β$ is lower (i.e. sensitivity increases). As you change one of the parameters, your sample size will change as well.\nIf you increase the baseline click through probability (under 0.5) then this increases the standard error, and therefore, you need a higher number of samples If you increase the practical significance level ($d_{min}$), you require a fewer number of samples since larger changes are easier to detect If you increase the confidence level ($1-\\alpha$), you want to be more certain that you are rejecting the null. At the same sensivitiy, this would require increasing the number of samples If you want to increase the sensitivity ($1-\\beta$), you need to collect more samples Analyze Results N_cont = 10072 # Control samples (pageviews) N_exp = 9886 # Test samples (pageviews) X_cont = 974 # Control clicks X_exp = 1242 # Exp. clicks p_pool = (X_cont + X_exp)/(N_cont+N_exp) se_pool = sqrt(p_pool*(1-p_pool)*(1/N_cont + 1/N_exp)) p_cont = X_cont/N_cont p_exp = X_exp/N_exp d_hat = p_exp - p_cont # d_hat = 0.02892847 m = 1.96*se_pool cf_min = d_hat-m cf_max = d_hat+m d_min = 0.02 # Minimum practical significance value for difference # cf_min = 0.0202105 # cf_max = 0.03764645 Since the minimum confidence limit is greater than 0 and the practical significance level of 0.02, we conclude that it is highly probable that click through probability is higher than 0.02 and is significant. Based on this, one would launch the new version.\nChoosing and Charaterizing Metrics :)\nMetric Definition Two use cases of metrics:\nInvariant checking (sanity checking): Metrics that shouldn’t change between your test and control Do you have the same number of users across the two? Is the distribution the same? Evaluation: High level business metrics: how much revenue you make, what your market share is, how many users you have Detailed metrics: user experience with the product How to make a definition:\nA high level concept for a metric/ one sentence summary: “active users”, “click-through probability” Nitty gritty details: How do you define what active is? Which events count towards activity? Summarize individual data measurements into a single metric: a sum or a count, an average For evaluation, you can choose either one metric or a whole suite of metrics.\nOverall Evaluation Criterion (OEC): a term that Microsoft uses for when they come up with a weighted function that combines all of these different metrics.\nHard to define and get everyone to agree Over-optimize Hard to explain why is it moving How generally applicable the metric is:\nIf you’re running a whole suite of AB tests, then ideally you’d have one or more metrics that you can use across the entire suite. If you are running a suite of A/B tests, it is preferable to have a metric that works across the entire suite. It’s much better to use a metric that’s less optimal than it is to come up with the perfect metric for your test.\nRefining the Customer Funnel User funnel indicates a series of steps taken by users through the site. It is called a funnel because every subsequent stage has fewer users than the stage above.\nEach stage is a metric:\nCount: No. of users who reach that point (keep in certain stages)\nRate: better for usability test. You want to increase rate of progression\nProbability: progression. A unique user progressed down the funnel\nDefining Metrics: Other Techniques External Data: great for brainstorming, think of new metrics idea; good for validating metrics; benchmark your own metrics against the industry; help you develop validation techniques Companies that collect data (e.g. Comscore, Nielsen) Companies that conduct surveys (e.g. Pew) Academic papers Internal Data: Retrospective analysis: Look at historic data to look at changes and see the evaluation Good to get a baseline and help you develop theories Surveys and User experience research: help you develop ideas on what you want to research The problem of these studies: show you correlation, not causation\nGathering Additional Data User Experience Research (UER):\nhigh depth on a few users.\ngood for brainstorming.\ncan use special equipment in a UER (e.g. eye movement camera)\nX validate the results (retrospective analysis)\nFocus groups:\nMedium depth and medium # of participants.\nGet feedback on hypotheticals\nX may run into the issue of groupthink\nSurveys:\nlow depth but high # of participants\nUseful for metrics you cannot directly measure.\nX Can’t directly compare with other metrics since population for survey and internal metrics may be different.\nApplying Other Techniques on Difficult Metrics Difficult Metrics:\nDon’t have access to data Takes too long Rate of returning for 2nd course\nTakes too long Survey -\u003e proxy Average happiness of shoppers\nDon’t have access to data Survey; UER - brainstorm Probability of finding info via search\nDon’t have access to data External data; UER; Human raters Possible proxies: time spent; clickes on result; follow up queries Metric definitions \u0026 Data Capture Def #1 (Cookie probability): For each , number of cookies that click divided by number of cookies\nDef #2 (Pageview probability): Number of pageviews with a click within divided by number of pageviews\nDef #3 (Rate): Number of clicks divided by number of pageviews\nFiltering and Segmenting Good for evaluating definitions and building intuitions\nYou may have to filter out spam and fraud to de-bias the data. You do want to be careful you don’t introduce bias into your data by doing the filtering.\nOne way to figure out if you are biasing or de-biasing the data by filtering, is to slice your data by country, or by language, or by platform, and then calculate the metric for each slice after filterig. If you are affecting any slide disproportionately, then you may be biasing your data with filtering\nTo remove any weekly effects when looking say at total active cookies over time, use week-over-week i.e. divide current data by data from a week ago. Alternately, one can use year-over-year.\nSummary Metrics Summarize all of these individual data measurements into a single summary metric.\nCharacteristics for your metric:\nthe sensitivity and robustness: You want your metric to be sensitive enough, in order to actually detect a change when you, when you’re testing your possible future options, the distribution: The most ideal way of doing this is to do a retrospective analysis to compute a histogram. 4 categories of summary metrics:\nSums and counts: # of users who visited\nDistributional metrics: the means, the medians, the 25th, the 75th and 90th percentiles.\nProbabilities and rates.\nRatios: can compute a whole range of different business models, and various different things that you may care about, but they can be very difficult to characterize.\nSensitivity and Robustness Whether the metric is sensitive to changes you care about, and is robust to changes you don’t care about\nmean is sensitive to outliers - NOT robust median is robust but not sensitive to changes to small group of users How to measure Sensitivity and Robustness:\nA/A tests to see if the metric picks up any spurious differences Using prior experiments to see if the metric moves in a way that intuitively make sense. Retrospective analysis of log data: look back at changes you made to your website and see if the metrics you’re interested in actually moved in conjunction with chose changes. Absolute vs. Relative difference Suppose you run an experiment where you measure the number of visits to your homepage, and you measure 5000 visits in the control and 7000 in the experiment. Then the absolute difference is the result of subtracting one from the other, that is, 2000. The relative difference is the absolute difference divided by the control metric, that is, 40%.\nIf you are running a lot of experiments you want to use the relative difference i.e the percentage change.\nThe main advantage : you only have to choose one practical significance boundary to get stability over time rather than change it as the system changes. The main disadvantage : variability, relative differences such as ratios are not as well behaved as absolute differences Relative differences in probabilities For probability metrics, people often use percentage points to refer to absolute differences and percentages to refer to relative differences. For example, if your control click-through-probability were 5%, and your experiment click-through-probability were 7%, the absolute difference would be 2 percentage points, and the relative difference would be 40 percent. However, sometimes people will refer to the absolute difference as a 2 percent change, so if someone gives you a percentage, it’s important to clarify whether they mean a relative or absolute difference!\nVariability We want to check the variability of a metric to later determine the sizing of the experiment and to analyze confidence intervals and draw conclusions. If we have a metric that varies a lot, then the practical significance level that we are looking for may not be feasible.\nTo calculate the confidence interval, you need\nVariance (or standard deviation) Distribution Binomial distribution:\n$$SE=\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N}}$$\n$$m=z^*SE$$\nWe use the fact that this was a binomial distribution in two ways.\nwe use the fact that this was a binomial distribution to get this formula for the standard error.\nthis formula for the margin of error depends on the assumption that this is a normal distribution, as the binomial approaches a normal distribution as N gets larger.\nType of metric Distribution Estimated variance Probability Binomial (normal) $\\frac{\\hat{p}(1-\\hat{p})}{N}$ Mean Normal $\\frac{\\hat{\\sigma}^2}{n}$ ($\\hat{\\sigma}$ : variance of the sample) Medium/percentile Depends on the assumption of data distribution Depends Count/difference Normal (maybe) $Var(X)+Var(Y)$ Rates Poisson $\\bar{X}$ (mean) Ratios ($\\frac{\\hat{p_{exp}}}{\\hat{p_{cont}}}$ instead of $\\hat{p_{exp}}-\\hat{p_{cont}}$) Depends Depends The variance of the actual metric: if you were to collect a new sample, how would you expect this metric to vary?\nThe variance of the sample: take each of your data points and then collect the variance of them.\nCalculating CI for a Mean\nimport math N= [87029, 113407, 84843, 104994, 99327, 92052, 60684] N_mean=sum(N)/len(N) N_std=math.sqrt(sum([(n-N_mean)**2 for n in N])/(len(N)-1)) SE=N_std/math.sqrt(len(N)) cf_min,cf_max=N_mean-SE*1.96,N_mean+SE*1.96 Non-parametric methods A way to analyze the data without making an assumption about what the distribution Is.\nsign test Compute variance empirically Reason of using Non-parametric methods:\nfor more complicated metrics, you might need to estimate the variance empirically instead of computing it analytically. At Google, it was observed that the analytical estimates of variance was often under-estimated, and therefore they have resorted to use empirical measurements based on A/A test to evaluate variance. -\u003e using A versus A experiments across the board to estimate the empirical variability of all of our metrics.\nWhat are A versus A experiments: in an A versus A test, what you have is a control, A against another control A, and so there’s actually no change in what the users are seeing. What that means that any differences that you measure are due to the underlying variability, maybe of your system, of the user population, what users are doing, all of those types of things.\nIf you see a lot of variability in a metric in an A versus A test, it’s probably too sensitive to be useful in, in evaluating a real experiment.\nHow many A/A tests are needed to get a good sense?: The key rule of thumb to keep in mind is that the standard deviation is going to be proportional to the square root of the number of sample.\nWhat if you can’t run many A/A tests for some reason?: another option is to run one really big A versus A experiment. And then using bootstrap, where what you do is you take that big sample, and you randomly divvy it up into a bunch of small samples and you do the comparison within those random subsets.\nOne advantage of running the lots of different A/A tests is because if your experiment system is itself complicated, it’s actually a very good test of your system.\nUses of A/A tests:\nCompare result to what you expect (sanity check) Estimate variance empirically and use your assumption about the distribution to calculate confidence Directly estimate confidence interval without making any assumption of the data Calculating a Confidence Interval Empirically\n# click-through-probability of 40 A/A tests or bootstrap samples import math group1=[0.02, 0.11, 0.14, 0.05, 0.09, 0.11, 0.09, 0.1 , 0.14, 0.08, 0.09, 0.08, 0.09, 0.08, 0.12, 0.09, 0.16, 0.11, 0.12, 0.11, 0.06, 0.11, 0.13, 0.1 , 0.08, 0.14, 0.1 , 0.08, 0.12, 0.09, 0.14, 0.1 , 0.08, 0.08, 0.07, 0.13, 0.11, 0.08, 0.1 , 0.11] group2=[0.07, 0.11, 0.05, 0.07, 0.1 , 0.07, 0.1 , 0.1 , 0.12, 0.14, 0.04, 0.07, 0.07, 0.06, 0.15, 0.09, 0.12, 0.1 , 0.08, 0.09, 0.08, 0.08, 0.14, 0.09, 0.1 , 0.08, 0.08, 0.09, 0.08, 0.11, 0.11, 0.1 , 0.14, 0.1 , 0.08, 0.05, 0.19, 0.11, 0.08, 0.13] confidence_level=0.95 # assume metric is normally distributed difference=[i-j for i,j in zip(group1,group2)] mean=sum(difference)/len(difference) SD=math.sqrt(sum([(i-mean)**2 for i in difference ])/(len(difference)-1)) # m=SD*z-score m=SD*1.96 ci_max=mean+m ci_min=mean-m \u003e ci_min,ci_max \u003e (-0.06702773846019527, 0.07552773846019528) # no assumption of metric distribution difference=sorted(difference) ci_min,ci_max=difference[1],difference[-2] \u003e ci_min,ci_max \u003e (-0.06000000000000001, 0.08) In summary, different metrics have different variability. The variability may be high for certain metrics which makes them useless even if they make business or product sense. Computing the variability of a metric is tricky and one needs to take a lot of care.\nFor a lot of analysts, a majority of the time is spent is validating and choosing a metric compared to actually running the experiment. Being able to standardize the definitions was critical in the test. When measuring latency, are you talking about when the first byte loads and when a last byte loads. Also, for latency, the mean may not change at all. The signals (e.g. slow/fast connections or browsers) causes lumps in the distribution, and no central measure works. One needs to look at the right percentile metric. The key thing is that you are build\nDesign an Experiment Choose subject: What are the units in the population you are going to run the test on? (unit of diversion) Choose population: What population are you going to use (US only?) Size Duration Unit of Diversion Commonly used units of diversion are:\nUser identifier (id): Typically the username or email address used on the website. It is typically stable and unchanging. If user id is used as a unit of diversion, then it is either in the test group or the control group. User ID is personally identifiable Anonymous id: This is usually an anonymous identifier such as a cookie. It changes with browser or device. People may often refresh their cookies every time they log in. It is difficult to refresh a cookie on an app or a phone compared to the computer. Event: An event is a page load that can change for each user. This is used typically for non-user-visible changes. Less common:\nDevice id: Typically available for mobile devices. It is tied to a specific device and cannot be changed by the user. Personally identifiable. IP address: location specific, but may change as the user changes location (e.g. testing on infrastructure change to test impact on latency) 3 main considerations in selecting an appropriate unit of diversion:\nConsistency Ethical - Informed consent: a issue when using user id. Variability Consistency of Diversion If you’re testing a change that crosses the sign in, sign out border, then a user ID doesn’t work as well, use cookie instead.\nFor user visible changes, you would definitely use a cookie or a user ID. But there are lots of non-user-visible changes (latency changes, back-end infra changes or ranking changes):\nExamples:\nCookie: change button color and size (distracting if changes on reload; different look on difference devices is ok) User id: Add instructors note before quizzes (cross device consistency important) Event: Reducing video load time; change order of search results (users won’t probably notice) IP based diversion not very useful:\nnot as consistent as cookie or user id as ip randomly change depending on the provider; no clean randomization like event based diversion. only choice: testing out one hosting provider versus a different hosting provider to understand the impact of latency. May not get a clean comparison between your experiment and your control:some providers aggregate all of those modem dialup users into a single IP address, so it’s hard to find comparable population of users in the control group. Unit of Analysis v.s. Unit of Diversion Variability is higher when it is calculated empirically than when calculated analytically. This is because the unit of analysis (i.e. the denominator in the metric. e.g. page view in click through rate) is different from the unit of variability.\nWhen your unit of diversion is also a page view, so as would be the case in an event base diversion, then the analytically computed variability is likely to be very close to the empirically computed variability.\nIf, however, your unit of diversion is a cookie or a user id then the variability of your same metric click through rate is actually going to be much higher.\nThis is because when you’re actually computing your variability analytically, you’re fundamentally making an assumption about the distribution of the data and what’s considered to be independent.\nWhen you’re doing event-based diversion every single event is a different randomdraw, and so your independence assumption is actually valid.\nWhen you’re doing cookie or user ID based diversion, that independence assumption is no longer valid because you’re actually diverting groups of events. And so they’re actually correlated together.\nMeasure variablity of a metric:\nUnit of diversion: queries (event-based) v.s. cookies\nMetric: coverage (the percentage of queries for which an ad is shown) = $\\frac{\\text{number of queries with ads}}{\\text{number of queries}}$\nUnit of analysis: query\nBinomial: $SE=\\sqrt{\\frac{p(1-p)}{N}}$\nThe SE of coverage under cookies diversion is much larger then that of queries.\nWhen unit of analysis = unit of diversion, variability tends to be lower and closer to the anlytical estimate.\nChoose Population intra-user experiment: expose the same user to this feature being on and of over time, and you actually analyze how they behave in different time windows.\npitfalls: be really careful that you choose a comparable time window.(not 2 weeks before Christmas) different behaviors as a result of a frustration or a learning problem, where people learn to use the particular feature in the first two weeks interleaved experiment: expose the same user to the A and the B side at the same time. And typically this only works in cases where you’re looking at reordering a list.\ninter-user experiments: different people on the A side and on the B side.\nTarget Population Some easy divisions of your user space: what browser they’re on, what geo location they come from, what country, what language they’re using.\nReasons why you might make that decision in advance:\nFor a feature not sure if you’re going to release it and it’s a pretty high profile launch, you might want to restrict how many of your users have actually seen it. If you’re running a couple of different experiments at your company at the same time, you might not want to overlap. You may not want to dilute the effect of your experiment across a global population. So if you’re analyzing an experiment for the first time, and it only affects English, you may want to actually do your analysis specific on English, Targeting an Experiment:\nFiltering could also affect variability\n# New Zealand N_cont = 6021 X_cont = 302 N_exp = 5979 X_exp = 374 p_cont=X_cont/N_cont p_exp=X_exp/N_exp p_pool=(X_cont+X_exp)/(N_cont+N_exp) SE=math.sqrt((p_pool*(1-p_pool)*(1/N_cont+1/N_exp))) d_hat=p_exp-p_cont m=1.96*SE \u003e m,d_hat \u003e (0.00825068746366646, 0.012394485165776618) # Global N_cont = 50000 + 6021 X_cont = 2500 + 302 N_exp = 50000 + 5979 X_exp = 2500 + 374 p_cont=X_cont/N_cont p_exp=X_exp/N_exp p_pool=(X_cont+X_exp)/(N_cont+N_exp) SE=math.sqrt((p_pool*(1-p_pool)*(1/N_cont+1/N_exp))) d_hat=p_exp-p_cont m=1.96*SE \u003e m,d_hat \u003e (0.0025691881506085417, 0.0013237234004343165) Population v.s. Cohort Cohort: people who enter the experiment at the same time. You define an entering class and you only look at users who entered your experiment on both sides around the same time, and you go forward from there.\nCohorts are harder to analyze, limit your experiment to a subset of population, can affect variability .\nWant to use cohort instead of population:user stability\nLooking for learning effects Examining user retention Want to increate user activity Anything that require users to be established Experiment Design and Sizing How to reduce the size of experiments:\nExperiment: change order of courses in the course list\nMetric: click-through-rate\nUnit-of-diversion: cookie\n$d_{min}=0.05$, $\\alpha=0.05$, $\\beta=0.2$, $SE=0.0628$ for 1000 page views\nResult: need 300,000 pageviews per group!\nWhich strategies could reduce the number of page views?\nIncrease $d_{min}$, $\\alpha$, or $\\beta$ Change unit of diversion to page view Makes unit of diversion same as unit of analysis The variability of the metric might decrease and thus decrease the number of page views you need to be confident in your results Target experiment to specific traffic Non-english traffic will dilute the results Could impact choice of practical significance boundrey since you’re only looking at a subset of the traffic, you might need a bigger change before it matters to the business; Or since your variablity is probably lower, you might want to detect smaller changes rather than decreasing the number of page views. How are you actually going to detect impact? What effect does that have on the size of the experiment when you really don’t know what fraction of your population is going to be affected?\nrun a pilot where you turn on the experiment for a little while and see who’s affected, or you can even just use the first day or the first week of data to try to get a better guess at what fraction of your population you’re really looking at. Duration v.s. Exposure Practical considerations in experimental design:\nDuration When to run the experiment Fraction of the traffic to send to the experiment The duration of your experiment, is related to the proportion of traffic that you’re sending through your experiment.\nWhy not run on all of the traffic to get results quicker?\nSafety: not sure if it’s working on all browser or how the users will react Press: people blogging about features you might not keep Other sources of variablity : holiday, weekly variation Running multiple tasks at your company: to be comparable, run them at the same time on smaller percentages of traffic. Note: There is weekly variation in traffic and metric, so it’s better to run on mix of weekend and weekday days. For risky change, run longer with less traffic.\nLearning Effects learning effects is basically when you want to measure user learning. Or effectively whether a user is adapting to a change or not.\nTwo different types of learning effects\nChange aversion “I don’t like anything.” Knowledge effect “Let me try everything around.” When users first encounter a change they will react in one of these two ways, but will eventually plateau to a very different behavior.\nConsiderations when measure a learning effect:\nThe key issue with trying to is time： It takes time for you just to actually adapt to a change and often times you don’t have the luxury of taking that much time to make a decision.\nChoosing the unit of diversion correctly. - a cookie or a user ID\nDosage: because a lot of the learning is based on not just a slight time but how often they see the change. Then you probably want to be using a cohort as opposed to just a population. - choose a cohort in both the experiment and the control based on either how long they’ve been exposed to the change or how many times they’ve seen it.\nrisk and duration: both of those mean to run it through a small proportion of your users for a longer period of time.\npre-periods and post-periods:\nBefore run your A/B test, you’re on a pre-period on the exact same populations but they’re receiving the exact same frequence. It’s an A/A test on the same set of users.\nIn the pre-period, if you measure any difference between your experiment and your control populations that difference is due to something else (system variability, user variability).Pre-period helps you know that any difference that you measure in your experiment and control is due to the experiment, and not due to any preexisting and inherent differences in your population.\nA post-period is saying, after I run my experiment, my control, I’m going to run another A versus A test. And then, what, what we can say is that if there are any differences in the experiment and the control populations after I’ve run my experiment, then I can attribute those differences to user learning that happened in the experiment period.\nAnalyze Results Sanity Checks 2 main types of checks:\nPopulation sizing metrics based on your unit of diversion\nCheck your experiment population and your control populations are actually comparable. Actual invariants: metrics that are same in the control and experiment groups and shouldn’t change when you run your experiment\nStep 1: Choosing invariant metircs\nStep 2: Checking invariants\nRun experiment for 2 weeks ​\tUnit of diversion: cookie ​\tTotal control: 64454 ​\tTotal experiment: 61818 ​\tHow would you figure out if the difference is within expectations? ​\tGiven: each cookie is randomly assigned to the control or experiment group with a prob of 0.5. 1. Compute SD of binomial with $p=0.5$ of success. $SD=\\sqrt{0.5 \\cdot0.5/(64454+61818)}=0.0014$ 2. Multiple by z-score to get margin of error. $m=1.96*SD=0.0027$ 3. Compute confidence interval around 0.5. $ [0.4973, 0.5027]$ 4. Check whether observed fraction is within interval. $64454/(64454+61818)=0.5104$ Not within interval Step 3: what to do\nTalk to engineers Try slicing to see if one particular slice is weird Check age of cookies - does one group has more new cookies? What happens if one of your sanity checks fails: analyzing why your sanity checks fail\nwork with your engineers to understand is there something going on with the experiment infrastructure, experiment set up, experiment diversion.\nretrospective analysis: Try and recreate experiment diversion from the data capture,\nuse pre \u0026 post periods\nIf you’re in a pre-period,did I see the same changes in those invariance in my pre-period? If I saw them in the pre-period and the experiment, that points to a problem with the experiment infrastructure, the set up, something along those lines. if you see the change only in your experiment but not in the pre-period, that points to something with the experiment itself (data capture or something along those lines). The most common reasons for data not matching up:\ndata capture: Maybe the change triggers very rarely, and you capture it correctly in the experiment, but you don’t capture quickly in the control. experiment’s set up: Maybe you set up the filter for the experiment, but not the control. experiment system The key thing : if there really is a learning effect, then not very much change in the beginning, but it’s increasing over time.\nNot a learning effect: a big change from the beginning\nSingle Metric Goal: make a business decision about whether your experiment has favorably impacted your metrics. Analytically, decide if you’ve observed a statistically significant result of your experiment.\nHow do you decide if the change was statistically significant?:\nCharacterizing the metric, understanding how it behaves Use variability to estimate how long we needed to run the experiment for and size our experiment appropriately Use the results of last 2 steps to estimate the variability we need to analyze the A/B experiment What if our results are not statistically significant?:\nBreak down the result into different platforms, different days of the week -\u003e find bugs in your experiment setup \u0026 give you a new hypothesis about how people are reacting to the experiment. Cross checking your results with other methods, like non parametric sign tests to compare the results to what you got from your parametric hypothesis test. Analysis with Single Metric Experiment I : change color and place of ‘start now’ button.\nMetric: click-through-rate\nUnit of diversion: cookie\n$X_{cont}=352$, $N_{cont}=7370$\n$X_{exp}=565$, $N_{exp}=7270$\n$d_{min}=0.01$, $\\alpha=0.05$, $\\beta=0.2$\nEmpirical SE: $0.0035$ with 10000 page views in each group\n$$ \\begin{align} \u0026SE \\sim \\sqrt{\\frac{1}{N_1}+\\frac{1}{N_2}} \\ \u0026\\frac{0.035}{\\sqrt{\\frac{1}{10000}+\\frac{1}{10000}}}=\\frac{SE}{\\sqrt{\\frac{1}{7370}+\\frac{1}{7270}}} \\ \u0026SE=0.0041 \\ \u0026\\hat{d}=\\hat{r}{exp}-\\hat{r}{cont}=\\frac{565}{7270}-\\frac{352}{7370}=0.03 \\ \u0026m=1.96*0.0041=0.0080 \\ \u0026Confidence\\ Interval:[0.022,0.038] \\end{align} $$ Recommandation: launch\nSign Test:\n​\tNumber of days: 7\n​\tNumber of days with positive change: 7\n​\tIf no difference, 50% chance of positive change each day\n​\t(7 days too short, cannot assume normal by binomial)\n​\tTwo-tailed p-value: 0.0156 (the prob of observing a result at least this extreme by chance)\n​\t- Since this is less than the chosen alpha of .05, the sign test agrees with the hypothesis test that this result is unlikely to happen by chance.\n​\tRecommandation: launch\nExperiment II :\nMetric: click-through-rate\n$d_{min}=0.01$, $\\alpha=0.05$,\nEmpirical SE: $0.0062$ with 5000 page views in each group\nEffect size: Xs_cont = [196, 200, 200, 216, 212, 185, 225, 187, 205, 211, 192, 196, 223, 192] Ns_cont = [2029, 1991, 1951, 1985, 1973, 2021, 2041, 1980, 1951, 1988, 1977, 2019, 2035, 2007] Xs_exp = [179, 208, 205, 175, 191, 291, 278, 216, 225, 207, 205, 200, 297, 299] Ns_exp = [1971, 2009, 2049, 2015, 2027, 1979, 1959, 2020, 2049, 2012, 2023, 1981, 1965, 1993] SE_emp=0.0062 Ns_emp=5000 SE=SE_emp/math.sqrt(1/Ns_emp+1/Ns_emp)*math.sqrt(1/sum(Ns_cont)+1/sum(Ns_exp)) d_hat=sum(Xs_exp)/sum(Ns_exp)-sum(Xs_cont)/sum(Ns_cont) m=1.96*SE ci_min,ci_max=d_hat-m,d_hat+m \u003e ci_min,ci_max \u003e (0.006465853496236934, 0.016736185710796242) Since the confidence level is larger than 0, it’s statistically significant.\nSign test: ctr_cont=[i/j for i,j in zip(Xs_cont,Ns_cont)] ctr_exp=[i/j for i,j in zip(Xs_exp,Ns_exp)] days=len(Xs_cont) positive_days=sum([exp\u003econt for exp,cont in zip(ctr_exp,ctr_cont)]) \u003e days,positive_days \u003e (14, 9) The two-tail P value is $0.4240$. Since p-value is larger than alpha, it’s not significant.\nSingle Metric: Gochas What if the sign test and the hypothesis test don’t agree?\nStatistical reasons for counter-intuitive results: Simpson’s paradox\nDifferent subgroups in the data like user populations. Within each subgroup, the results are stable but when aggregated together, it’s the mix of subgroups that actually drives your result.\nNew users might be correlated with weekend use, experienced users who react differently are correlated with weekday use. Within each group, their behavior is stable. What drives the results of your experiment are how many people from each group. Multiple Metrics What changes when you have multiple evaluation metrics instead of just one?\nAs you test more metrics, it becomes more likely that one of them will show a statistically significant result by chance.\nBut the significant result shouldn’t be repeatable. If you did the same experiment on another day or you slices or do some bootstrap analysis, you wouldn’t see the same metric showing up as significant differences every time, it should occur randomly.\nMultiple comparisons: adjusts your significance level, so that it accounts for how many metrics or how many different tests you’re doing.\nDo it when you want automatic alerting that tells you one of my metrics was significantly different on this experiment. Tracking Multiple Metrics For 3 independent metrics, $\\alpha=0.05$, the chance of at least 1 false positive is $P(FP≥1)=1-0.95^3=0.143$\n$$\\text{Overall } \\alpha=1-(1-\\alpha)^{\\text{number of metrics}}$$\nProblem: Probability of any False Positive increases as you increase number of metrics\nSolution: Use higher confidence level for each metric\nMethod1: Assume independence of metrics ​\t$\\alpha_{overall}=1-(1-\\alpha_{individual})^n$, calculate $\\alpha_{individual}$ with specified $\\alpha_{overall}$\nMethod2: Bonferroni Correction Simple\nNo assumption\nConservative - guarantee to give $\\alpha_{overall}$ at lease as small as specified\n$\\alpha_{individual}=\\frac{ \\alpha_{overall}}{n}$\nProblem: Given metrics are correlated, and tend to move at the same time, this method is too conservative.\nRecommendation:\nRigorous answer: use a more sophisticated method like closed testing procedure, the Boole-Bonferroni bound, and the Holm-Bonferroni method In practice: Judgment Call, possibly based on business strategy Different Strategies:\nControl probability that any metric shows a false positive $\\alpha_{overall}$ -\u003e family wise error rate (FWER) (Overal alpha)\nControl false discovery rate (FDR):\n$$FDR=E[\\frac{ \\text{false positives} }{ \\text{rejections}}]$$\nOut of all of the rejections of the null, that is, all of the metrics that you declare to have a statistically significant difference. How many of them had a real difference as opposed to how many were false positives? This really only makes sense if you have a huge number of metrics, say hundreds. e.g.: Suppose that you have 200 metrics that you’re measuring and you capped the false discovery rate at 0.05. What this means is that you’re okay with having 5 false positives and 95 true positives in every experiment. The family wise error rate, or the overall alpha in this case, would be one, since you have at least one false positive every time. But the false discovery rate is 0.05.\nAnalyzing Multiple Metrics How do I actually make a recommendation?\nYou really have to understand how people are reacting to the change because you can’t quantitatively evaluate which one is better.\nAn alternative to using multiple metrics is to use an ‘Overall Evaluation Criterion’ (OEC).\nHow to choose a good OEC?\nStart with some kind of business analysis (Our company, as a whole, wants to look at 25% revenue plus 75% increase usage of the site.) Run a whole bunch of different experiments and validate how they steer you. Having an OIC doesn’t have to be a formal number. It’s really just trying to encapsulate what your company cares about. And how much you’re going to be balancing something like stay time and clicks.\nDrawing Conclusion If you have statistically significant results, now the questions come down to\nDo you understand the change?\nDo you want to launch the change?\nWhat if your change has a positive impact on one slice of your users, but for another slice, there’s no impact or there’s a negative impact?\nIs it a question about having different users, and how much they like or don’t like the change? Have you seen that effect in other experiments? Do you have a bug? How do you decide whether to launch your change or not?\nDo I have statistically significant and practically significant results in order to justify the change?\nDo I understand what that change has actually done with regards to user experience?\nIs it worth it?\nThe end goal is actually making that recommendation that shows your judgment.\nGochas: Changes Over Time It’s good practice to always do a ramp-up when you actually want to launch a change.\nThe other thing is to remove all of the filters. So if you’re only testing your change on English, for example, you want to test your change during your ramp-up on all users to check if there’s been any incidental impact to unaffected users that you didn’t test in the original experiment.\nComplication: the effect may actually flatten out as you ramp up the change.\nReasons of the effects not being repeatable:\nseasonality effects, holidays Capture seasonal / event-driven impacts: holdback\nYou launch your change to everyone except for a small holdback (a set of users), that don’t get the change, and you continue comparing their behavior to the control. Now, in that case you should see the reverse of the impact that you saw in your experiment. And what you can do is you can track that over time until you’re confident that your results are actually repeatable. novelty effect or change aversion: as users discover or adopt your change, then their behavior can change and therefore the measured effects can change. Cohort analysis If you don’t control for the budgets properly, the effect can change as you ramp up. Summary Check that your experiment was set up properly, Look at your end variance.Check that your experiment metrics are actually looking sane. Remember you aren’t just looking for statistical significance, you’re really making a business decision. Can’t actually forget the overall business analysis Judgment call with regards to the user experience and the business: What’s the engineering cost of maintaining the change? Are there customer support or sales issues? What’s the opportunity cost if you actually choose to launch the change relative to the rewards you’re going to get from the change or potentially not launching the change? Policy and Ethics for Experiments Four Principles 1. Risk In the study, what risk is the participant undertaking? The main threshold is whether the risk exceeds that of “minimal risk”. Minimal risk is defined as the probability and magnitude of harm that a participant would encounter in normal daily life.\n2. Benefits It is important to be able to state what the benefit would be from completing the study.\n3. Alternatives In online experiments, the issues to consider are what the other alternative services that a user might have, and what the switching costs might be, in terms of time, money, information, etc.\n4. Data Sensitivity What data is being collected, and what is the expectation of privacy and confidentiality?\nFor new data being collected and stored, how sensitive is the data and what are the internal safeguards for handling that data? E.g., what access controls are there, how are breaches to that security caught and managed, etc.? Then, for that data, how will it be used and how will participants’ data be protected? How are participants guaranteed that their data, which was collected for use in the study, will not be used for some other purpose? This becomes more important as the sensitivity of the data increases. Finally, what data may be published more broadly, and does that introduce any additional risk to the participants? ","wordCount":"7355","inLanguage":"en","datePublished":"2020-07-03T14:24:21Z","dateModified":"2020-07-03T14:24:21Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nancyyanyu.github.io/posts/ab_testing_udacity/"},"publisher":{"@type":"Organization","name":"Nancy's Notebook","logo":{"@type":"ImageObject","url":"https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nancyyanyu.github.io/ accesskey=h title="Nancy's Notebook (Alt + H)"><img src=https://nancyyanyu.github.io/apple-touch-icon.png alt aria-label=logo height=35>Nancy's Notebook</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nancyyanyu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://nancyyanyu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://nancyyanyu.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nancyyanyu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nancyyanyu.github.io/posts/>Posts</a></div><h1 class=post-title>Study Notes of Udacity A/B Testing</h1><div class=post-meta><span title='2020-07-03 14:24:21 +0000 UTC'>July 3, 2020</span>&nbsp;·&nbsp;35 min&nbsp;·&nbsp;7355 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/AB_Testing_Udacity/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>Study note of Udacity <a href=https://classroom.udacity.com/courses/ud257>A/B Testing course</a>.</p><h1 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h1><p>A/B testing is a methodology for testing product changes. You split your users to two groups - the control group which sees the default feature, and an experimental group that sees the new features.</p><h2 id=what-ab-testing-isnt-good-for>What A/B testing <em><strong>isn’t</strong></em> good for<a hidden class=anchor aria-hidden=true href=#what-ab-testing-isnt-good-for>#</a></h2><p>A/B testing is not good for <strong>testing new experiences</strong>. It may result in <em>change aversion</em> (where users don’t like changes to the norm), or a <em>novelty effect</em> (where users see something new and test out everything).</p><p>The two things with new experiences is</p><ul><li>having a baseline and</li><li>how much time needs to be allowed for the users to adapt to the new experience, so you can say what is going to be the plateaud experience and make a robust decision.</li></ul><p>Finally, A/B testing cannot tell you <strong>if you are missing something</strong>.</p><p>In these cases, user logs can be used to develop hypothesis that can then be used in an A/B test. A/B testing gives broad quantitiative data, while other techniques such as user research, focus groups, human evaluation give you deep qualitative data</p><h2 id=metric-choice>Metric Choice<a hidden class=anchor aria-hidden=true href=#metric-choice>#</a></h2><p><strong>Click-Through Rate</strong>: $\frac{Number \ of \ clicks }{Number \ of \ page views}$</p><ul><li>Use a rate when you want to measure the <strong>usability</strong> (how often do they actually find that button.)</li></ul><p><strong>Click-Through Probability</strong> $\frac{Unique \ visitors \ who \ click }{Unique \ visitors \ to \ page}$</p><ul><li>Use a probability when you want to measure the <strong>total impact</strong> (how often users went to the second level page on your site)</li></ul><blockquote><p>We&rsquo;re interested in whether users are progressing to the second level of the funnel, which is why we picked a probability.</p></blockquote><h3 id=how-to-compute-rate--probability->How to compute rate & probability ?<a hidden class=anchor aria-hidden=true href=#how-to-compute-rate--probability->#</a></h3><p>Rate: on every page view you capture the event, and then whenever a user clicks you also capture that click event</p><ul><li>Sum the page views, you sum the clicks and you divide.</li></ul><p>Probability: match each page view with all of the child clicks, so that you count, at most, one child click per page view.</p><h2 id=review-distribution>Review Distribution<a hidden class=anchor aria-hidden=true href=#review-distribution>#</a></h2><h3 id=binomial-distribution>Binomial Distribution<a hidden class=anchor aria-hidden=true href=#binomial-distribution>#</a></h3><p>For a binomial distribution with probability $p$ , the mean is given by $p$ and the standard deviation is $\sqrt{p \times (1−p)/N}$ where $N$ is the number of trials.</p><p>A binomial distribution can be used when</p><ol><li>The outcomes are of 2 types</li><li>Each event is independent of the other</li><li>Each event has an identical distribution (i.e. $p$ is the same for all)</li></ol><blockquote><p>We expect click-through probability to follow a binomial distribution</p></blockquote><h3 id=confidence-interval>Confidence Interval<a hidden class=anchor aria-hidden=true href=#confidence-interval>#</a></h3><p>Click or non-click</p><p>$x$: # of users who clicked</p><p>$N$: # of users</p><p>$\hat{p}=\frac{x}{N}$ : estimate of probability</p><ul><li>$\hat{p}=\frac{100}{1000}=0.1$</li></ul><p>To use normal: check $N \cdot \hat{p}>5$ and $N \cdot (1-\hat{p})>5$</p><p>$z$- distribution: standard normal distribution with $\mu=0$ and $\sigma=1$</p><ul><li>With 95% confidence, the true value would be within $1.96 $ and $-1.96$</li></ul><p>$m=z \cdot SE=z \cdot \sqrt{\frac{\hat{p}(1-\hat{p})}{N}}$: margin of error</p><ul><li>$m=0.019$,</li><li>Confidence interval: $[0.081,0.119]$</li></ul><p>This means, if you&rsquo;d run the experiment again with another 1000 page views, you&rsquo;d maybe expect to see between 80 and 120 clicks, but more or less than that would be pretty surprising.</p><h4 id=standard-deviation-of-binomial>Standard deviation of binomial<a hidden class=anchor aria-hidden=true href=#standard-deviation-of-binomial>#</a></h4><p>If you look up a binomial distribution elsewhere, you may find that it has a mean of $np$ and a standard deviation of $\sqrt{np(1-p)}$. This is for a binomial distribution defined as <strong>the total number of successes</strong>, whereas we will use <strong>the fraction or proportion of successes</strong> throughout this class. In this cas, the mean is $p$ and standard deviation is $\sqrt{\frac{p(1-p)}{n}}$.</p><h4 id=useful-equations>Useful equations<a hidden class=anchor aria-hidden=true href=#useful-equations>#</a></h4><p>You may find these equations helpful in solving the quiz:
<a href="https://lh3.googleusercontent.com/7G4sR5EnVaKfqmc98cJorT0F9TtKIYEql7WDIYuemeWSGcGf6fW_MqrxR8fAs39n6gdmZ2ubbg28ttH_O9c=s0#w=134&amp;h=128">p_hat = X/N SE = sqrt(p_hat (1-p_hat) / N) m = z* SE</a></p><h2 id=hypothesis-testing>Hypothesis Testing<a hidden class=anchor aria-hidden=true href=#hypothesis-testing>#</a></h2><p><strong>null hypothesis:</strong> there&rsquo;s no difference in click-through probability between our control, and our experiment.</p><p><strong>alternative hypothesis:</strong> whether the click-through rate is different? Or it&rsquo;s higher, or lower? Or are we interested in any kind of difference at all?</p><h3 id=two-tailed-vs-one-tailed-tests>Two-tailed vs. one-tailed tests<a hidden class=anchor aria-hidden=true href=#two-tailed-vs-one-tailed-tests>#</a></h3><p>Two-tailed: The null hypothesis and alternative hypothesis allows you to distinguish between three cases:</p><ol><li>A statistically significant positive result</li><li>A statistically significant negative result</li><li>No statistically significant difference.</li></ol><p>one-tailed: allows you to distinguish between two cases:</p><ol><li>A statistically significant positive result</li><li>No statistically significant result</li></ol><p>Which one you should use depends on what action you will take based on the results. If you&rsquo;re going to launch the experiment for a statistically significant positive change, and otherwise not, then you don&rsquo;t need to distinguish between a negative result and no result, so a one-tailed test is good enough. If you want to learn the direction of the difference, then a two-tailed test is necessary.</p><h2 id=comparing-two-samples---pooled-standard-error>Comparing two samples - Pooled Standard Error<a hidden class=anchor aria-hidden=true href=#comparing-two-samples---pooled-standard-error>#</a></h2><p>For comparing two samples, we calculate the <strong>pooled standard error</strong>. For e.g., suppose $Xcont$ and $Ncont$ are the control number of users that click, and the total number of users in the control group. Let $X_{exp}$ and $N_{exp}$ be the values for the experiment.</p><p>The pooled probability is given by</p><p>$\hat{p_{pool}}=\frac{X_{cont}+X_{exp}}{N_{cont}+N_{exp}}$</p><p>$$SE_{pool}=\sqrt{\hat{p_{pool}}∗(1−\hat{p_{pool}})∗(\frac{1}{N_{cont}}+\frac{1}{N_{exp}})}$$</p><p>$$\hat{d}=\hat{p_{exp}}−\hat{p_{cont}}$$</p><p>$$H_0:d=0 \ where \ \hat{d} ∼N(0,SE_{pool})$$</p><p>$d$ is practical significance boundary. If $\hat{d} >1.96∗SE_{pool}$ or $ \hat{d}&lt;−1.96∗SE_{pool}$ then we can reject the null hypothesis and state that our difference represents a statistically significant difference</p><h3 id=practical-or-substantive-significance>Practical or Substantive Significance<a hidden class=anchor aria-hidden=true href=#practical-or-substantive-significance>#</a></h3><p>Practical significance is the level of change that you would expect to see from a business standpoint for the change to be valuable.</p><p>The differences in the magnitudes for what&rsquo;s consider practically significant can be quite different. What you really want to observe is repeatability.</p><p>Statistical significance is about <strong>repeatability</strong>. And you want to make sure when you setup your experiment that you get that guarantee that yes, these results are repeatable so it&rsquo;s statistically significant.</p><p>The statistical significance bar is often lower than the practical significance bar, so that if the outcome is practically significance, it is also statistically significant.</p><h2 id=size-vs-power-trade-off>Size vs Power trade-off<a hidden class=anchor aria-hidden=true href=#size-vs-power-trade-off>#</a></h2><p><strong>Statistical power:</strong> given that we have control over how many page views go into our control and our experiment, we have to decide how many page views we need in order to get a statistically significant result. -> determine the number of data points needed to get a statistically significant result.</p><p><strong>Power</strong> has an inverse trade-off with <strong>size</strong>. The smaller the change you want to detect or the increased confidence you want to have in the result, means you have to run a larger experiment.</p><p>As you increase the number of samples, the confidence interval moves closer to the mean.</p><p>$$α=P(reject\ null | null\ true)$$ - Falsely concluding there is a difference.</p><p>$$β=P(fail\ to\ reject\ null | null\ false)$$ - Falsely concluding there is no difference</p><p>$1−β$ is referred to as the <strong>sensitivity</strong> of the experiment, or <strong>statistical power</strong>. In general, you want your experiment
to have a high level of sensitivity at the practical significance boundary. People often choose high sensitivity, typically around <em>80%</em>.</p><p>For a small sample,</p><ul><li>$α$ is low (you are unlikely to launch a bad experiment)</li><li>$β$ is high (you are likely to fail to launch an experiment that actually did have a difference you care about).</li></ul><p>For a large sample,</p><ul><li>$α$ remains the same</li><li>$β$ is lower (i.e. sensitivity increases).</li></ul><p>As you change one of the parameters, your sample size will change as well.</p><ul><li>If you increase the baseline click through probability (under 0.5) then this increases the standard error, and therefore, you need a higher number of samples</li><li>If you increase the practical significance level ($d_{min}$), you require a fewer number of samples since larger changes are easier to detect</li><li>If you increase the confidence level ($1-\alpha$), you want to be more certain that you are rejecting the null. At the same sensivitiy, this would require increasing the number of samples</li><li>If you want to increase the sensitivity ($1-\beta$), you need to collect more samples</li></ul><h3 id=analyze-results>Analyze Results<a hidden class=anchor aria-hidden=true href=#analyze-results>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>N_cont</span> <span class=o>=</span> <span class=mi>10072</span>  <span class=c1># Control samples (pageviews)</span>
</span></span><span class=line><span class=cl><span class=n>N_exp</span> <span class=o>=</span> <span class=mi>9886</span>  <span class=c1># Test samples (pageviews)</span>
</span></span><span class=line><span class=cl><span class=n>X_cont</span> <span class=o>=</span> <span class=mi>974</span>  <span class=c1># Control clicks</span>
</span></span><span class=line><span class=cl><span class=n>X_exp</span> <span class=o>=</span> <span class=mi>1242</span>  <span class=c1># Exp. clicks</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>p_pool</span> <span class=o>=</span> <span class=p>(</span><span class=n>X_cont</span> <span class=o>+</span> <span class=n>X_exp</span><span class=p>)</span><span class=o>/</span><span class=p>(</span><span class=n>N_cont</span><span class=o>+</span><span class=n>N_exp</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>se_pool</span> <span class=o>=</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>p_pool</span><span class=o>*</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>p_pool</span><span class=p>)</span><span class=o>*</span><span class=p>(</span><span class=mi>1</span><span class=o>/</span><span class=n>N_cont</span> <span class=o>+</span> <span class=mi>1</span><span class=o>/</span><span class=n>N_exp</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>p_cont</span> <span class=o>=</span> <span class=n>X_cont</span><span class=o>/</span><span class=n>N_cont</span>
</span></span><span class=line><span class=cl><span class=n>p_exp</span> <span class=o>=</span> <span class=n>X_exp</span><span class=o>/</span><span class=n>N_exp</span>
</span></span><span class=line><span class=cl><span class=n>d_hat</span> <span class=o>=</span> <span class=n>p_exp</span> <span class=o>-</span> <span class=n>p_cont</span>
</span></span><span class=line><span class=cl><span class=c1># d_hat = 0.02892847</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=mf>1.96</span><span class=o>*</span><span class=n>se_pool</span>
</span></span><span class=line><span class=cl><span class=n>cf_min</span> <span class=o>=</span> <span class=n>d_hat</span><span class=o>-</span><span class=n>m</span>
</span></span><span class=line><span class=cl><span class=n>cf_max</span> <span class=o>=</span> <span class=n>d_hat</span><span class=o>+</span><span class=n>m</span>
</span></span><span class=line><span class=cl><span class=n>d_min</span> <span class=o>=</span> <span class=mf>0.02</span> <span class=c1># Minimum practical significance value for difference</span>
</span></span><span class=line><span class=cl><span class=c1># cf_min = 0.0202105</span>
</span></span><span class=line><span class=cl><span class=c1># cf_max = 0.03764645</span>
</span></span></code></pre></div><p>Since the minimum confidence limit is greater than 0 and the practical significance level of 0.02, we conclude that it is highly probable that click through probability is higher than 0.02 and is significant. Based on this, one would launch the new version.</p><h1 id=choosing-and-charaterizing-metrics>Choosing and Charaterizing Metrics<a hidden class=anchor aria-hidden=true href=#choosing-and-charaterizing-metrics>#</a></h1><p>:)</p><h2 id=metric-definition>Metric Definition<a hidden class=anchor aria-hidden=true href=#metric-definition>#</a></h2><p><strong>Two use cases of metrics:</strong></p><ul><li><em><strong>Invariant checking</strong></em> (sanity checking): Metrics that shouldn’t change between your test and control<ul><li>Do you have the same number of users across the two?</li><li>Is the distribution the same?</li></ul></li><li><em><strong>Evaluation</strong></em>:<ul><li>High level business metrics: how much revenue you make, what your market share is, how many users you have</li><li>Detailed metrics: user experience with the product</li></ul></li></ul><p><strong>How to make a definition</strong>:</p><ol><li>A high level concept for a metric/ one sentence summary: &ldquo;active users&rdquo;, &ldquo;click-through probability&rdquo;</li><li>Nitty gritty details: How do you define what active is? Which events count towards activity?</li><li>Summarize individual data measurements into a single metric: a sum or a count, an average</li></ol><p>For evaluation, you can choose either one metric or a whole suite of metrics.</p><p><strong>Overall Evaluation Criterion (OEC):</strong> a term that Microsoft uses for when they come up with a weighted function that combines all of these different metrics.</p><ul><li>Hard to define and get everyone to agree</li><li>Over-optimize</li><li>Hard to explain why is it moving</li></ul><p><strong>How generally applicable the metric is</strong>:</p><p>If you&rsquo;re running a whole suite of AB tests, then ideally you&rsquo;d have one or more metrics that you can use across the entire suite. If you are running a suite of A/B tests, it is preferable to have a metric that works across the entire suite. It&rsquo;s much better to use a metric that&rsquo;s <strong>less optimal</strong> than it is to come up with the perfect metric for your test.</p><h3 id=refining-the-customer-funnel>Refining the Customer Funnel<a hidden class=anchor aria-hidden=true href=#refining-the-customer-funnel>#</a></h3><p>User funnel indicates a series of steps taken by users through the site. It is called a funnel because every subsequent stage has fewer users than the stage above.</p><p>Each stage is a metric:</p><ul><li><p>Count: No. of users who reach that point (keep in certain stages)</p></li><li><p>Rate: better for usability test. You want to increase rate of progression</p></li><li><p>Probability: progression. A unique user progressed down the funnel</p></li></ul><h2 id=defining-metrics-other-techniques>Defining Metrics: Other Techniques<a hidden class=anchor aria-hidden=true href=#defining-metrics-other-techniques>#</a></h2><ol><li><strong>External Data:</strong> great for brainstorming, think of new metrics idea; good for validating metrics; benchmark your own metrics against the industry; help you develop validation techniques<ul><li>Companies that collect data (e.g. Comscore, Nielsen)</li><li>Companies that conduct surveys (e.g. Pew)</li><li>Academic papers</li></ul></li><li>Internal Data:<ul><li><strong>Retrospective analysis</strong>:<ul><li>Look at historic data to look at changes and see the evaluation</li><li>Good to get a baseline and help you develop theories</li></ul></li><li><strong>Surveys and User experience research</strong>:<ul><li>help you develop ideas on what you want to research</li></ul></li></ul></li></ol><p>The problem of these studies: show you correlation, not causation</p><h3 id=gathering-additional-data><strong>Gathering Additional Data</strong><a hidden class=anchor aria-hidden=true href=#gathering-additional-data>#</a></h3><ol><li><p><strong>User Experience Research (UER)</strong>:</p><ul><li><p>high depth on a few users.</p></li><li><p>good for brainstorming.</p></li><li><p>can use special equipment in a UER (e.g. eye movement camera)</p><p>X validate the results (retrospective analysis)</p></li></ul></li><li><p><strong>Focus groups</strong>:</p><ul><li><p>Medium depth and medium # of participants.</p></li><li><p>Get feedback on hypotheticals</p><p>X may run into the issue of groupthink</p></li></ul></li><li><p><strong>Surveys</strong>:</p><ul><li><p>low depth but high # of participants</p></li><li><p>Useful for metrics you cannot directly measure.</p><p>X Can’t directly compare with other metrics since population for survey and internal metrics may be different.</p></li></ul></li></ol><h3 id=applying-other-techniques-on-difficult-metrics>Applying Other Techniques on Difficult Metrics<a hidden class=anchor aria-hidden=true href=#applying-other-techniques-on-difficult-metrics>#</a></h3><p><strong>Difficult Metrics:</strong></p><ol><li>Don&rsquo;t have access to data</li><li>Takes too long</li></ol><p><strong>Rate of returning for 2nd course</strong></p><ul><li>Takes too long</li><li>Survey -> proxy</li></ul><p><strong>Average happiness of shoppers</strong></p><ul><li>Don&rsquo;t have access to data</li><li>Survey; UER - brainstorm</li></ul><p><strong>Probability of finding info via search</strong></p><ul><li>Don&rsquo;t have access to data</li><li>External data; UER; Human raters</li><li>Possible proxies: time spent; clickes on result; follow up queries</li></ul><h2 id=metric-definitions--data-capture>Metric definitions & Data Capture<a hidden class=anchor aria-hidden=true href=#metric-definitions--data-capture>#</a></h2><p>Def #1 (Cookie probability): For each , number of cookies that click divided by number of cookies</p><p>Def #2 (Pageview probability): Number of pageviews with a click within divided by number of pageviews</p><p>Def #3 (Rate): Number of clicks divided by number of pageviews</p><h3 id=filtering-and-segmenting>Filtering and Segmenting<a hidden class=anchor aria-hidden=true href=#filtering-and-segmenting>#</a></h3><blockquote><p>Good for evaluating definitions and building intuitions</p></blockquote><p>You may have to filter out spam and fraud to de-bias the data. You do want to be careful you don&rsquo;t introduce bias into your data by doing the filtering.</p><p>One way to figure out if you are biasing or de-biasing the data by filtering, is to <strong>slice your data</strong> by country, or by language, or by platform, and then calculate the metric for each slice after filterig. If you are affecting any slide disproportionately, then you may be biasing your data with filtering</p><p>To remove any weekly effects when looking say at total active cookies over time, use week-over-week i.e. divide current data by data from a week ago. Alternately, one can use year-over-year.</p><h2 id=summary-metrics>Summary Metrics<a hidden class=anchor aria-hidden=true href=#summary-metrics>#</a></h2><blockquote><p>Summarize all of these individual data measurements into a single summary metric.</p></blockquote><p><strong>Characteristics for your metric</strong>:</p><ol><li>the <strong>sensitivity</strong> and <strong>robustness</strong>: You want your metric to be sensitive enough, in order to actually detect a change when you, when you&rsquo;re testing
your possible future options,</li><li>the <strong>distribution</strong>: The most ideal way of doing this is to do a retrospective analysis to compute a histogram.</li></ol><p><strong>4 categories of summary metrics:</strong></p><ol><li><p>Sums and counts: # of users who visited</p></li><li><p>Distributional metrics: the means, the medians, the 25th, the 75th and 90th percentiles.</p></li><li><p>Probabilities and rates.</p></li><li><p>Ratios: can compute a whole range of different business models, and various different things that you may care about, but they can be very
difficult to characterize.</p></li></ol><h2 id=sensitivity-and-robustness><strong>Sensitivity</strong> and <strong>Robustness</strong><a hidden class=anchor aria-hidden=true href=#sensitivity-and-robustness>#</a></h2><p>Whether the metric is sensitive to changes you care about, and is robust to changes you don’t care about</p><ul><li><strong>mean</strong> is sensitive to outliers - NOT robust</li><li><strong>median</strong> is robust but not sensitive to changes to small group of users</li></ul><p>How to measure <strong>Sensitivity</strong> and <strong>Robustness</strong>:</p><ol><li><strong>A/A tests</strong> to see if the metric picks up any spurious differences</li><li>Using <em>prior experiments</em> to see if the metric moves in a way that intuitively make sense.</li><li><em>Retrospective analysis</em> of log data: look back at changes you made to your website and see if the metrics you&rsquo;re interested in actually moved in conjunction with chose changes.</li></ol><h2 id=absolute-vs-relative-difference>Absolute vs. Relative difference<a hidden class=anchor aria-hidden=true href=#absolute-vs-relative-difference>#</a></h2><p>Suppose you run an experiment where you measure the number of visits to your homepage, and you measure 5000 visits in the control and 7000 in the experiment. Then the absolute difference is the result of subtracting one from the other, that is, 2000. The <strong>relative difference</strong> is the absolute difference divided by the control metric, that is, 40%.</p><p>If you are running a lot of experiments you want to use the relative difference i.e the percentage change.</p><ul><li>The main advantage : you only have to choose one <em>practical significance boundary</em> to get stability over time rather than change it as the system changes.</li><li>The main disadvantage : <strong>variability</strong>, relative differences such as <em>ratios</em> are not as well behaved as absolute differences</li></ul><h6 id=relative-differences-in-probabilities>Relative differences in probabilities<a hidden class=anchor aria-hidden=true href=#relative-differences-in-probabilities>#</a></h6><p>For probability metrics, people often use percentage points to refer to absolute differences and percentages to refer to relative differences. For example, if your control click-through-probability were 5%, and your experiment click-through-probability were 7%, the absolute difference would be 2 percentage points, and the relative difference would be 40 percent. However, sometimes people will refer to the absolute difference as a 2 percent change, so if someone gives you a percentage, it&rsquo;s important to clarify whether they mean a relative or absolute difference!</p><h2 id=variability>Variability<a hidden class=anchor aria-hidden=true href=#variability>#</a></h2><p>We want to check the variability of a metric to later determine the sizing of the experiment and to analyze confidence intervals and draw conclusions. If we have a metric that varies a lot, then the practical significance level that we are looking for may not be feasible.</p><p>To calculate the confidence interval, you need</p><ul><li>Variance (or standard deviation)</li><li>Distribution</li></ul><p>Binomial distribution:</p><p>$$SE=\sqrt{\frac{\hat{p}(1-\hat{p})}{N}}$$</p><p>$$m=z^*SE$$</p><p>We use the fact that this was a binomial distribution in two ways.</p><ol><li><p>we use the fact that this was a binomial distribution to get this formula for the standard error.</p></li><li><p>this formula for the margin of error depends on the assumption that this is a normal distribution, as the binomial approaches a normal distribution as N gets larger.</p></li></ol><table><thead><tr><th>Type of metric</th><th>Distribution</th><th>Estimated variance</th></tr></thead><tbody><tr><td>Probability</td><td>Binomial (normal)</td><td>$\frac{\hat{p}(1-\hat{p})}{N}$</td></tr><tr><td>Mean</td><td>Normal</td><td>$\frac{\hat{\sigma}^2}{n}$ ($\hat{\sigma}$ : variance of the sample)</td></tr><tr><td>Medium/percentile</td><td>Depends on the assumption of data distribution</td><td>Depends</td></tr><tr><td>Count/difference</td><td>Normal (maybe)</td><td>$Var(X)+Var(Y)$</td></tr><tr><td>Rates</td><td>Poisson</td><td>$\bar{X}$ (mean)</td></tr><tr><td>Ratios ($\frac{\hat{p_{exp}}}{\hat{p_{cont}}}$ instead of $\hat{p_{exp}}-\hat{p_{cont}}$)</td><td>Depends</td><td>Depends</td></tr></tbody></table><p>The variance of the actual metric: if you were to collect a new sample, how would you expect this metric to vary?</p><p>The variance of the sample: take each of your data points and then collect the variance of them.</p><p><strong>Calculating CI for a Mean</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=n>N</span><span class=o>=</span> <span class=p>[</span><span class=mi>87029</span><span class=p>,</span> <span class=mi>113407</span><span class=p>,</span> <span class=mi>84843</span><span class=p>,</span> <span class=mi>104994</span><span class=p>,</span> <span class=mi>99327</span><span class=p>,</span> <span class=mi>92052</span><span class=p>,</span> <span class=mi>60684</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>N_mean</span><span class=o>=</span><span class=nb>sum</span><span class=p>(</span><span class=n>N</span><span class=p>)</span><span class=o>/</span><span class=nb>len</span><span class=p>(</span><span class=n>N</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>N_std</span><span class=o>=</span><span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=nb>sum</span><span class=p>([(</span><span class=n>n</span><span class=o>-</span><span class=n>N_mean</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span> <span class=k>for</span> <span class=n>n</span> <span class=ow>in</span> <span class=n>N</span><span class=p>])</span><span class=o>/</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>N</span><span class=p>)</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>SE</span><span class=o>=</span><span class=n>N_std</span><span class=o>/</span><span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>N</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>cf_min</span><span class=p>,</span><span class=n>cf_max</span><span class=o>=</span><span class=n>N_mean</span><span class=o>-</span><span class=n>SE</span><span class=o>*</span><span class=mf>1.96</span><span class=p>,</span><span class=n>N_mean</span><span class=o>+</span><span class=n>SE</span><span class=o>*</span><span class=mf>1.96</span>
</span></span></code></pre></div><h3 id=non-parametric-methods>Non-parametric methods<a hidden class=anchor aria-hidden=true href=#non-parametric-methods>#</a></h3><p>A way to analyze the data without making an assumption about what the distribution Is.</p><ul><li><strong>sign test</strong></li><li>Compute variance empirically</li></ul><p>Reason of using Non-parametric methods:</p><ul><li>for more complicated metrics, you might need to estimate the variance empirically instead of computing it analytically.</li><li>At Google, it was observed that the analytical estimates of variance was often under-estimated, and therefore they have resorted to use empirical measurements based on A/A test to evaluate variance.</li></ul><p>-> using <strong>A versus A experiments</strong> across the board to estimate the empirical variability of all of our metrics.</p><p><strong>What are A versus A experiments</strong>: in an A versus A test, what you have is a control, A against another control A, and so there&rsquo;s actually no change
in what the users are seeing. What that means that any differences that you measure are due to the underlying variability, maybe of your system, of the user population, what users are doing, all of those types of things.</p><p>If you see a lot of variability in a metric in an A versus A test, it&rsquo;s probably too sensitive to be useful in, in evaluating a real experiment.</p><p><strong>How many A/A tests are needed to get a good sense?</strong>: The key rule of thumb to keep in mind is that the standard deviation is going to be proportional to the square root of the number of sample.</p><p><strong>What if you can&rsquo;t run many A/A tests for some reason?</strong>: another option is to run one really big A versus A experiment. And then using bootstrap, where what you do is you take that big sample, and you randomly divvy it up into a bunch of small samples and you do the comparison within those random subsets.</p><p>One advantage of running the lots of different A/A tests is because if your experiment system is itself complicated, it&rsquo;s actually a very good test of your system.</p><p><strong>Uses of A/A tests:</strong></p><ol><li>Compare result to what you expect (sanity check)</li><li>Estimate variance empirically and use your assumption about the distribution to calculate confidence</li><li>Directly estimate confidence interval without making any assumption of the data</li></ol><p><strong>Calculating a Confidence Interval Empirically</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># click-through-probability of 40 A/A tests or bootstrap samples</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=n>group1</span><span class=o>=</span><span class=p>[</span><span class=mf>0.02</span><span class=p>,</span> <span class=mf>0.11</span><span class=p>,</span> <span class=mf>0.14</span><span class=p>,</span> <span class=mf>0.05</span><span class=p>,</span> <span class=mf>0.09</span><span class=p>,</span> <span class=mf>0.11</span><span class=p>,</span> <span class=mf>0.09</span><span class=p>,</span> <span class=mf>0.1</span> <span class=p>,</span> <span class=mf>0.14</span><span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.09</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.09</span><span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.12</span><span class=p>,</span> <span class=mf>0.09</span><span class=p>,</span> <span class=mf>0.16</span><span class=p>,</span> <span class=mf>0.11</span><span class=p>,</span> <span class=mf>0.12</span><span class=p>,</span> <span class=mf>0.11</span><span class=p>,</span> <span class=mf>0.06</span><span class=p>,</span> <span class=mf>0.11</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.13</span><span class=p>,</span> <span class=mf>0.1</span> <span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.14</span><span class=p>,</span> <span class=mf>0.1</span> <span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.12</span><span class=p>,</span> <span class=mf>0.09</span><span class=p>,</span> <span class=mf>0.14</span><span class=p>,</span> <span class=mf>0.1</span> <span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.07</span><span class=p>,</span> <span class=mf>0.13</span><span class=p>,</span> <span class=mf>0.11</span><span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.1</span> <span class=p>,</span> <span class=mf>0.11</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>group2</span><span class=o>=</span><span class=p>[</span><span class=mf>0.07</span><span class=p>,</span> <span class=mf>0.11</span><span class=p>,</span> <span class=mf>0.05</span><span class=p>,</span> <span class=mf>0.07</span><span class=p>,</span> <span class=mf>0.1</span> <span class=p>,</span> <span class=mf>0.07</span><span class=p>,</span> <span class=mf>0.1</span> <span class=p>,</span> <span class=mf>0.1</span> <span class=p>,</span> <span class=mf>0.12</span><span class=p>,</span> <span class=mf>0.14</span><span class=p>,</span> <span class=mf>0.04</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.07</span><span class=p>,</span> <span class=mf>0.07</span><span class=p>,</span> <span class=mf>0.06</span><span class=p>,</span> <span class=mf>0.15</span><span class=p>,</span> <span class=mf>0.09</span><span class=p>,</span> <span class=mf>0.12</span><span class=p>,</span> <span class=mf>0.1</span> <span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.09</span><span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.14</span><span class=p>,</span> <span class=mf>0.09</span><span class=p>,</span> <span class=mf>0.1</span> <span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.09</span><span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.11</span><span class=p>,</span> <span class=mf>0.11</span><span class=p>,</span> <span class=mf>0.1</span> <span class=p>,</span> <span class=mf>0.14</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.1</span> <span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.05</span><span class=p>,</span> <span class=mf>0.19</span><span class=p>,</span> <span class=mf>0.11</span><span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.13</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>confidence_level</span><span class=o>=</span><span class=mf>0.95</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># assume metric is normally distributed</span>
</span></span><span class=line><span class=cl><span class=n>difference</span><span class=o>=</span><span class=p>[</span><span class=n>i</span><span class=o>-</span><span class=n>j</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span><span class=n>j</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>group1</span><span class=p>,</span><span class=n>group2</span><span class=p>)]</span>
</span></span><span class=line><span class=cl><span class=n>mean</span><span class=o>=</span><span class=nb>sum</span><span class=p>(</span><span class=n>difference</span><span class=p>)</span><span class=o>/</span><span class=nb>len</span><span class=p>(</span><span class=n>difference</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>SD</span><span class=o>=</span><span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=nb>sum</span><span class=p>([(</span><span class=n>i</span><span class=o>-</span><span class=n>mean</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>difference</span> <span class=p>])</span><span class=o>/</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>difference</span><span class=p>)</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># m=SD*z-score</span>
</span></span><span class=line><span class=cl><span class=n>m</span><span class=o>=</span><span class=n>SD</span><span class=o>*</span><span class=mf>1.96</span>
</span></span><span class=line><span class=cl><span class=n>ci_max</span><span class=o>=</span><span class=n>mean</span><span class=o>+</span><span class=n>m</span>
</span></span><span class=line><span class=cl><span class=n>ci_min</span><span class=o>=</span><span class=n>mean</span><span class=o>-</span><span class=n>m</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=n>ci_min</span><span class=p>,</span><span class=n>ci_max</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=p>(</span><span class=o>-</span><span class=mf>0.06702773846019527</span><span class=p>,</span> <span class=mf>0.07552773846019528</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># no assumption of metric distribution</span>
</span></span><span class=line><span class=cl><span class=n>difference</span><span class=o>=</span><span class=nb>sorted</span><span class=p>(</span><span class=n>difference</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ci_min</span><span class=p>,</span><span class=n>ci_max</span><span class=o>=</span><span class=n>difference</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span><span class=n>difference</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=n>ci_min</span><span class=p>,</span><span class=n>ci_max</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=p>(</span><span class=o>-</span><span class=mf>0.06000000000000001</span><span class=p>,</span> <span class=mf>0.08</span><span class=p>)</span>
</span></span></code></pre></div><p>In summary, different metrics have different variability. The variability may be high for certain metrics which makes them useless even if they make business or product sense. Computing the variability of a metric is tricky and one needs to take a lot of care.</p><p>For a lot of analysts, a majority of the time is spent is validating and choosing a metric compared to actually running the experiment. Being able to standardize the definitions was critical in the test. When measuring latency, are you talking about when the first byte loads and when a last byte loads. Also, for latency, the mean may not change at all. The signals (e.g. slow/fast connections or browsers) causes lumps in the distribution, and no central measure works. One needs to look at the right percentile metric. The key thing is that you are build</p><h1 id=design-an-experiment><strong>Design an Experiment</strong><a hidden class=anchor aria-hidden=true href=#design-an-experiment>#</a></h1><ol><li>Choose <strong>subject</strong>: What are the units in the population you are going to run the test on? (unit of diversion)</li><li>Choose <strong>population</strong>: What population are you going to use (US only?)</li><li>Size</li><li>Duration</li></ol><h2 id=unit-of-diversion>Unit of Diversion<a hidden class=anchor aria-hidden=true href=#unit-of-diversion>#</a></h2><p>Commonly used units of diversion are:</p><ol><li><strong>User identifier (id)</strong>: Typically the username or email address used on the website. It is typically stable and unchanging. If user id is used as a unit of diversion, then it is either in the test group or the control group. User ID is personally identifiable</li><li><strong>Anonymous id</strong>: This is usually an anonymous identifier such as a <strong>cookie</strong>. It changes with browser or device. People may often refresh their cookies every time they log in. It is difficult to refresh a cookie on an app or a phone compared to the computer.</li><li><strong>Event</strong>: An event is a page load that can change for each user. This is used typically for non-user-visible changes.</li></ol><p>Less common:</p><ol><li><strong>Device id</strong>: Typically available for mobile devices. It is tied to a specific device and cannot be changed by the user. Personally identifiable.</li><li><strong>IP address</strong>: location specific, but may change as the user changes location (e.g. testing on infrastructure change to test impact on latency)</li></ol><p>3 main considerations in selecting an appropriate unit of diversion:</p><ol><li><strong>Consistency</strong></li><li>Ethical - Informed consent: a issue when using user id.</li><li>Variability</li></ol><h3 id=consistency-of-diversion>Consistency of Diversion<a hidden class=anchor aria-hidden=true href=#consistency-of-diversion>#</a></h3><p>If you&rsquo;re testing a change that crosses the sign in, sign out border, then a <strong>user ID</strong> doesn&rsquo;t work as well, use cookie instead.</p><p><strong>For user visible changes</strong>, you would definitely use a <strong>cookie</strong> or a <strong>user ID</strong>. But there are lots of non-user-visible changes (latency changes, back-end infra changes or ranking changes):</p><p>Examples:</p><ul><li>Cookie: change button color and size (distracting if changes on reload; different look on difference devices is ok)</li><li>User id: Add instructors note before quizzes (cross device consistency important)</li><li>Event: Reducing video load time; change order of search results (users won&rsquo;t probably notice)</li></ul><p>IP based diversion not very useful:</p><ul><li>not as consistent as cookie or user id as ip randomly change depending on the provider; no clean randomization like event based diversion.</li><li>only choice: testing out one hosting provider versus a different hosting provider to understand the impact of latency.</li><li>May not get a clean comparison between your experiment and your control:some providers aggregate all of those modem dialup users into a single IP address, so it&rsquo;s hard to find comparable population of users in the control group.</li></ul><h3 id=unit-of-analysis-vs-unit-of-diversion>Unit of Analysis v.s. Unit of Diversion<a hidden class=anchor aria-hidden=true href=#unit-of-analysis-vs-unit-of-diversion>#</a></h3><p><strong>Variability</strong> is higher when it is calculated empirically than when calculated analytically. This is because the <strong>unit of analysis</strong> (i.e. the denominator in the metric. e.g. page view in click through rate) is different from the unit of variability.</p><p>When your <em>unit of diversion</em> is also a page view, so as would be the case in an <strong>event</strong> base diversion, then the analytically computed variability is likely to be very close to the empirically computed variability.</p><p>If, however, your unit of diversion is a <strong>cookie</strong> or a user id then the variability of your same metric click through rate is actually going to be much higher.</p><p>This is because when you&rsquo;re actually computing your variability analytically, you&rsquo;re fundamentally making an assumption about the distribution of the data and what&rsquo;s considered to be independent.</p><ul><li><p>When you&rsquo;re doing event-based diversion every single event is a different randomdraw, and so your independence assumption is actually valid.</p></li><li><p>When you&rsquo;re doing cookie or user ID based diversion, that independence assumption is no longer valid because you&rsquo;re actually diverting groups of events. And so they&rsquo;re actually correlated together.</p></li></ul><p><strong>Measure variablity of a metric</strong>:</p><p>Unit of diversion: queries (event-based) v.s. cookies</p><p>Metric: coverage (the percentage of queries for which an ad is shown) = $\frac{\text{number of queries with ads}}{\text{number of queries}}$</p><p>Unit of analysis: query</p><p>Binomial: $SE=\sqrt{\frac{p(1-p)}{N}}$</p><p>The SE of coverage under cookies diversion is much larger then that of queries.</p><blockquote><p>When unit of analysis = unit of diversion, variability tends to be lower and closer to the anlytical estimate.</p></blockquote><h2 id=choose-population>Choose Population<a hidden class=anchor aria-hidden=true href=#choose-population>#</a></h2><p><strong>intra-user experiment:</strong> expose the same user to this feature being on and of over time, and you actually analyze how they behave in different time windows.</p><ul><li>pitfalls:<ul><li>be really careful that you choose a comparable time window.(not 2 weeks before Christmas)</li><li>different behaviors as a result of a frustration or a learning problem, where people learn to use the particular feature in the first two weeks</li></ul></li></ul><p><strong>interleaved experiment</strong>: expose the same user to the A and the B side at the same time. And typically this only works in cases where you&rsquo;re looking at reordering a list.</p><p><strong>inter-user experiments</strong>: different people on the A side and on the B side.</p><h3 id=target-population>Target Population<a hidden class=anchor aria-hidden=true href=#target-population>#</a></h3><p>Some easy divisions of your user space: what browser they&rsquo;re on, what geo location they come from, what country, what language they&rsquo;re using.</p><p>Reasons why you might make that decision in advance:</p><ul><li>For a feature not sure if you&rsquo;re going to release it and it&rsquo;s a pretty high profile launch, you might want to restrict how many of your users have actually seen it.</li><li>If you&rsquo;re running a couple of different experiments at your company at the same time, you might not want to overlap.</li><li>You may not want to dilute the effect of your experiment across a global population. So if you&rsquo;re analyzing an experiment for the first time, and it only affects English, you may want to actually do your analysis specific on English,</li></ul><p><strong>Targeting an Experiment</strong>:</p><blockquote><p>Filtering could also affect variability</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># New Zealand</span>
</span></span><span class=line><span class=cl><span class=n>N_cont</span> <span class=o>=</span> <span class=mi>6021</span>
</span></span><span class=line><span class=cl><span class=n>X_cont</span> <span class=o>=</span> <span class=mi>302</span>
</span></span><span class=line><span class=cl><span class=n>N_exp</span> <span class=o>=</span> <span class=mi>5979</span>
</span></span><span class=line><span class=cl><span class=n>X_exp</span> <span class=o>=</span> <span class=mi>374</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>p_cont</span><span class=o>=</span><span class=n>X_cont</span><span class=o>/</span><span class=n>N_cont</span>
</span></span><span class=line><span class=cl><span class=n>p_exp</span><span class=o>=</span><span class=n>X_exp</span><span class=o>/</span><span class=n>N_exp</span>
</span></span><span class=line><span class=cl><span class=n>p_pool</span><span class=o>=</span><span class=p>(</span><span class=n>X_cont</span><span class=o>+</span><span class=n>X_exp</span><span class=p>)</span><span class=o>/</span><span class=p>(</span><span class=n>N_cont</span><span class=o>+</span><span class=n>N_exp</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>SE</span><span class=o>=</span><span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>((</span><span class=n>p_pool</span><span class=o>*</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>p_pool</span><span class=p>)</span><span class=o>*</span><span class=p>(</span><span class=mi>1</span><span class=o>/</span><span class=n>N_cont</span><span class=o>+</span><span class=mi>1</span><span class=o>/</span><span class=n>N_exp</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=n>d_hat</span><span class=o>=</span><span class=n>p_exp</span><span class=o>-</span><span class=n>p_cont</span>
</span></span><span class=line><span class=cl><span class=n>m</span><span class=o>=</span><span class=mf>1.96</span><span class=o>*</span><span class=n>SE</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=n>m</span><span class=p>,</span><span class=n>d_hat</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=p>(</span><span class=mf>0.00825068746366646</span><span class=p>,</span> <span class=mf>0.012394485165776618</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Global</span>
</span></span><span class=line><span class=cl><span class=n>N_cont</span> <span class=o>=</span> <span class=mi>50000</span> <span class=o>+</span> <span class=mi>6021</span>
</span></span><span class=line><span class=cl><span class=n>X_cont</span> <span class=o>=</span> <span class=mi>2500</span> <span class=o>+</span> <span class=mi>302</span>
</span></span><span class=line><span class=cl><span class=n>N_exp</span> <span class=o>=</span> <span class=mi>50000</span> <span class=o>+</span> <span class=mi>5979</span>
</span></span><span class=line><span class=cl><span class=n>X_exp</span> <span class=o>=</span> <span class=mi>2500</span> <span class=o>+</span> <span class=mi>374</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>p_cont</span><span class=o>=</span><span class=n>X_cont</span><span class=o>/</span><span class=n>N_cont</span>
</span></span><span class=line><span class=cl><span class=n>p_exp</span><span class=o>=</span><span class=n>X_exp</span><span class=o>/</span><span class=n>N_exp</span>
</span></span><span class=line><span class=cl><span class=n>p_pool</span><span class=o>=</span><span class=p>(</span><span class=n>X_cont</span><span class=o>+</span><span class=n>X_exp</span><span class=p>)</span><span class=o>/</span><span class=p>(</span><span class=n>N_cont</span><span class=o>+</span><span class=n>N_exp</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>SE</span><span class=o>=</span><span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>((</span><span class=n>p_pool</span><span class=o>*</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>p_pool</span><span class=p>)</span><span class=o>*</span><span class=p>(</span><span class=mi>1</span><span class=o>/</span><span class=n>N_cont</span><span class=o>+</span><span class=mi>1</span><span class=o>/</span><span class=n>N_exp</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=n>d_hat</span><span class=o>=</span><span class=n>p_exp</span><span class=o>-</span><span class=n>p_cont</span>
</span></span><span class=line><span class=cl><span class=n>m</span><span class=o>=</span><span class=mf>1.96</span><span class=o>*</span><span class=n>SE</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=n>m</span><span class=p>,</span><span class=n>d_hat</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=p>(</span><span class=mf>0.0025691881506085417</span><span class=p>,</span> <span class=mf>0.0013237234004343165</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=population-vs-cohort>Population v.s. Cohort<a hidden class=anchor aria-hidden=true href=#population-vs-cohort>#</a></h3><p>Cohort: people who enter the experiment at the same time. You define an entering class and you only look at users who entered your experiment on both sides around the same time, and you go forward from there.</p><p><strong>Cohorts</strong> are harder to analyze, limit your experiment to a subset of population, can affect variability .</p><p>Want to use cohort instead of population:<strong>user stability</strong></p><ul><li>Looking for learning effects</li><li>Examining user retention</li><li>Want to increate user activity</li><li>Anything that require users to be established</li></ul><h2 id=experiment-design-and-sizing>Experiment Design and Sizing<a hidden class=anchor aria-hidden=true href=#experiment-design-and-sizing>#</a></h2><p><strong>How to reduce the size of experiments:</strong></p><p>Experiment: change order of courses in the course list</p><p>Metric: click-through-rate</p><p>Unit-of-diversion: cookie</p><p>$d_{min}=0.05$, $\alpha=0.05$, $\beta=0.2$, $SE=0.0628$ for 1000 page views</p><p>Result: need 300,000 pageviews per group!</p><p>Which strategies could reduce the number of page views?</p><ol><li>Increase $d_{min}$, $\alpha$, or $\beta$</li><li>Change unit of diversion to page view<ul><li>Makes unit of diversion same as unit of analysis</li><li>The variability of the metric might decrease and thus decrease the number of page views you need to be confident in your results</li></ul></li><li>Target experiment to specific traffic<ul><li>Non-english traffic will dilute the results</li><li>Could impact choice of practical significance boundrey<ul><li>since you&rsquo;re only looking at a subset of the traffic, you might need a bigger change before it matters to the business;</li><li>Or since your variablity is probably lower, you might want to detect smaller changes rather than decreasing the number of page views.</li></ul></li></ul></li></ol><p>How are you actually going to detect impact? What effect does that have on the size of the experiment when you really don&rsquo;t know what fraction of your population is going to be affected?</p><ul><li>run a pilot where you turn on the experiment for a little while and see who&rsquo;s affected, or you can even just use the first day or the first week of data to try to get a better guess at what fraction of your population you&rsquo;re really looking at.</li></ul><h2 id=duration--vs-exposure>Duration v.s. Exposure<a hidden class=anchor aria-hidden=true href=#duration--vs-exposure>#</a></h2><p><strong>Practical considerations in experimental design</strong>:</p><ol><li>Duration</li><li>When to run the experiment</li><li>Fraction of the traffic to send to the experiment</li></ol><p>The duration of your experiment, is related to the proportion of traffic that you&rsquo;re sending through your experiment.</p><p><strong>Why not run on all of the traffic to get results quicker?</strong></p><ul><li>Safety: not sure if it&rsquo;s working on all browser or how the users will react</li><li>Press: people blogging about features you might not keep</li><li>Other sources of variablity : holiday, weekly variation</li><li>Running multiple tasks at your company: to be comparable, run them at the same time on smaller percentages of traffic.</li></ul><p><strong>Note:</strong> There is weekly variation in traffic and metric, so it&rsquo;s better to run on mix of weekend and weekday days. For risky change, run longer with less traffic.</p><h2 id=learning-effects>Learning Effects<a hidden class=anchor aria-hidden=true href=#learning-effects>#</a></h2><p><strong>learning effects</strong> is basically when you want to measure user learning. Or effectively whether a user is adapting to a change or not.</p><p>Two different types of learning effects</p><ol><li>Change aversion &ldquo;I don&rsquo;t like anything.&rdquo;</li><li>Knowledge effect &ldquo;Let me try everything around.&rdquo;</li></ol><p>When users first encounter a change they will react in one of these two ways, but will eventually plateau to a very different behavior.</p><p><strong>Considerations when measure a learning effect:</strong></p><ol><li><p>The key issue with trying to is <strong>time</strong>： It takes time for you just to actually adapt to a change and often times you don&rsquo;t have the luxury of taking that much time to make a decision.</p></li><li><p>Choosing the unit of diversion correctly. - a cookie or a user ID</p></li><li><p>Dosage: because a lot of the learning is based on not just a slight time but how often they see the change. Then you probably want to be using a cohort as opposed to just a population. - choose a cohort in both the experiment and the control based on either how long they&rsquo;ve been exposed to the change or how many times they&rsquo;ve seen it.</p></li><li><p>risk and duration: both of those mean to run it through a small proportion of your users for a longer period of time.</p></li></ol><p><strong>pre-periods and post-periods</strong>:</p><p>Before run your <strong>A/B test</strong>, you&rsquo;re on a <em>pre-period</em> on the exact same populations but they&rsquo;re receiving the exact same frequence. It&rsquo;s an <strong>A/A test</strong> on the same set of users.</p><p>In the <strong>pre-period</strong>, if you measure any difference between your experiment and your control populations that difference is due to something else (system variability, user variability).<strong>Pre-period</strong> helps you know that any difference that you measure in your experiment and control is due to the experiment, and not due to any preexisting and inherent differences in your population.</p><p>A <strong>post-period</strong> is saying, after I run my experiment, my control, I&rsquo;m going to run another <em>A versus A test</em>. And then, what, what we can say is that if there are any differences in the experiment and the control populations after I&rsquo;ve run my experiment, then I can attribute those differences to <strong>user learning</strong> that happened in the experiment period.</p><h1 id=analyze-results-1>Analyze Results<a hidden class=anchor aria-hidden=true href=#analyze-results-1>#</a></h1><h2 id=sanity-checks>Sanity Checks<a hidden class=anchor aria-hidden=true href=#sanity-checks>#</a></h2><p><strong>2 main types of checks:</strong></p><ol><li><p><strong>Population sizing metrics</strong> based on your unit of diversion</p><ul><li>Check your experiment population and your control populations are actually comparable.</li></ul></li><li><p><strong>Actual invariants:</strong> metrics that are same in the control and experiment groups and shouldn&rsquo;t change when you run your experiment</p></li></ol><p><strong>Step 1: Choosing invariant metircs</strong></p><p><strong>Step 2: Checking invariants</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-markdown data-lang=markdown><span class=line><span class=cl>	Run experiment for 2 weeks
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>​	Unit of diversion: cookie
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>​	Total control: 64454
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>​	Total experiment: 61818
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>​	How would you figure out if the difference is within expectations?
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>​	Given: each cookie is randomly assigned to the control or experiment group with a prob of 0.5.
</span></span></code></pre></div><pre><code>1. Compute SD of binomial with $p=0.5$ of success. $SD=\sqrt{0.5 \cdot0.5/(64454+61818)}=0.0014$
2. Multiple by z-score to get margin of error. $m=1.96*SD=0.0027$
3. Compute confidence interval around 0.5. $ [0.4973, 0.5027]$
4. Check whether observed fraction is within interval. $64454/(64454+61818)=0.5104$ Not within interval
</code></pre><p><strong>Step 3: what to do</strong></p><ul><li>Talk to engineers</li><li>Try <em>slicing</em> to see if one particular slice is weird</li><li>Check age of cookies - does one group has more new cookies?</li></ul><p><strong>What happens if one of your sanity checks fails</strong>: analyzing why your sanity checks fail</p><ol><li><p>work with your engineers to understand is there something going on with the <em>experiment infrastructure</em>, <em>experiment set up</em>, <em>experiment diversion</em>.</p></li><li><p><strong>retrospective analysis</strong>: Try and recreate experiment diversion from the data capture,</p></li><li><p>use pre & post periods</p><ul><li>If you&rsquo;re in a pre-period,did I see the same changes in those invariance in my pre-period?<ul><li>If I saw them in the pre-period and the experiment, that points to a problem with the experiment infrastructure, the set up, something along those lines.</li><li>if you see the change only in your experiment but not in the pre-period, that points to something with the experiment itself (data capture or
something along those lines).</li></ul></li></ul></li></ol><p><strong>The most common reasons for data not matching up:</strong></p><ul><li>data capture: Maybe the change triggers very rarely, and you capture it correctly in the experiment, but you don&rsquo;t capture quickly in the control.</li><li>experiment&rsquo;s set up: Maybe you set up the filter for the experiment, but not the control.</li><li>experiment system</li></ul><blockquote><p>The key thing : if there really is a learning effect, then not very much change in the beginning, but it&rsquo;s increasing over time.</p></blockquote><blockquote><p>Not a learning effect: a big change from the beginning</p></blockquote><h2 id=single-metric>Single Metric<a hidden class=anchor aria-hidden=true href=#single-metric>#</a></h2><p>Goal: make a business decision about whether your experiment has favorably impacted your metrics. Analytically, decide if you&rsquo;ve observed a statistically significant result of your experiment.</p><p><strong>How do you decide if the change was statistically significant?</strong>:</p><ul><li>Characterizing the metric, understanding how it behaves</li><li>Use variability to estimate <strong>how long</strong> we needed to run the experiment for and <strong>size</strong> our experiment appropriately</li><li>Use the results of last 2 steps to estimate the <strong>variability</strong> we need to analyze the A/B experiment</li></ul><p><strong>What if our results are not statistically significant?</strong>:</p><ul><li>Break down the result into different platforms, different days of the week -> find bugs in your experiment setup & give you a new hypothesis about how people are reacting to the experiment.</li><li>Cross checking your results with other methods, like non parametric sign tests to compare the results to what you got from your parametric hypothesis test.</li></ul><h3 id=analysis-with-single-metric>Analysis with Single Metric<a hidden class=anchor aria-hidden=true href=#analysis-with-single-metric>#</a></h3><p><strong>Experiment I :</strong> change color and place of &lsquo;start now&rsquo; button.</p><p>Metric: click-through-rate</p><p>Unit of diversion: cookie</p><p>$X_{cont}=352$, $N_{cont}=7370$</p><p>$X_{exp}=565$, $N_{exp}=7270$</p><p>$d_{min}=0.01$, $\alpha=0.05$, $\beta=0.2$</p><p><em>Empirical SE</em>: $0.0035$ with 10000 page views in each group</p><p>$$
\begin{align}
&amp;SE \sim \sqrt{\frac{1}{N_1}+\frac{1}{N_2}} \
&\frac{0.035}{\sqrt{\frac{1}{10000}+\frac{1}{10000}}}=\frac{SE}{\sqrt{\frac{1}{7370}+\frac{1}{7270}}} \
&amp;SE=0.0041 \
&\hat{d}=\hat{r}<em>{exp}-\hat{r}</em>{cont}=\frac{565}{7270}-\frac{352}{7370}=0.03 \
&amp;m=1.96*0.0041=0.0080 \
&amp;Confidence\ Interval:[0.022,0.038]
\end{align}
$$
Recommandation: <strong>launch</strong></p><p><strong>Sign Test:</strong></p><p>​ Number of days: 7</p><p>​ Number of days with positive change: 7</p><p>​ If no difference, 50% chance of positive change each day</p><p>​ (7 days too short, cannot assume normal by binomial)</p><p>​ Two-tailed <strong>p-value</strong>: <em>0.0156</em> (the prob of observing a result at least this extreme by chance)</p><p>​ - Since this is less than the chosen alpha of .05, the sign test agrees with the hypothesis test that this result is unlikely to happen by chance.</p><p>​ Recommandation: <strong>launch</strong></p><p><strong>Experiment II :</strong></p><p>Metric: click-through-rate</p><p>$d_{min}=0.01$, $\alpha=0.05$,</p><p>Empirical SE: $0.0062$ with 5000 page views in each group</p><ul><li><strong>Effect size:</strong></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Xs_cont</span> <span class=o>=</span> <span class=p>[</span><span class=mi>196</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>216</span><span class=p>,</span> <span class=mi>212</span><span class=p>,</span> <span class=mi>185</span><span class=p>,</span> <span class=mi>225</span><span class=p>,</span> <span class=mi>187</span><span class=p>,</span> <span class=mi>205</span><span class=p>,</span> <span class=mi>211</span><span class=p>,</span> <span class=mi>192</span><span class=p>,</span> <span class=mi>196</span><span class=p>,</span> <span class=mi>223</span><span class=p>,</span> <span class=mi>192</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>Ns_cont</span> <span class=o>=</span> <span class=p>[</span><span class=mi>2029</span><span class=p>,</span> <span class=mi>1991</span><span class=p>,</span> <span class=mi>1951</span><span class=p>,</span> <span class=mi>1985</span><span class=p>,</span> <span class=mi>1973</span><span class=p>,</span> <span class=mi>2021</span><span class=p>,</span> <span class=mi>2041</span><span class=p>,</span> <span class=mi>1980</span><span class=p>,</span> <span class=mi>1951</span><span class=p>,</span> <span class=mi>1988</span><span class=p>,</span> <span class=mi>1977</span><span class=p>,</span> <span class=mi>2019</span><span class=p>,</span> <span class=mi>2035</span><span class=p>,</span> <span class=mi>2007</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>Xs_exp</span> <span class=o>=</span> <span class=p>[</span><span class=mi>179</span><span class=p>,</span> <span class=mi>208</span><span class=p>,</span> <span class=mi>205</span><span class=p>,</span> <span class=mi>175</span><span class=p>,</span> <span class=mi>191</span><span class=p>,</span> <span class=mi>291</span><span class=p>,</span> <span class=mi>278</span><span class=p>,</span> <span class=mi>216</span><span class=p>,</span> <span class=mi>225</span><span class=p>,</span> <span class=mi>207</span><span class=p>,</span> <span class=mi>205</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>297</span><span class=p>,</span> <span class=mi>299</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>Ns_exp</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1971</span><span class=p>,</span> <span class=mi>2009</span><span class=p>,</span> <span class=mi>2049</span><span class=p>,</span> <span class=mi>2015</span><span class=p>,</span> <span class=mi>2027</span><span class=p>,</span> <span class=mi>1979</span><span class=p>,</span> <span class=mi>1959</span><span class=p>,</span> <span class=mi>2020</span><span class=p>,</span> <span class=mi>2049</span><span class=p>,</span> <span class=mi>2012</span><span class=p>,</span> <span class=mi>2023</span><span class=p>,</span> <span class=mi>1981</span><span class=p>,</span> <span class=mi>1965</span><span class=p>,</span> <span class=mi>1993</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>SE_emp</span><span class=o>=</span><span class=mf>0.0062</span>
</span></span><span class=line><span class=cl><span class=n>Ns_emp</span><span class=o>=</span><span class=mi>5000</span>
</span></span><span class=line><span class=cl><span class=n>SE</span><span class=o>=</span><span class=n>SE_emp</span><span class=o>/</span><span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>1</span><span class=o>/</span><span class=n>Ns_emp</span><span class=o>+</span><span class=mi>1</span><span class=o>/</span><span class=n>Ns_emp</span><span class=p>)</span><span class=o>*</span><span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>1</span><span class=o>/</span><span class=nb>sum</span><span class=p>(</span><span class=n>Ns_cont</span><span class=p>)</span><span class=o>+</span><span class=mi>1</span><span class=o>/</span><span class=nb>sum</span><span class=p>(</span><span class=n>Ns_exp</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>d_hat</span><span class=o>=</span><span class=nb>sum</span><span class=p>(</span><span class=n>Xs_exp</span><span class=p>)</span><span class=o>/</span><span class=nb>sum</span><span class=p>(</span><span class=n>Ns_exp</span><span class=p>)</span><span class=o>-</span><span class=nb>sum</span><span class=p>(</span><span class=n>Xs_cont</span><span class=p>)</span><span class=o>/</span><span class=nb>sum</span><span class=p>(</span><span class=n>Ns_cont</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>m</span><span class=o>=</span><span class=mf>1.96</span><span class=o>*</span><span class=n>SE</span>
</span></span><span class=line><span class=cl><span class=n>ci_min</span><span class=p>,</span><span class=n>ci_max</span><span class=o>=</span><span class=n>d_hat</span><span class=o>-</span><span class=n>m</span><span class=p>,</span><span class=n>d_hat</span><span class=o>+</span><span class=n>m</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=n>ci_min</span><span class=p>,</span><span class=n>ci_max</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=p>(</span><span class=mf>0.006465853496236934</span><span class=p>,</span> <span class=mf>0.016736185710796242</span><span class=p>)</span>
</span></span></code></pre></div><p>Since the confidence level is larger than 0, it&rsquo;s statistically significant.</p><ul><li><strong>Sign test:</strong></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>ctr_cont</span><span class=o>=</span><span class=p>[</span><span class=n>i</span><span class=o>/</span><span class=n>j</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span><span class=n>j</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>Xs_cont</span><span class=p>,</span><span class=n>Ns_cont</span><span class=p>)]</span>
</span></span><span class=line><span class=cl><span class=n>ctr_exp</span><span class=o>=</span><span class=p>[</span><span class=n>i</span><span class=o>/</span><span class=n>j</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span><span class=n>j</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>Xs_exp</span><span class=p>,</span><span class=n>Ns_exp</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>days</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>Xs_cont</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>positive_days</span><span class=o>=</span><span class=nb>sum</span><span class=p>([</span><span class=n>exp</span><span class=o>&gt;</span><span class=n>cont</span> <span class=k>for</span> <span class=n>exp</span><span class=p>,</span><span class=n>cont</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>ctr_exp</span><span class=p>,</span><span class=n>ctr_cont</span><span class=p>)])</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=n>days</span><span class=p>,</span><span class=n>positive_days</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=p>(</span><span class=mi>14</span><span class=p>,</span> <span class=mi>9</span><span class=p>)</span>
</span></span></code></pre></div><p>The two-tail P value is $0.4240$. Since p-value is larger than alpha, it&rsquo;s not significant.</p><h3 id=single-metric-gochas>Single Metric: Gochas<a hidden class=anchor aria-hidden=true href=#single-metric-gochas>#</a></h3><p><strong>What if the sign test and the hypothesis test don&rsquo;t agree?</strong></p><p>Statistical reasons for counter-intuitive results: <strong>Simpson&rsquo;s paradox</strong></p><blockquote><p>Different subgroups in the data like user populations. Within each subgroup, the results are stable but when aggregated together, it&rsquo;s the mix of subgroups that actually drives your result.</p></blockquote><ul><li>New users might be correlated with weekend use, experienced users who react differently are correlated with weekday use. Within each group, their behavior is stable. What drives the results of your experiment are how many people from each group.</li></ul><h2 id=multiple-metrics>Multiple Metrics<a hidden class=anchor aria-hidden=true href=#multiple-metrics>#</a></h2><p><strong>What changes when you have multiple evaluation metrics instead of just one?</strong></p><blockquote><p>As you test more metrics, it becomes more likely that one of them will show a statistically significant result by chance.</p></blockquote><p>But the significant result shouldn&rsquo;t be repeatable. If you did the same experiment on another day or you slices or do some bootstrap analysis, you
wouldn&rsquo;t see the same metric showing up as significant differences every time, it should occur randomly.</p><p><strong>Multiple comparisons</strong>: adjusts your significance level, so that it accounts for how many metrics or how many different tests you&rsquo;re doing.</p><ul><li>Do it when you want automatic alerting that tells you one of my metrics was significantly different on this experiment.</li></ul><h3 id=tracking-multiple-metrics>Tracking Multiple Metrics<a hidden class=anchor aria-hidden=true href=#tracking-multiple-metrics>#</a></h3><p>For 3 <em>independent</em> metrics, $\alpha=0.05$, the chance of at least 1 false positive is $P(FP≥1)=1-0.95^3=0.143$</p><p>$$\text{Overall } \alpha=1-(1-\alpha)^{\text{number of metrics}}$$</p><p><strong>Problem:</strong> Probability of any False Positive increases as you increase number of metrics</p><p><strong>Solution</strong>: Use higher confidence level for each metric</p><ul><li><strong>Method1:</strong> Assume <em>independence</em> of metrics</li></ul><p>​ $\alpha_{overall}=1-(1-\alpha_{individual})^n$, calculate $\alpha_{individual}$ with specified $\alpha_{overall}$</p><ul><li><strong>Method2: Bonferroni Correction</strong><ul><li><p>Simple</p></li><li><p>No assumption</p></li><li><p>Conservative - guarantee to give $\alpha_{overall}$ at lease as small as specified</p></li><li><p>$\alpha_{individual}=\frac{ \alpha_{overall}}{n}$</p></li><li><p>Problem: Given metrics are correlated, and tend to move at the same time, this method is too conservative.</p></li><li><p>Recommendation:</p><ul><li>Rigorous answer: use a more sophisticated method like <a href=http://en.wikipedia.org/wiki/Closed_testing_procedure>closed testing procedure</a>, the <a href=http://en.wikipedia.org/wiki/Bonferroni_bound>Boole-Bonferroni bound</a>, and the <a href=http://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method>Holm-Bonferroni method</a></li><li>In practice: <strong>Judgment Call</strong>, possibly based on business strategy</li></ul></li></ul></li></ul><p><strong>Different Strategies:</strong></p><ul><li><p>Control probability that <em>any</em> metric shows a false positive $\alpha_{overall}$ -> <strong>family wise error rate (FWER)</strong> (Overal alpha)</p></li><li><p>Control false discovery rate (FDR):</p><p>$$FDR=E[\frac{ \text{false positives} }{ \text{rejections}}]$$</p><ul><li>Out of all of the rejections of the null, that is, all of the metrics that you declare to have a statistically significant difference. How many of them had a real difference as opposed to how many were false positives?</li><li>This really only makes sense if you have a huge number of metrics, say hundreds.</li></ul></li></ul><p>e.g.: Suppose that you have 200 metrics that you&rsquo;re measuring and you capped the false discovery rate at 0.05. What this means is that you&rsquo;re okay with having 5 false positives and 95 true positives in every experiment. The family wise error rate, or the overall alpha in this case, would be one, since you have at least one false positive every time. But the false discovery rate is 0.05.</p><h3 id=analyzing-multiple-metrics>Analyzing Multiple Metrics<a hidden class=anchor aria-hidden=true href=#analyzing-multiple-metrics>#</a></h3><p><strong>How do I actually make a recommendation?</strong></p><ul><li><p>You really have to understand how people are reacting to the change because you can&rsquo;t quantitatively evaluate which one is better.</p></li><li><p>An alternative to using multiple metrics is to use an ‘Overall Evaluation Criterion’ (OEC).</p></li></ul><p><strong>How to choose a good OEC?</strong></p><ul><li>Start with some kind of business analysis (Our company, as a whole, wants to look at 25% revenue plus 75% increase usage of the site.)</li><li>Run a whole bunch of different experiments and validate how they steer you.</li></ul><blockquote><p>Having an OIC doesn&rsquo;t have to be a formal number. It&rsquo;s really just trying to encapsulate what your company cares about. And how much you&rsquo;re going to be balancing something like stay time and clicks.</p></blockquote><h2 id=drawing-conclusion>Drawing Conclusion<a hidden class=anchor aria-hidden=true href=#drawing-conclusion>#</a></h2><p>If you have statistically significant results, now the questions come down to</p><ul><li><p>Do you understand the change?</p></li><li><p>Do you want to launch the change?</p></li></ul><p>What if your change has a positive impact on one slice of your users, but for another slice, there&rsquo;s no impact or there&rsquo;s a negative impact?</p><ul><li>Is it a question about having different users, and how much they like or don&rsquo;t like the change? Have you seen that effect in other experiments? Do you have a bug?</li></ul><p><strong>How do you decide whether to launch your change or not?</strong></p><ul><li><p>Do I have statistically significant and practically significant results in order to justify the change?</p></li><li><p>Do I understand what that change has actually done with regards to user experience?</p></li><li><p>Is it worth it?</p></li></ul><blockquote><p><strong>The end goal is actually making that recommendation that shows your judgment.</strong></p></blockquote><h2 id=gochas-changes-over-time>Gochas: Changes Over Time<a hidden class=anchor aria-hidden=true href=#gochas-changes-over-time>#</a></h2><p>It&rsquo;s good practice to always do a ramp-up when you actually want to launch a change.</p><p>The other thing is to remove all of the filters. So if you&rsquo;re only testing your change on English, for example, you want to test your change during your ramp-up on all users to check if there&rsquo;s been any incidental impact to unaffected users that you didn&rsquo;t test in the original experiment.</p><p><strong>Complication:</strong> the effect may actually flatten out as you ramp up the change.</p><p>Reasons of the effects not being repeatable:</p><ol><li>seasonality effects, holidays<ul><li><p>Capture seasonal / event-driven impacts: <strong>holdback</strong></p><ul><li>You launch your change to everyone except for a small holdback (a set of users), that don&rsquo;t get the change, and you continue comparing their
behavior to the control. Now, in that case you should see the reverse of the impact that you saw in your experiment. And what you can do is you can track that over time until you&rsquo;re confident that your results are actually repeatable.</li></ul></li></ul></li><li><strong>novelty effect</strong> or <strong>change aversion</strong>: as users discover or adopt your change, then their behavior can change and therefore the measured effects can change.<ul><li><strong>Cohort analysis</strong></li></ul></li><li>If you don&rsquo;t control for the budgets properly, the effect can change as you ramp up.</li></ol><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><ol><li>Check that your experiment was set up properly, Look at your end variance.Check that your experiment metrics
are actually looking sane.</li><li>Remember you aren&rsquo;t just looking for statistical significance, you&rsquo;re really making a business decision.</li><li>Can&rsquo;t actually forget the overall business analysis<ul><li>Judgment call with regards to the user experience and the business:<ul><li>What&rsquo;s the engineering cost of maintaining the change?</li><li>Are there customer support or sales issues?</li><li>What&rsquo;s the opportunity cost if you actually choose to launch the change relative to the rewards you&rsquo;re going to get from the change or potentially not launching the change?</li></ul></li></ul></li></ol><h1 id=policy-and-ethics-for-experiments>Policy and Ethics for Experiments<a hidden class=anchor aria-hidden=true href=#policy-and-ethics-for-experiments>#</a></h1><h2 id=four-principles>Four Principles<a hidden class=anchor aria-hidden=true href=#four-principles>#</a></h2><h3 id=1-risk>1. Risk<a hidden class=anchor aria-hidden=true href=#1-risk>#</a></h3><p>In the study, <em>what risk is the participant undertaking</em>? The main threshold is whether the risk exceeds that of “minimal risk”. Minimal risk is defined as the probability and magnitude of harm that a participant would encounter in normal daily life.</p><h3 id=2-benefits>2. Benefits<a hidden class=anchor aria-hidden=true href=#2-benefits>#</a></h3><p>It is important to be able to state what the benefit would be from completing the study.</p><h3 id=3-alternatives>3. Alternatives<a hidden class=anchor aria-hidden=true href=#3-alternatives>#</a></h3><p>In online experiments, the issues to consider are what the other alternative services that a user might have, and what the switching costs might be, in terms of time, money, information, etc.</p><h3 id=4-data-sensitivity>4. Data Sensitivity<a hidden class=anchor aria-hidden=true href=#4-data-sensitivity>#</a></h3><p><em>What data is being collected, and what is the expectation of privacy and confidentiality</em>?</p><ul><li>For new data being collected and stored, how sensitive is the data and what are the internal safeguards for handling that data? E.g., what access controls are there, how are breaches to that security caught and managed, etc.?</li><li>Then, for that data, how will it be used and how will participants’ data be protected? How are participants guaranteed that their data, which was collected for use in the study, will not be used for some other purpose? This becomes more important as the sensitivity of the data increases.</li><li>Finally, what data may be published more broadly, and does that introduce any additional risk to the participants?</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://nancyyanyu.github.io/tags/ab-testing/>ab-testing</a></li></ul><nav class=paginav><a class=prev href=https://nancyyanyu.github.io/posts/ab_testing_final_project/><span class=title>« Prev</span><br><span>A/B Testing Final Project</span></a>
<a class=next href=https://nancyyanyu.github.io/posts/apache_pyspark-sql-and-dataframe/><span class=title>Next »</span><br><span>Spark SQL & DataFrame, SparkETL</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Study Notes of Udacity A/B Testing on twitter" href="https://twitter.com/intent/tweet/?text=Study%20Notes%20of%20Udacity%20A%2fB%20Testing&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fab_testing_udacity%2f&amp;hashtags=abtesting"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Notes of Udacity A/B Testing on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fab_testing_udacity%2f&amp;title=Study%20Notes%20of%20Udacity%20A%2fB%20Testing&amp;summary=Study%20Notes%20of%20Udacity%20A%2fB%20Testing&amp;source=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fab_testing_udacity%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Notes of Udacity A/B Testing on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fab_testing_udacity%2f&title=Study%20Notes%20of%20Udacity%20A%2fB%20Testing"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Notes of Udacity A/B Testing on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fab_testing_udacity%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Notes of Udacity A/B Testing on whatsapp" href="https://api.whatsapp.com/send?text=Study%20Notes%20of%20Udacity%20A%2fB%20Testing%20-%20https%3a%2f%2fnancyyanyu.github.io%2fposts%2fab_testing_udacity%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Notes of Udacity A/B Testing on telegram" href="https://telegram.me/share/url?text=Study%20Notes%20of%20Udacity%20A%2fB%20Testing&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fab_testing_udacity%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Notes of Udacity A/B Testing on ycombinator" href="https://news.ycombinator.com/submitlink?t=Study%20Notes%20of%20Udacity%20A%2fB%20Testing&u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fab_testing_udacity%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://nancyyanyu.github.io/>Nancy's Notebook</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>