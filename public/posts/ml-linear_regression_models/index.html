<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Study Note: Linear Regression Part I - Linear Regression Models | Nancy's Notebook</title><meta name=keywords content="Linear Regression,Regression"><meta name=description content="Simple Linear Regression Models
Linear Regression Model


Form of the linear regression model: $y=\beta_{0}+\beta_{1}X+\epsilon$.


Training data: ($x_1$,$y_1$) &mldr; ($x_N$,$y_N$). Each $x_{i} =(x_{i1},x_{i2},&mldr;,x_{ip})^{T}$ is a vector of feature measurements for the $i$-th case.


Goal: estimate the parameters $β$


Estimation method: Least Squares, we pick the coeﬃcients $β =(β_0,β_1,&mldr;,β_p)^{T}$ to minimize the residual sum of squares


Assumptions:

Observations $y_i$ are uncorrelated and have constant variance $\sigma^2$;
$x_i$ are ﬁxed (non random)
The regression function $E(Y |X)$ is linear, or the linear model is a reasonable approximation.
"><meta name=author content="Me"><link rel=canonical href=https://nancyyanyu.github.io/posts/ml-linear_regression_models/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Study Note: Linear Regression Part I - Linear Regression Models"><meta property="og:description" content="Simple Linear Regression Models
Linear Regression Model


Form of the linear regression model: $y=\beta_{0}+\beta_{1}X+\epsilon$.


Training data: ($x_1$,$y_1$) &mldr; ($x_N$,$y_N$). Each $x_{i} =(x_{i1},x_{i2},&mldr;,x_{ip})^{T}$ is a vector of feature measurements for the $i$-th case.


Goal: estimate the parameters $β$


Estimation method: Least Squares, we pick the coeﬃcients $β =(β_0,β_1,&mldr;,β_p)^{T}$ to minimize the residual sum of squares


Assumptions:

Observations $y_i$ are uncorrelated and have constant variance $\sigma^2$;
$x_i$ are ﬁxed (non random)
The regression function $E(Y |X)$ is linear, or the linear model is a reasonable approximation.
"><meta property="og:type" content="article"><meta property="og:url" content="https://nancyyanyu.github.io/posts/ml-linear_regression_models/"><meta property="og:image" content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Study Note: Linear Regression Part I - Linear Regression Models"><meta name=twitter:description content="Simple Linear Regression Models
Linear Regression Model


Form of the linear regression model: $y=\beta_{0}+\beta_{1}X+\epsilon$.


Training data: ($x_1$,$y_1$) &mldr; ($x_N$,$y_N$). Each $x_{i} =(x_{i1},x_{i2},&mldr;,x_{ip})^{T}$ is a vector of feature measurements for the $i$-th case.


Goal: estimate the parameters $β$


Estimation method: Least Squares, we pick the coeﬃcients $β =(β_0,β_1,&mldr;,β_p)^{T}$ to minimize the residual sum of squares


Assumptions:

Observations $y_i$ are uncorrelated and have constant variance $\sigma^2$;
$x_i$ are ﬁxed (non random)
The regression function $E(Y |X)$ is linear, or the linear model is a reasonable approximation.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nancyyanyu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Study Note: Linear Regression Part I - Linear Regression Models","item":"https://nancyyanyu.github.io/posts/ml-linear_regression_models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Study Note: Linear Regression Part I - Linear Regression Models","name":"Study Note: Linear Regression Part I - Linear Regression Models","description":"Simple Linear Regression Models Linear Regression Model Form of the linear regression model: $y=\\beta_{0}+\\beta_{1}X+\\epsilon$.\nTraining data: ($x_1$,$y_1$) \u0026hellip; ($x_N$,$y_N$). Each $x_{i} =(x_{i1},x_{i2},\u0026hellip;,x_{ip})^{T}$ is a vector of feature measurements for the $i$-th case.\nGoal: estimate the parameters $β$\nEstimation method: Least Squares, we pick the coeﬃcients $β =(β_0,β_1,\u0026hellip;,β_p)^{T}$ to minimize the residual sum of squares\nAssumptions:\nObservations $y_i$ are uncorrelated and have constant variance $\\sigma^2$; $x_i$ are ﬁxed (non random) The regression function $E(Y |X)$ is linear, or the linear model is a reasonable approximation. ","keywords":["Linear Regression","Regression"],"articleBody":"Simple Linear Regression Models Linear Regression Model Form of the linear regression model: $y=\\beta_{0}+\\beta_{1}X+\\epsilon$.\nTraining data: ($x_1$,$y_1$) … ($x_N$,$y_N$). Each $x_{i} =(x_{i1},x_{i2},…,x_{ip})^{T}$ is a vector of feature measurements for the $i$-th case.\nGoal: estimate the parameters $β$\nEstimation method: Least Squares, we pick the coeﬃcients $β =(β_0,β_1,…,β_p)^{T}$ to minimize the residual sum of squares\nAssumptions:\nObservations $y_i$ are uncorrelated and have constant variance $\\sigma^2$; $x_i$ are ﬁxed (non random) The regression function $E(Y |X)$ is linear, or the linear model is a reasonable approximation. Residual Sum of Squares Residual: $e_i = y_i−\\hat{y_i}$ represents the ith residual—this is the difference between the i-th observed response value and the i-th response value that is predicted by our linear model. $$ \\begin{align} \\text{Residual sum of squares(RSS) } \\quad R(\\beta)\u0026=e_1^2+e_2^2+e_3^2+…e_n^2 \\ \u0026=\\sum_{i=1}^{N}(y_{i}-f(x_{i}))^2 \\ \u0026=\\sum_{i=1}^{N}(y_{i}-\\beta_{0}-\\sum_{j=1}^{p}X_{ij}\\beta_{j})^2 \\end{align} $$\nSolution Denote by $X$ the $N × (p + 1) $ matrix with each row an input vector ( $1$ in the ﬁrst position), and similarly let $y$ be the $N$-vector of outputs in the training set.\n$$ \\begin{align} \\min RSS(\\beta)= (y-\\mathbf{X}\\beta)^T(y-\\mathbf{X}\\beta) \\end{align} $$ A quadratic function in the $p + 1$ parameters\nTaking derivatives:\n$$ \\begin{align} \\frac{\\partial RSS}{\\partial \\beta}=-2\\mathbf{X}^T(y-\\mathbf{X}\\beta) \\end{align} $$\n$$ \\begin{align} \\frac{\\partial^2 RSS}{\\partial \\beta \\partial \\beta^T}=2\\mathbf{X}^T\\mathbf{X} \\end{align} $$\nAssuming (for the moment) that $\\mathbf{X}$ has full column rank (i.e. each of the columns of the matrix are linearly independent), hence $\\mathbf{X}^T\\mathbf{X}$ is positive deﬁnite (every eigenvalue is positive), we set the ﬁrst derivative to zero: $\\mathbf{X}^T(y-\\mathbf{X}\\beta)=0$\nLeast squares coefficient estimates for simple linear regression: $$ \\begin{align} \\Rightarrow \\hat{\\beta_1}\u0026=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty \\ \u0026=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2} \\ \\hat{\\beta_0}\u0026=\\bar{y}-\\hat{\\beta_1}\\bar{x} \\end{align} $$ where $\\bar{y}=\\sum_{i=1}^ny_i/n$, $\\bar{x}=\\sum_{i=1}^nx_i/n$ are the sample means.\nFitted values at the training inputs: $\\hat{y}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$\nHat matrix: $H=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$\nAssessing the Accuracy of the Coefficient Estimates Assume that the true relationship between $X$ and $Y$ takes the form $Y = f(X) + \\epsilon$ for some unknown function $f$, where $\\epsilon$ is a mean-zero random error term.\nLeast squares line: $$ \\begin{align} \\hat{y_i} = \\hat{β_0} + \\hat{β_1}x_i \\end{align} $$ Population regression line: $$ \\begin{align} Y=\\beta_0+\\beta_1X+\\epsilon \\end{align} $$ The error term is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in $Y$ , and there may be measurement error. We typically assume that the error term is independent of $X$.\nPopulation V.S. Sample The true relationship is generally not known for real data, but the least squares line can always be computed using the coefficient estimates.\nWhy there are two different lines describe the relationship between the predictor and the response?\nThe concept of these two lines is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of a large population. The sample mean $\\bar{x}=\\sum_{i=1}^nx_i/n$ and the population mean $\\mu$ are different, but in general the sample mean $\\bar{x}$ will provide a good estimate of the population mean $\\mu$. Unbiased\nIf we use the sample mean $\\bar{x}$ to estimate μ, this estimate is unbiased, in the sense that on average, we expect $\\bar{x}$ to equal $μ$. An unbiased estimator does not systematically over- or under-estimate the true parameter. Standard Error How accurate is the sample mean $\\hat{\\mu}$ as an estimate of μ?\nStandard error of $\\hat{\\mu}$: standard deviation of the means; average amount that this estimate $\\hat{\\mu}$ differs from the actual value of $μ$. $$ \\begin{align} Var(\\hat{\\mu})=SE(\\hat{\\mu})^2=\\frac{s^2}{n} \\ s=\\sqrt{\\frac{\\sum_{i}\\epsilon^2}{n-1}} \\end{align} $$ where $σ$ is the standard deviation of each of the realizations $y_i$ of $Y$ provided that the $n$ observations are uncorrelated. Standard Deviation V.S. Standard Error\nThe standard deviation (SD) measures the amount of variability, or dispersion, for a subject set of data from the mean The standard error of the mean (SEM) measures how far the sample mean of the data is likely to be from the true population mean. Statistical Properties of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ How close $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are to the true values $\\beta_0$ and $\\beta_1$? $$ \\begin{align} Var(\\hat{\\beta_0})^2\u0026=\\sigma^2\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2} \\right] \\ Var(\\hat{\\beta_1})^2\u0026=\\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2} \\end{align} $$\n​\twhere $\\sigma^2 = Var(\\epsilon)$\nProof: $$ \\begin{align} \\hat{\\beta_1}\u0026=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2} = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})y_i}{\\sum_{i=1}^n(x_i-\\bar{x})^2} \\ Var(\\hat{\\beta_1})\u0026=Var(\\frac{\\sum_{i=1}^n(x_i-\\bar{x})y_i}{\\sum_{i=1}^n(x_i-\\bar{x})^2} ) \\ \u0026=Var(\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(\\beta_0+\\beta_1x_i+\\epsilon_i)}{\\sum_{i=1}^n(x_i-\\bar{x})^2}) \\ \u0026=Var(\\frac{\\sum_{i=1}^n(x_i-\\bar{x})\\epsilon_i}{\\sum_{i=1}^n(x_i-\\bar{x})^2}) \\quad\\text{ note only $\\epsilon$ is a r.v.} \\ \u0026=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})^2Var(\\epsilon_i)}{(\\sum_{i=1}^n(x_i-\\bar{x})^2)^2} \\quad \\text{independence of $\\epsilon_i$ and, $Var(𝑘𝑋)=𝑘^2Var(𝑋)$} \\ \u0026=\\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2} \\end{align} $$\nFor these formulas to be strictly valid, we need to assume that the errors $\\epsilon_i$ for each observation are uncorrelated with common variance $σ^2$.\nEstimate $\\sigma^2$ Residual standard error(RSE): $\\sigma$ is not known, but can be estimated from the data. This estimate is known as the residual standard error (an unbiased estimate of $σ$) $$ \\begin{align} RSE=\\hat{\\sigma}=\\sqrt{RSS/(n-2)}=\\sqrt{\\frac{1}{N-2}\\sum^{N}_{i=1}(y_i-\\hat{y_i})^2 } \\end{align} $$\nThe divisor n − 2 is used rather than n because two parameters have been estimated from the data, giving n − 2 degrees of freedom. Sampling Properties of $\\beta$ The variance–covariance matrix of the least squares parameter estimates: $$ \\begin{align} Var(\\hat{\\beta})=(\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2 \\end{align} $$ Unbiased estimate of $\\sigma^2$: $$ \\begin{align} \\hat{\\sigma}^2=\\frac{1}{N-p-1}\\sum^{N}_{i=1}(y_i-\\hat{y_i})^2 \\end{align} $$ If the errors, $\\epsilon_i$ , are independent normal random variables, then the estimated slope and intercept, being linear combinations of independent normally distributed random variables, are normally distributed as well.\nMore generally, if $\\epsilon_i$ are independent and the $x_i$ satisfy certain assumptions, a version of the central limit theorem implies that, for large n, the estimated slope and intercept are approximately normally distributed.\nThus, $\\beta$ follows multivariate normal distribution with mean vector and variance–covariance matrix: $$ \\begin{align}\\hat{\\beta} \\sim N(\\beta,(\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2 ) \\end{align} $$ Also, a chi-squared distribution with $N −p−1$ degrees of freedom: $$ \\begin{align} (N-p-1)\\hat{\\sigma}^2 \\sim \\sigma^2 \\chi_{N-p-1}^{2} \\end{align} $$ ($\\hat{\\beta}$ and $\\hat{\\sigma^2}$ are indep.)\nWe use these distributional properties to form tests of hypothesis and conﬁdence intervals for the parameters $\\beta_j$:\nConfidence Intervals\nA 95% confidence confidence interval: is defined as a range of values such that with 95% interval probability, the range will contain the true unknown value of the parameter.\nFor linear regression, the 95% confidence interval for $β_1$ approximately takes the form $$ \\begin{align} \u0026\\hat{\\beta_1} \\pm 1.96 \\cdot SE(\\hat{\\beta_1}) \\ \u0026SE(\\hat{\\beta_1}) =\\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2} \\end{align} $$ (which relies on the assumption that the errors are Gaussian. Also, the factor of 2 in front of the $SE(\\hat{\\beta_1})$ term will vary slightly depending on the number of observations n in the linear regression. To be precise, rather than the number 2, it should contain the 97.5% quantile of a t-distribution with n−2 degrees of freedom.)\nHypothesis Tests The most common hypothesis test involves testing the null test hypothesis of\nH_0: No relationship between X and Y or β1=0 versus the alternative hypothesis\nH_a : There is some relationship between X and Y or β1≠0 To test the null hypothesis, we need to determine whether $\\hat{\\beta_1}$, our estimate for $\\beta_1$, is sufficiently far from zero that we can be confident that $\\beta_1$ is non-zero $\\Rightarrow$ it depends on $SE( \\hat{\\beta_1}$)\nIf $SE( \\hat{\\beta_1}$) is small, then even relatively small values of $\\hat{\\beta_1}$ may provide strong evidence that $\\beta_1 \\neq 0$, and hence that there is a relationship between X and Y t-statistic To test the hypothesis that a particular coeﬃcient $\\beta_j= 0$, we form the standardized coeﬃcient or Z-score $$ \\begin{align} t=\\frac{\\hat{\\beta_1}-0}{SE(\\hat{\\beta_1})} \\ or \\quad z_j=\\frac{\\hat{\\beta_j}-0}{\\hat{\\sigma}\\sqrt{\\upsilon_j}} \\end{align} $$ where $\\upsilon_j$ is the j-th diagonal element of $(\\mathbf{X}^T\\mathbf{X})^{-1}$\nwhich measures the number of standard deviations that $\\hat{\\beta_1}$ is away from 0.If there really is no relationship between $X$ and $Y$ , then we expect it will have a $t$-distribution with $n−2$ degrees of freedom.\nUnder the null hypothesis that $\\beta_j= 0$, $z_j$ is distributed as $t_{N-p-1}$, and hence a large (absolute) value of $z_j$ will lead to rejection of this null hypothesis. If $\\hat{\\sigma}$ is replaced by a known value $σ$, then $z_j$ would have a standard normal distribution.\np-value The probability of observing any value $≥ t$ or $≤ -t$, assuming $β_1 = 0$. (Here $|t|=2.17, p-value=0.015$. The area in red is 0.015 + 0.015 = 0.030, 3%. If we had chosen a significance level of 5%, this would mean that we had achieved statistical significance. We would reject the null hypothesis in favor of the alternative hypothesis.)\nInterpretation:a small p-value indicates It is unlikely to observe such a substantial association between the predictor and the response due to LUCK, in the absence of any real association between the predictor and the response. We reject the null hypothesis—that is, we declare a relationship to exist between X and Y F-statistic: To test if a categorical variable with $k$ levels can be excluded from a model, we need to test whether the coeﬃcients of the dummy variables used to represent the levels can all be set to zero. Here we use the $F$ statistic：\n$$ \\begin{align} F=\\frac{(RSS_0-RSS_1)/(p_1-p_0)}{RSS_1/(N-p_1-1)} \\end{align} $$\n$RSS_1$ is the residual sum-of-squares for the least squares ﬁt of the bigger model with $p_1+1$ parameters; $RSS_0$ the same for the nested smaller model with $p_0 +1$ parameters, having $p_1 −p_0$ parameters constrained to be zero. The $F$ statistic measures the change in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an estimate of $\\sigma^2$.\nUnder the Gaussian assumptions, and the null hypothesis that the smaller model is correct, the $F$ statistic will have a $F_{p_1-p_0,N-p_1-1}$ distribution.\nThe Gauss–Markov Theorem We focus on estimation of any linear combination of the parameters $\\theta=\\alpha^T\\beta$, for example, predictions $f(x_0)=x^T_0\\beta$ are of this form.The least squares estimate of $\\alpha^T\\beta$ is: $$ \\begin{equation} \\hat{\\theta}=\\alpha^T\\hat{\\beta}=\\alpha^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\end{equation} $$ Considering $\\mathbf{X}$ to be ﬁxed, this is a linear function $\\mathbf{c}^T_0\\mathbf{y}$ of the response vector $\\mathbf{y}$.If we assume that the linear model is correct, $\\alpha^T\\hat{\\beta}$ is unbiased.\nThe Gauss–Markov theorem states that if we have any other linear estimator $\\tilde{\\theta}=\\mathbf{c}^T\\mathbf{y}$ that is unbiased for $\\alpha^T\\beta$, that is, $E(\\mathbf{c}^T\\mathbf{y})= \\alpha^T\\beta$, then: $$ \\begin{equation} Var(\\alpha^T\\hat{\\beta})\\leq Var(\\mathbf{c}^T\\mathbf{y}) \\end{equation} $$\nAssessing the Accuracy of the Model The quality of a linear regression fit is typically assessed using two related quantities: the residual standard error (RSE) and the $R^2$ statistic.\nResidual Standard Error The $RSE$ is an estimate of the standard deviation of $\\epsilon$: the average amount that the response will deviate from the true regression line $$ \\begin{align} RSS\u0026=\\sum_{i=1}^n(y_i-\\hat{y})^2 \\ RSE\u0026=\\sqrt{\\frac{1}{n-2}RSS}=\\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n(y_i-\\hat{y})^2} \\end{align} $$ In the case of the advertising data, we see from the linear regression output in Table 3.2 that the RSE is 3.26. In other words, actual sales in each market deviate from the true regression line by approximately 3,260 units, on average.\nThe mean value of sales over all markets is approximately 14,000 units, and so the percentage error is 3,260/14,000 = 23%.\nThe RSE is considered a measure of the lack of fit of the model $Y = β_0 + β_1X + \\epsilon$ to the data.\n$R^2$ Statistic The $RSE$ is measured in the units of $Y$ , it is not always clear what constitutes a good $RSE$.\nThe $R^2$ statistic takes the form of a proportion—the proportion of variance explained—and so it always takes on a value between 0 and 1, and is independent of the scale of $Y$ . $$ \\begin{align} R^2 = (TSS − RSS)/TSS= 1− RSS/TSS = 1-\\frac{\\sum(y_i-\\hat{y})^2}{\\sum(y_i-\\bar{y})^2} \\end{align} $$ TSS(total sum of squares): $\\sum(y_i-\\bar{y})^2$ - the amount of variability inherent in the response before the regression is performed\nRSS: $\\sum_{i=1}^n(y_i-\\hat{y})^2$ - the amount of variability that is left unexplained after performing the regression\n(TSS−RSS): measures the amount of variability in the response that is explained (or removed) by performing the regression, and $R^2$ measures the proportion of variability in $Y$ that can be explained using $X$.\nInterpretation：\nclose to 1 : a large proportion of the variability in the response has been explained by the regression. close to 0 : the regression did not explain much of the variability in the response The linear model is wrong The inherent error $σ^2$ is high, or both. Squared Correlation V.S. R2 Statistic Correlation: $$ \\begin{align} Corr(X,Y)=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}}=\\hat{\\beta}1\\frac{\\sqrt{\\sum{i=1}^n(x_i-\\bar{x})^2}}{\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}} \\end{align} $$\nthe correlation is zero if and only if the slope is zero.\nalso a measure of the linear relationship between X and Y.\nIn the simple linear regression setting, $R^2 = [Cor]^2$. In other words, the squared correlation and the R2 statistic are identical\np-Value v.s. $R^2$ Referring answer fromn Faye Anderson:\nThere is no established association/relationship between p-value and R-square. This all depends on the data (i.e.; contextual).\nR-square value tells you how much variation is explained by your model. So $R^2=0.1$ means that your model explains 10% of variation within the data. The greater R-square the better the model.\nWhereas p-value tells you about the F-statistic hypothesis testing of the “fit of the intercept-only model and your model are equal”. So if the p-value is less than the significance level (usually 0.05) then your model fits the data well.\nThus you have four scenarios:\n1) low R-square and low p-value (p-value \u003c= 0.05) : means that your model doesn’t explain much of variation of the data but it is significant (better than not having a model)\n2) low R-square and high p-value (p-value \u003e 0.05) : means that your model doesn’t explain much of variation of the data and it is not significant (worst scenario)\n3) high R-square and low p-value : means your model explains a lot of variation within the data and is significant (best scenario)\n4) high R-square and high p-value : means that your model explains a lot of variation within the data but is not significant (model is worthless)\nVariance \u0026 Bias Consider the mean squared error of an estimator $\\tilde{\\theta}$ in estimating $\\theta$:\n$$ \\begin{align} MSE(\\tilde{\\theta})\u0026= E(\\tilde{\\theta}-\\theta)^2 \\ \u0026= E(\\tilde{\\theta^2}+\\theta^2-2\\theta\\tilde{\\theta}) \\ \u0026= E(\\tilde{\\theta^2})-E^2(\\tilde{\\theta})+E^2(\\tilde{\\theta})+E(\\theta^2-2\\theta\\tilde{\\theta})\\ \u0026= Var(\\tilde{\\theta})+[E(\\tilde{\\theta})-\\theta]^2 \\end{align} $$ The ﬁrst term is the variance, while the second term is the squared bias.\nThe Gauss-Markov theorem implies that the least squares estimator has the smallest mean squared error of all linear estimators with no bias. However, there may well exist a biased estimator with smaller mean squared error. Such an estimator would trade a little bias for a larger reduction in variance.\nFrom a more pragmatic point of view, most models are distortions of the truth, and hence are biased; picking the right model amounts to creating the right balance between bias and variance.\nMean squared error is intimately related to prediction accuracy. Consider the prediction of the new response at input $x_0$, $$ \\begin{equation} y_0=f(x_0)+\\epsilon_0 \\end{equation} $$\nPrediction error \u0026 MSE The expected prediction error of an estimate $\\tilde{f}(x_0)=x_0^T\\tilde{\\beta}$:\n$$ \\begin{align} E(y_0-\\tilde{f}(x_0))^2 \u0026=E(f(x_0)+\\epsilon_0-x_0^T\\tilde{\\beta})^2 \\ \u0026=E(\\epsilon_0^2)+E(f(x_0)-x_0^T\\tilde{\\beta})^2-2E(\\epsilon_0(f(x_0)-x_0^T\\tilde{\\beta})) \\ \u0026=\\sigma^2+E(f(x_0)-x_0^T\\tilde{\\beta})^2 \\ \u0026=\\sigma^2+MSE(\\tilde{f}(x_0)) \\end{align} $$ Therefore, expected prediction error and mean squared error diﬀer only by the constant $\\sigma^2$, representing the variance of the new observation $y_0$.\nMultiple Linear Regression Multiple linear regression model takes the form: $$ \\begin{align} Y=\\beta_0+\\beta_1X_1+,,,+\\beta_pX_p+\\epsilon \\end{align} $$\nEstimating the Regression Coefficients We choose $β_0, β_1, . . . , β_p$ to minimize the sum of squared residuals $$ \\begin{align} RSS\u0026=\\sum_{i=1}^n(y_i-\\hat{y}i)^2 \\ \u0026=\\sum{i=1}^n(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_{i1}-,,,-\\hat{\\beta_p}x_{ip})^2 \\end{align} $$ Does it make sense for the multiple regression to suggest no relationship between sales and newspaper while the simple linear regression implies the opposite?\nNotice that the correlation between radio and newspaper is 0.35. In markets where we spend more on radio our sales will tend to be higher, and as our correlation matrix shows, we also tend to spend more on newspaper advertising in those same markets. Hence, in a simple linear regression which only examines sales versus newspaper, we will observe that higher values of newspaper tend to be associated with higher values of sales, even though newspaper advertising does not actually affect sales. Some Important Questions 1. Is There a Relationship Between the Response and Predictors? Hypothesis Test We use a hypothesis test to answer this question.\nWe test the null hypothesis\nH_0 : β1 = β2 = · · · = βp = 0 versus the alternative\nH_a : at least one βj is non-zero This hypothesis test is performed by computing the F-statistic, $$ \\begin{align} F=\\frac{(TSS-RSS)/p}{RSS/(n-p-1)} \\end{align} $$ where $TSS =\\sum(y_i − \\bar{y})^2$ and $RSS =\\sum(y_i−\\hat{y}_i)^2$.\nIf the linear model assumptions are correct, one can show that $$ \\begin{align} E[RSS/(n-p-1)]=\\sigma^2 \\end{align} $$ and that, provided $H_0$ is true, $$ \\begin{align} E[(TSS-RSS)/p]=\\sigma^2 \\end{align} $$\nWhen there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to 1. On the other hand, if $H_a$ is true, then $E[(TSS-RSS)/p]\u003e\\sigma^2$, so we expect $F$ to be greater than 1. 2. How large does the F-statistic need to be before we can reject $H_0$ and conclude that there is a relationship? When $n$ is large, an F-statistic that is just a little larger than 1 might still provide evidence against $H_0$. In contrast, a larger F-statistic is needed to reject $H_0$ if $n$ is small. For the advertising data, the p-value associated with the F-statistic in Table 3.6 is essentially zero, so we have extremely strong evidence that at least one of the media is associated with increased sales. To test that a particular subset of $q$ of the coefficients are zero\nThis corresponds to a null hypothesis\n$$ H_0 : β_{p-q+1} = β-{p-q+2} = · · · = β_p = 0 $$ In this case we fit a second model that uses all the variables except those last $q$. Suppose that the residual sum of squares for that model is $RSS_0$. Then the appropriate F-statistic is $$ \\begin{align} F=\\frac{(RSS_0-RSS)/q}{RSS/(n-p-1)} \\end{align} $$\nF-statistics v.s. t-statistics Equivalency: In Table 3.4, for each individual predictor a t-statistic and a p-value were reported. These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. It turns out that each of these are exactly equivalent to the F-test that omits that single variable from the model, leaving all the others in—i.e. q=1 in the model. So it reports the partial effect of adding that variable to the model.\nThe square of each t-statistic is the corresponding F-statistic.\n$p$ is large: If any one of the p-values for the individual variables is very small, then at least one of the predictors is related to the response. However, this logic is flawed, especially when the number of predictors $p$ is large.\nIf we use the individual t-statistics and associated p-values to decide whether there is any association between the variables and the response, high chance we will incorrectly conclude there is a relationship. However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors. $p \u003e n$: more coefficients $β_j$ to estimate than observations from which to estimate them.\ncannot even fit the multiple linear regression model using least squares, 3. Do all the predictors help to explain Y , or is only a subset of the predictors useful? Variable Selection Various statistics can be used to judge the quality of a model: Mallow’s Cp, Akaike informa-Mallow’s Cp tion criterion (AIC) Bayesian information criterion (BIC) adjusted R2 There are three classical approaches to select models: Forward selection Backward selection Mixed selection 4. How well does the model fit the data? Two of the most common numerical measures of model fit are the RSE and $R^2$\n$R^2$ Statistics An $R^2$ value close to $1$ indicates that the model explains a large portion of the variance in the response variable. $$ \\begin{align} R^2 = (TSS − RSS)/TSS= 1− RSS/TSS \\end{align} $$ Recall that in simple regression, $R^2$ is the square of the correlation of the response and the variable. In multiple linear regression, it turns out that it equals $Cor(Y, \\hat{Y} )^2$, the square of the correlation between the response and the fitted linear model; in fact one property of the fitted linear model is that it maximizes this correlation among all possible linear models.\n$R^2$ will always increase when more variables are added to the model, even if those variables are only weakly associated with the response.\nThis is due to the fact that adding another variable to the least squares equations must allow us to fit the training data (though not necessarily the testing data) more accurately. The fact that adding newspaper advertising to the model containing only TV and radio advertising leads to just a tiny increase in $R_2$ provides additional evidence that newspaper can be dropped from the model. RSE RSE is defined as $$ \\begin{align} RSE=\\sqrt{\\frac{RSS}{n-p-1}} \\end{align} $$ Models with more variables can have higher RSE if the decrease in RSS is small relative to the increase in $p$.\nGraphical summaries It suggests a synergy or interaction effect between the advertising media, whereby combining the media together results in a bigger boost to sales than using any single medium\n5. Given a set of predictor values, what response value should we predict, and how accurate is our prediction? Uncertainty associated with prediction The coefficient estimates $\\hat{\\beta_0},\\hat{\\beta_1},…,\\hat{\\beta_p}$ are estimates for $β_0, β_1, . . . , β_p$. That is, the least squares plane $$ \\begin{align} \\hat{Y}=\\hat{\\beta_0}+\\hat{\\beta_1}X_1+,…+\\hat{\\beta_p}X_p \\end{align} $$ is only an estimate for the true population regression plane $$ \\begin{align} f(X)=\\beta_0+\\beta_1X_1+,…+\\beta_pX_p \\end{align} $$ The inaccuracy in the coefficient estimates is related to the reducible error. We can compute a confidence interval in order to determine how close $\\hat{Y}$ will be to $f(X)$. In practice, assuming a linear model for $f(X)$ is almost always an approximation of reality, so there is an additional source of potentially reducible error which we call model bias. Even if we knew $f(X)$—true values for $β_0, β_1, . . . , β_p$—the response value cannot be predicted perfectly because of the random error $\\epsilon$ –irreducible error. How much will Y vary from $\\hat{Y}$? – prediction intervals Prediction intervals Prediction intervals are always wider than confidence intervals\nBecause they incorporate both the error in the estimate for f(X) (the reducible error) and the uncertainty as to how much an individual point will differ from the population regression plane (the irreducible error). E.g.\nconfidence interval : quantify the uncertainty surrounding the average sales over a large number of cities. prediction interval : quantify the uncertainty surrounding sales for a particular city. Multiple Regression from Simple Univariate Regression Gram-Schmidt正交化\nSimple Univariate Regression Suppose ﬁrst that we have a univariate model with no intercept: $$ \\begin{align} \\mathbf{Y}=\\mathbf{X}\\beta+\\epsilon \\end{align} $$ The least squares estimate and residuals are: $$ \\begin{align} \\hat{\\beta}\u0026=\\frac{\\sum_1^Nx_iy_i}{\\sum_1^Nx_i^2} \\ r_i\u0026=y_i-x_i\\hat{\\beta} \\end{align} $$ The least squares estimate and residuals are: $$ \\begin{align} \\hat{\\beta}\u0026=\\frac{\\sum_1^Nx_iy_i}{\\sum_1^Nx_i^2} \\ r_i\u0026=y_i-x_i\\hat{\\beta} \\end{align} $$ Let $\\mathbf{y}=(y_1,…,y_N)^T$,$\\mathbf{x}=(x_1,…,x_N)^T$\nDefine the inner product between $\\mathbf{x}$ and $\\mathbf{y}$: $$ \\begin{align} \u003c\\mathbf{x},\\mathbf{y}\u003e=\\sum_1^Nx_iy_i=\\mathbf{x}^T\\mathbf{y} \\end{align} $$ Then, $$ \\begin{align} \\hat{\\beta}\u0026=\\frac{\u003c\\mathbf{x},\\mathbf{y}\u003e}{\u003c\\mathbf{x},\\mathbf{x}\u003e} \\ \\mathbf{r}\u0026=\\mathbf{y}-\\mathbf{x}\\hat{\\beta} \\end{align} $$ Suppose the inputs $x_1, x_2,…, x_p$ (the columns of the data matrix $\\mathbf{X}$) are orthogonal; that is $\u003c\\mathbf{x_k},\\mathbf{x_j}\u003e=0$. Then the multiple least squares estimates $\\hat{\\beta_j}$ are equal to $\\frac{\u003c\\mathbf{x_j},\\mathbf{y}\u003e}{\u003c\\mathbf{x_j},\\mathbf{x_j}\u003e}$—the univariate estimates. In other words, when the inputs are orthogonal, they have no eﬀect on each other’s parameter estimates in the model.\nOrthogonalization $$ \\begin{align} \\hat{\\beta}_1\u0026=\\frac{\u003c\\mathbf{x}-\\bar{x}\\mathbf{1},\\mathbf{y}\u003e}{\u003c\\mathbf{x}-\\bar{x}\\mathbf{1},\\mathbf{x}-\\bar{x}\\mathbf{1}\u003e} \\ \\end{align} $$\n$\\bar{x}=\\sum_ix_i/N$; $\\mathbf{1}$, the vector of N ones; Steps:\nregress $\\mathbf{x}$ on $\\mathbf{1}$ to produce the residual $\\mathbf{z}=\\mathbf{x}-\\bar{x}\\mathbf{1}$; regress $\\mathbf{y}$ on the residual $\\mathbf{z}$ to give the coeﬃcient $\\hat{\\beta}_1$ Regress $\\mathbf{a}$ on $\\mathbf{b}$ ($\\mathbf{b}$ is adjusted for $\\mathbf{a}$),(or $\\mathbf{b}$ is “orthogonalized” with respect to $\\mathbf{a}$); a simple univariate regression of $\\mathbf{b}$ on a with no intercept, producing coeﬃcient $\\hat{\\lambda}=\\frac{\u003c\\mathbf{a},\\mathbf{b}\u003e}{\u003c\\mathbf{a},\\mathbf{a}\u003e}$ and residual vector $ \\mathbf{b}-\\hat{\\lambda} \\mathbf{a}$.\nThe orthogonalization does not change the subspace spanned by $x_1$ and $x_2$, it simply produces an orthogonal basis for representing it.\nGram–Schmidt procedure for multiple regression ALGORITHM 3.1 Regression by Successive Orthogonalization\nInitialize $\\mathbf{z_0}=\\mathbf{x_0}=\\mathbf{1}$. For j=1,2,…,1,,…,p, Regress $\\mathbf{x_j}$ on $\\mathbf{z_0},\\mathbf{z_1},…,\\mathbf{z_{j-1}}$ to produce coeﬃcients $\\hat{\\lambda}{l,j}=\\frac{\u003c\\mathbf{z_l},\\mathbf{x_j}\u003e}{\u003c\\mathbf{z_l},\\mathbf{z_l}\u003e}$, l=0,1,…,j-1, and residual vector $\\mathbf{z_j}=\\mathbf{x_j}-\\sum{k=0}^{j-1}\\hat{\\lambda_{kj}}\\mathbf{z_k}$ Regress $\\mathbf{y}$ on the residual $\\mathbf{z_p}$ to give the estimate $\\hat{\\beta_p}=\\frac{\u003c\\mathbf{z_p},\\mathbf{y}\u003e}{\u003c\\mathbf{z_p},\\mathbf{z_p}\u003e}$. Note:\nEach of the $\\mathbf{x}_j$ is a linear combination of the $\\mathbf{z}_k$, $k ≤ j$. Since the $\\mathbf{z}_j$ are all orthogonal, they form a basis for the column space of $\\mathbf{X}$, and hence the least squares projection onto this subspace is $\\mathbf{\\hat{y}}$. By rearranging the $x_j$ , any one of them could be in the last position, and a similar results holds. The multiple regression coeﬃcient $\\mathbf{x}_j$ represents the additional contribution of $\\mathbf{x}j$ on $\\mathbf{y}$, after $\\mathbf{x}j$ has been adjusted for $x_0, x_1,…, x{j−1},x{j+1},…, x_p$. Precision of Coefficient Estimation If $\\mathbf{x}_p$ is highly correlated with some of the other $\\mathbf{x}_k$’s, the residual vector $\\mathbf{z}_p$ will be close to zero, and the coeﬃcient $\\mathbf{x}_j$ will be very unstable.\nFrom $\\hat{\\beta_p}=\\frac{\u003c\\mathbf{z_p},\\mathbf{y}\u003e}{\u003c\\mathbf{z_p},\\mathbf{z_p}\u003e}$, we also obtain an alternate formula for the variance estimates:\n$$ \\begin{align}Var(\\hat{\\beta}_p)=\\frac{\\sigma^2}{\u003c\\mathbf{z_p},\\mathbf{z_p}\u003e}=\\frac{\\sigma^2}{||\\mathbf{z_p}||^2} \\end{align} $$\nThe precision with which we can estimate $\\hat{\\beta_p}$ depends on the length of the residual vector $\\mathbf{z_p}$; this represents how much of $\\mathbf{x_p}$ is unexplained by the other $\\mathbf{x_k}$’s\nQR decomposition We can represent step 2 of Algorithm 3.1 in matrix form:\n$$ \\begin{align} \\mathbf{X}=\\mathbf{Z}\\mathbf{Γ} \\end{align} $$\n$\\mathbf{Z}$ has as columns the $\\mathbf{z_j}$ (in order) $\\mathbf{Γ}$ is the upper triangular matrix with entries $\\hat{\\lambda}_{kj}$ Introducing the diagonal matrix D with jth diagonal entry $D_{jj} = ||\\mathbf{z_j}||$, we get\nQR decomposition of X:\n$$ \\begin{align} \\mathbf{X}=\\mathbf{Z}\\mathbf{D}^{-1}\\mathbf{D}\\mathbf{Γ}=\\mathbf{Q}\\mathbf{R} \\end{align} $$\n$\\mathbf{Q}$ is an $N ×(p+1)$ orthogonal matrix, $Q^TQ=I$; $\\mathbf{R}$ is a $(p +1) × (p + 1) $Vupper triangular matrix. Least squares solution: $$ \\begin{align} \\hat{\\beta}\u0026=R^{-1}Q^T\\mathbf{y} \\ \\mathbf{\\hat{y}}\u0026=QQ^T\\mathbf{y} \\end{align} $$\n3.2.4 Multiple Outputs Suppose we have multiple outputs Y1,Y2,…,YK that we wish to predict from our inputs X0,X1,X2,…,Xp. We assume a linear model for each output:\n$$ \\begin{align} Y_k\u0026=\\beta_{0k}+\\sum_{j=1}^pX_j\\beta_{jk}+\\epsilon_k \\ \u0026=f_k(X)+\\epsilon_k \\end{align} $$\nWith N training cases we can write the model in matrix notation:\n$$ \\begin{align} Y=XB+E \\end{align} $$\nY: N×K response matrix\nX: N×(p+1) input matrix\nB: (p+1)× K matrix of parameters\nE: N×K matrix of errors\nA straightforward generalization of the univariate loss function:\n$$ \\begin{align} RSS(B)\u0026=\\sum_{k=1}^K\\sum_{i=1}^N(y_{ik}-f_k(x_i))^2 \\ \u0026=tr[(Y-XB)^T(Y-XB)] \\end{align} $$ The least squares estimates have exactly the same form as before: $$ \\begin{align} \\hat{B}=(X^TX)^{-1}X^Ty \\end{align} $$ If the errors $\\epsilon =(\\epsilon_1,…,\\epsilon_K)$ in are correlated, suppose $Cov(\\epsilon)= Σ$, then the multivariate weighted criterion: $$ \\begin{align} RSS(B;Σ)\u0026=\\sum_{i=1}^N(y_{ik}-f_k(x_i))^TΣ^{-1}(y_{ik}-f_k(x_i)) \\end{align} $$\nComparison of Linear Regression with K-Nearest Neighbors Parametric v.s. Non-parametric Linear regression is an example of a parametric approach because it assumes a linear functional form for $f(X)$.\nParametric methods\nAdvantages: Easy to fit, because one need estimate only a small number of coefficients. Simple interpretations, and tests of statistical significance can be easily performed Disadvantage: Strong assumptions about the form of $f(X)$. If the specified functional form is far from the truth, and prediction accuracy is our goal, then the parametric method will perform poorly. Non-parametric methods\nDo not explicitly assume a parametric form for $f(X)$, and thereby provide an alternative and more flexible approach for performing regression. K-nearest neighbors regression (KNN regression) KNN Regression Given a value for $K$ and a prediction point $x_0$, KNN regression first identifies the $K$ training observations that are closest to $x_0$, represented by $N_0$. It then estimates $f(x_0)$ using the average of all the training responses in $N_0$. $$ \\begin{align} \\hat{f}(x_0)=\\frac{1}{K}\\sum_{x_i\\in N_0}y_i \\end{align} $$\nThe optimal value for $K$ will depend on the bias-variance trade-off. A small value for $K$ provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent on just one observation. A larger K provide a smoother and less variable fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in $f(X)$ The parametric approach will outperform the nonparametric approach if the selected parametric form is close to the true form of $f(x)$.\nA non-parametric approach incurs a cost in variance that is not offset by a reduction in bias KNN performs slightly worse than linear regression when the relationship is linear, but much better than linear regression for non-linear situations. The increase in dimension has only caused a small deterioration in the linear regression test set MSE, but it has caused more than a ten-fold increase in the MSE for KNN.\nThis decrease in performance as the dimension increases is a common problem for KNN, and results from the fact that in higher dimensions there is effectively a reduction in sample size$\\Rightarrow$ curse of dimensionality As a general rule, parametric methods will tend to outperform non-parametric approaches when there is a small number of observations per predictor. Ref:\nJames, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.\nHastie, Trevor, et al. “The elements of statistical learning: data mining, inference and prediction.” The Mathematical Intelligencer 27.2 (2005): 83-85\nRice, John A. Mathematical statistics and data analysis. Cengage Learning, 2006.\n","wordCount":"4699","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nancyyanyu.github.io/posts/ml-linear_regression_models/"},"publisher":{"@type":"Organization","name":"Nancy's Notebook","logo":{"@type":"ImageObject","url":"https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nancyyanyu.github.io/ accesskey=h title="Nancy's Notebook (Alt + H)"><img src=https://nancyyanyu.github.io/apple-touch-icon.png alt aria-label=logo height=35>Nancy's Notebook</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nancyyanyu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://nancyyanyu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://nancyyanyu.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nancyyanyu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nancyyanyu.github.io/posts/>Posts</a></div><h1 class=post-title>Study Note: Linear Regression Part I - Linear Regression Models</h1><div class=post-meta>23 min&nbsp;·&nbsp;4699 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/ML-Linear_Regression_Models/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h1 id=simple-linear-regression-models>Simple Linear Regression Models<a hidden class=anchor aria-hidden=true href=#simple-linear-regression-models>#</a></h1><h2 id=linear-regression-model>Linear Regression Model<a hidden class=anchor aria-hidden=true href=#linear-regression-model>#</a></h2><ul><li><p>Form of the linear regression model: <em>$y=\beta_{0}+\beta_{1}X+\epsilon$</em>.</p></li><li><p>Training data: ($x_1$,$y_1$) &mldr; ($x_N$,$y_N$). Each $x_{i} =(x_{i1},x_{i2},&mldr;,x_{ip})^{T}$ is a vector of feature measurements for the $i$-th case.</p></li><li><p>Goal: estimate the parameters $β$</p></li><li><p>Estimation method: <strong>Least Squares</strong>, we pick the coeﬃcients $β =(β_0,β_1,&mldr;,β_p)^{T}$ to minimize the <strong>residual sum of squares</strong></p></li></ul><p><strong>Assumptions:</strong></p><ul><li>Observations $y_i$ are uncorrelated and have constant variance $\sigma^2$;</li><li>$x_i$ are ﬁxed (non random)</li><li>The regression function $E(Y |X)$ is linear, or the linear model is a reasonable approximation.</li></ul><h2 id=residual-sum-of-squares>Residual Sum of Squares<a hidden class=anchor aria-hidden=true href=#residual-sum-of-squares>#</a></h2><p><strong>Residual</strong>: $e_i = y_i−\hat{y_i}$ represents the ith residual—this is the difference between the i-th observed response value and the i-th response value that is predicted by our linear model.
$$
\begin{align}
\text{Residual sum of squares(RSS) } \quad R(\beta)&=e_1^2+e_2^2+e_3^2+&mldr;e_n^2 \
&=\sum_{i=1}^{N}(y_{i}-f(x_{i}))^2 \
&=\sum_{i=1}^{N}(y_{i}-\beta_{0}-\sum_{j=1}^{p}X_{ij}\beta_{j})^2
\end{align}
$$</p><h3 id=solution>Solution<a hidden class=anchor aria-hidden=true href=#solution>#</a></h3><p>Denote by $X$ the $N × (p + 1) $ matrix with each row an input vector ( $1$ in the ﬁrst position), and similarly let $y$ be the $N$-vector of outputs in the training set.</p><p>$$
\begin{align} \min RSS(\beta)= (y-\mathbf{X}\beta)^T(y-\mathbf{X}\beta) \end{align}
$$
A quadratic function in the $p + 1$ parameters</p><p>Taking derivatives:</p><p>$$
\begin{align} \frac{\partial RSS}{\partial \beta}=-2\mathbf{X}^T(y-\mathbf{X}\beta) \end{align}
$$</p><p>$$
\begin{align} \frac{\partial^2 RSS}{\partial \beta \partial \beta^T}=2\mathbf{X}^T\mathbf{X} \end{align}
$$</p><p>Assuming (for the moment) that $\mathbf{X}$ has <strong>full column rank</strong> (i.e. each of the columns of the matrix are <em>linearly independent</em>), hence $\mathbf{X}^T\mathbf{X}$ is <strong>positive deﬁnite</strong> (every eigenvalue is positive), we set the ﬁrst derivative to zero: $\mathbf{X}^T(y-\mathbf{X}\beta)=0$</p><p><em>Least squares coefficient estimates for simple linear regression</em>:
$$
\begin{align}
\Rightarrow \hat{\beta_1}&=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty \
&=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2} \
\hat{\beta_0}&=\bar{y}-\hat{\beta_1}\bar{x}
\end{align}
$$
where $\bar{y}=\sum_{i=1}^ny_i/n$, $\bar{x}=\sum_{i=1}^nx_i/n$ are the <strong>sample means</strong>.</p><p>Fitted values at the training inputs: $\hat{y}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty$</p><p>Hat matrix: $H=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$</p><h2 id=assessing-the-accuracy-of-the-coefficient-estimates>Assessing the Accuracy of the Coefficient Estimates<a hidden class=anchor aria-hidden=true href=#assessing-the-accuracy-of-the-coefficient-estimates>#</a></h2><p>Assume that the true relationship between $X$ and $Y$ takes the form $Y = f(X) + \epsilon$ for some unknown function $f$, where $\epsilon$ is a mean-zero random error term.</p><p><strong>Least squares line</strong>:
$$
\begin{align}
\hat{y_i} = \hat{β_0} + \hat{β_1}x_i
\end{align}
$$
<strong>Population regression line</strong>:
$$
\begin{align}
Y=\beta_0+\beta_1X+\epsilon
\end{align}
$$
The <strong>error term</strong> is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in $Y$ , and there may be measurement error. We typically assume that the error term is <strong>independent</strong> of $X$.</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_regression_models/11_v2_hu1cd355710aa9f54413ff161c65ba3f63_26692_600x0_resize_box_3.png width=600 height=87><figcaption><small></small></figcaption></figure><h3 id=population-vs-sample>Population V.S. Sample<a hidden class=anchor aria-hidden=true href=#population-vs-sample>#</a></h3><p>The true relationship is generally not known for real data, but the least squares line can always be computed using the coefficient estimates.</p><p><strong>Why there are two different lines describe the relationship between the predictor and the response?</strong></p><ul><li>The concept of these two lines is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of a large population.</li><li>The <strong>sample mean</strong> $\bar{x}=\sum_{i=1}^nx_i/n$ and the <strong>population mean</strong> $\mu$ are different, but in general the sample
mean $\bar{x}$ will provide a good estimate of the population mean $\mu$.</li></ul><p><strong>Unbiased</strong></p><ul><li>If we use the sample mean $\bar{x}$ to estimate μ, this estimate is <strong>unbiased</strong>, in the sense that on average, we expect $\bar{x}$ to equal $μ$.</li><li>An unbiased estimator does not systematically over- or under-estimate the true parameter.</li></ul><h3 id=standard-error>Standard Error<a hidden class=anchor aria-hidden=true href=#standard-error>#</a></h3><p><strong>How accurate is the sample mean $\hat{\mu}$ as an estimate of μ?</strong></p><ul><li><strong>Standard error of $\hat{\mu}$</strong>: standard deviation of the means; average amount that this estimate $\hat{\mu}$ differs from the actual value of $μ$.
$$
\begin{align}
Var(\hat{\mu})=SE(\hat{\mu})^2=\frac{s^2}{n} \
s=\sqrt{\frac{\sum_{i}\epsilon^2}{n-1}}
\end{align}
$$
where $σ$ is the standard deviation of each of the realizations $y_i$ of $Y$ provided that the $n$ observations are <strong>uncorrelated</strong>.</li></ul><p><strong>Standard Deviation V.S. Standard Error</strong></p><ul><li>The <strong>standard deviation (SD)</strong> measures the amount of variability, or dispersion, for a subject set of data from the mean</li><li>The <strong>standard error</strong> of the mean (SEM) measures how far the sample mean of the data is likely to be from the <strong>true population mean</strong>.</li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_regression_models/12_v2_hu495efb0ecfb424d778c1eb41d1a9965f_44336_300x0_resize_box_3.png width=300 height=39><figcaption><small></small></figcaption></figure><h3 id=statistical-properties-of-hatbeta_0-and-hatbeta_1>Statistical Properties of $\hat{\beta}_0$ and $\hat{\beta}_1$<a hidden class=anchor aria-hidden=true href=#statistical-properties-of-hatbeta_0-and-hatbeta_1>#</a></h3><p><strong>How close $\hat{\beta}_0$ and $\hat{\beta}_1$ are to the true values $\beta_0$ and $\beta_1$?</strong>
$$
\begin{align}
Var(\hat{\beta_0})^2&=\sigma^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2} \right] \
Var(\hat{\beta_1})^2&=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2}
\end{align}
$$</p><p>​ where $\sigma^2 = Var(\epsilon)$</p><ul><li><p>Proof:
$$
\begin{align}
\hat{\beta_1}&=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2} = \frac{\sum_{i=1}^n(x_i-\bar{x})y_i}{\sum_{i=1}^n(x_i-\bar{x})^2} \
Var(\hat{\beta_1})&=Var(\frac{\sum_{i=1}^n(x_i-\bar{x})y_i}{\sum_{i=1}^n(x_i-\bar{x})^2} ) \
&=Var(\frac{\sum_{i=1}^n(x_i-\bar{x})(\beta_0+\beta_1x_i+\epsilon_i)}{\sum_{i=1}^n(x_i-\bar{x})^2}) \
&=Var(\frac{\sum_{i=1}^n(x_i-\bar{x})\epsilon_i}{\sum_{i=1}^n(x_i-\bar{x})^2}) \quad\text{ note only $\epsilon$ is a r.v.} \
&=\frac{\sum_{i=1}^n(x_i-\bar{x})^2Var(\epsilon_i)}{(\sum_{i=1}^n(x_i-\bar{x})^2)^2} \quad \text{independence of $\epsilon_i$ and, $Var(𝑘𝑋)=𝑘^2Var(𝑋)$} \
&=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2}
\end{align}
$$</p></li><li><p>For these formulas to be strictly valid, we need to assume that the errors $\epsilon_i$ for each observation are uncorrelated with common variance $σ^2$.</p></li></ul><h3 id=estimate-sigma2><strong>Estimate $\sigma^2$</strong><a hidden class=anchor aria-hidden=true href=#estimate-sigma2>#</a></h3><p><strong>Residual standard error(RSE)</strong>: $\sigma$ is not known, but can be estimated from the data. This estimate
is known as the <strong>residual standard error</strong> (an unbiased estimate of $σ$)
$$
\begin{align}
RSE=\hat{\sigma}=\sqrt{RSS/(n-2)}=\sqrt{\frac{1}{N-2}\sum^{N}_{i=1}(y_i-\hat{y_i})^2 }
\end{align}
$$</p><ul><li>The divisor <em>n</em> − 2 is used rather than <em>n</em> because two parameters have been estimated from the data, giving <em>n</em> − 2 degrees of freedom.</li></ul><h3 id=sampling-properties-of-beta>Sampling Properties of $\beta$<a hidden class=anchor aria-hidden=true href=#sampling-properties-of-beta>#</a></h3><p>The variance–covariance matrix of the least squares parameter estimates:
$$
\begin{align} Var(\hat{\beta})=(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2 \end{align}
$$
<strong>Unbiased estimate of $\sigma^2$:</strong>
$$
\begin{align} \hat{\sigma}^2=\frac{1}{N-p-1}\sum^{N}_{i=1}(y_i-\hat{y_i})^2 \end{align}
$$
If the errors, $\epsilon_i$ , are independent normal random variables, then the estimated slope and intercept, being linear combinations of independent normally distributed random variables, are normally distributed as well.</p><p>More generally, if $\epsilon_i$ are independent and the $x_i$ satisfy certain assumptions, a version of the <strong>central limit theorem</strong> implies that, for large <em>n</em>, the estimated slope and intercept are approximately normally distributed.</p><p>Thus, $\beta$ follows <strong>multivariate normal distribution</strong> with mean vector and variance–covariance matrix:
$$
\begin{align}\hat{\beta} \sim N(\beta,(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2 ) \end{align}
$$
Also, a chi-squared distribution with $N −p−1$ degrees of freedom:
$$
\begin{align} (N-p-1)\hat{\sigma}^2 \sim \sigma^2 \chi_{N-p-1}^{2} \end{align}
$$
($\hat{\beta}$ and $\hat{\sigma^2}$ are indep.)</p><p>We use these distributional properties to form tests of hypothesis and conﬁdence intervals for the parameters $\beta_j$:</p><p><strong>Confidence Intervals</strong></p><ul><li><p><strong>A 95% confidence confidence interval</strong>: is defined as a range of values such that with 95% interval probability, the range will contain the true unknown value of the parameter.</p></li><li><p>For linear regression, the 95% confidence interval for $β_1$ approximately takes the form
$$
\begin{align}
&\hat{\beta_1} \pm 1.96 \cdot SE(\hat{\beta_1}) \
&amp;SE(\hat{\beta_1}) =\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2}
\end{align}
$$
(which relies on the assumption that the errors are Gaussian. Also, the factor of 2 in front of the $SE(\hat{\beta_1})$ term will vary slightly depending on the number of observations n in the linear regression. To be precise, rather than the number 2, it should contain the 97.5% quantile of a t-distribution with n−2 degrees of freedom.)</p></li></ul><h3 id=hypothesis-tests>Hypothesis Tests<a hidden class=anchor aria-hidden=true href=#hypothesis-tests>#</a></h3><p>The most common hypothesis test involves testing the <strong>null test hypothesis</strong> of</p><pre tabindex=0><code>H_0: No relationship between X and Y or β1=0
</code></pre><p>versus the <strong>alternative hypothesis</strong></p><pre tabindex=0><code>H_a : There is some relationship between X and Y or β1≠0
</code></pre><p>To test the null hypothesis, we need to determine whether $\hat{\beta_1}$, our estimate for $\beta_1$, is sufficiently far from zero that we can be confident that $\beta_1$ is non-zero $\Rightarrow$ it depends on $SE( \hat{\beta_1}$)</p><ul><li>If $SE( \hat{\beta_1}$) is small, then even relatively small values of $\hat{\beta_1}$ may provide strong evidence that $\beta_1 \neq 0$, and hence that there is a relationship between X and Y</li></ul><h4 id=t-statistic><strong>t-statistic</strong><a hidden class=anchor aria-hidden=true href=#t-statistic>#</a></h4><p>To test the hypothesis that a particular coeﬃcient $\beta_j= 0$, we form the standardized coeﬃcient or Z-score
$$
\begin{align}
t=\frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})} \ or \quad
z_j=\frac{\hat{\beta_j}-0}{\hat{\sigma}\sqrt{\upsilon_j}}
\end{align}
$$
where $\upsilon_j$ is the j-th diagonal element of $(\mathbf{X}^T\mathbf{X})^{-1}$</p><p>which measures the number of standard deviations that $\hat{\beta_1}$ is away from 0.If there really is no relationship between $X$ and $Y$ , then we expect it will have a $t$-distribution with $n−2$ degrees of freedom.</p><p>Under the null hypothesis that $\beta_j= 0$, $z_j$ is distributed as $t_{N-p-1}$, and hence <strong>a large (absolute) value</strong> of $z_j$ will lead to <em>rejection of this null hypothesis</em>. If $\hat{\sigma}$ is replaced by a known value $σ$, then $z_j$ would have a standard normal distribution.</p><h4 id=p-value><strong>p-value</strong><a hidden class=anchor aria-hidden=true href=#p-value>#</a></h4><ul><li>The probability of observing any value $≥ t$ or $≤ -t$, assuming $β_1 = 0$.</li></ul><p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_regression_models/13_v2_hu7da117e0454beb7269cf549198e709ca_120981_300x0_resize_box_3.png width=300 height=83><figcaption><small></small></figcaption></figure>(Here $|t|=2.17, p-value=0.015$. The area in red is 0.015 + 0.015 = 0.030, 3%. If we had chosen a significance level of 5%, this would mean that we had achieved <em>statistical significance</em>. We would <em>reject the null hypothesis</em> in favor of the alternative hypothesis.)</p><ul><li><strong>Interpretation</strong>:a small p-value indicates<ul><li>It is unlikely to observe such a substantial association between the predictor and the response due to LUCK, in the absence of any real association between the predictor and the response.</li><li>We reject the null hypothesis—that is, we declare a relationship to exist between X and Y</li></ul></li></ul><h4 id=f-statistic><strong>F-statistic</strong>:<a hidden class=anchor aria-hidden=true href=#f-statistic>#</a></h4><p>To test if a <strong>categorical variable</strong> with $k$ levels can be <em>excluded</em> from a model, we need to test whether the coeﬃcients of the <strong>dummy variables</strong> used to represent the levels can all be set to zero. Here we use the $F$ statistic：</p><p>$$
\begin{align} F=\frac{(RSS_0-RSS_1)/(p_1-p_0)}{RSS_1/(N-p_1-1)} \end{align}
$$</p><ul><li>$RSS_1$ is the residual sum-of-squares for the least squares ﬁt of the <em>bigger model</em> with $p_1+1$ parameters;</li><li>$RSS_0$ the same for the nested smaller model with $p_0 +1$ parameters, having $p_1 −p_0$ parameters constrained to be zero.</li></ul><p>The $F$ statistic measures the change in residual sum-of-squares <em>per additional parameter</em> in the bigger model, and it is normalized by an estimate of $\sigma^2$.</p><p>Under the Gaussian assumptions, and the null hypothesis that the <strong>smaller model is correct</strong>, the $F$ statistic will have a $F_{p_1-p_0,N-p_1-1}$ distribution.</p><h2 id=the-gaussmarkov-theorem>The Gauss–Markov Theorem<a hidden class=anchor aria-hidden=true href=#the-gaussmarkov-theorem>#</a></h2><p>We focus on estimation of any linear combination of the parameters $\theta=\alpha^T\beta$, for example, predictions $f(x_0)=x^T_0\beta$ are of this form.The least squares estimate of $\alpha^T\beta$ is:
$$
\begin{equation}
\hat{\theta}=\alpha^T\hat{\beta}=\alpha^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
\end{equation}
$$
Considering $\mathbf{X}$ to be ﬁxed, this is a linear function $\mathbf{c}^T_0\mathbf{y}$ of the response vector $\mathbf{y}$.If we assume that the linear model is correct, $\alpha^T\hat{\beta}$ is unbiased.</p><p>The Gauss–Markov theorem states that if we have any other linear estimator $\tilde{\theta}=\mathbf{c}^T\mathbf{y}$ that is unbiased for $\alpha^T\beta$, that is, $E(\mathbf{c}^T\mathbf{y})= \alpha^T\beta$, then:
$$
\begin{equation}
Var(\alpha^T\hat{\beta})\leq Var(\mathbf{c}^T\mathbf{y})
\end{equation}
$$</p><h2 id=assessing-the-accuracy-of-the-model>Assessing the Accuracy of the Model<a hidden class=anchor aria-hidden=true href=#assessing-the-accuracy-of-the-model>#</a></h2><p>The quality of a linear regression fit is typically assessed using two related quantities: <strong>the residual standard error (RSE)</strong> and the <strong>$R^2$</strong> statistic.</p><h3 id=residual-standard-error>Residual Standard Error<a hidden class=anchor aria-hidden=true href=#residual-standard-error>#</a></h3><p>The $RSE$ is an estimate of the standard deviation of $\epsilon$: the average amount that the response
will deviate from the true regression line
$$
\begin{align}
RSS&=\sum_{i=1}^n(y_i-\hat{y})^2 \
RSE&=\sqrt{\frac{1}{n-2}RSS}=\sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i-\hat{y})^2}
\end{align}
$$<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_regression_models/14_v2_hu261f34fe8d826f41e0f94c3359d32e14_109693_600x0_resize_box_3.png width=600 height=186><figcaption><small></small></figcaption></figure></p><p>In the case of the advertising data, we see from the linear regression output in Table 3.2 that the RSE is 3.26. In other words, actual sales in each market deviate from the true regression line by approximately 3,260 units, on average.</p><p>The mean value of sales over all markets is approximately 14,000 units, and so the percentage error is
3,260/14,000 = 23%.</p><p>The RSE is considered a measure of the <strong>lack of fit</strong> of the model $Y = β_0 + β_1X + \epsilon$ to the data.</p><h3 id=r2-statistic>$R^2$ Statistic<a hidden class=anchor aria-hidden=true href=#r2-statistic>#</a></h3><p>The $RSE$ is measured in the units of $Y$ , it is not always clear what constitutes a good $RSE$.</p><p>The $R^2$ statistic takes the form of a <strong>proportion</strong>—the proportion of variance explained—and so it always takes on a value <strong>between 0 and 1</strong>, and is independent of the scale of $Y$ .
$$
\begin{align}
R^2 = (TSS − RSS)/TSS= 1− RSS/TSS = 1-\frac{\sum(y_i-\hat{y})^2}{\sum(y_i-\bar{y})^2}
\end{align}
$$
<strong>TSS(total sum of squares)</strong>: $\sum(y_i-\bar{y})^2$ - the amount of variability inherent in the response before the
regression is performed</p><p><strong>RSS</strong>: $\sum_{i=1}^n(y_i-\hat{y})^2$ - the amount of variability that is left unexplained after performing the regression</p><p><strong>(TSS−RSS)</strong>: measures the amount of variability in the response that is explained (or removed) by performing the regression, and <strong>$R^2$ measures the proportion of variability in $Y$ that can be explained using $X$.</strong></p><p><strong>Interpretation</strong>：</p><ul><li>close to 1 : a large proportion of the variability in the response has been explained by the regression.</li><li>close to 0 : the regression did not explain much of the variability in the response<ul><li>The linear model is wrong</li><li>The inherent error $σ^2$ is high, or both.</li></ul></li></ul><h3 id=squared-correlation-vs-r2-statistic>Squared Correlation V.S. R2 Statistic<a hidden class=anchor aria-hidden=true href=#squared-correlation-vs-r2-statistic>#</a></h3><p><strong>Correlation</strong>:
$$
\begin{align}
Corr(X,Y)=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}=\hat{\beta}<em>1\frac{\sqrt{\sum</em>{i=1}^n(x_i-\bar{x})^2}}{\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}
\end{align}
$$</p><ul><li><p>the correlation is zero if and only if the slope is zero.</p></li><li><p>also a measure of the linear relationship between X and Y.</p></li></ul><blockquote><p>In the simple linear regression setting, $R^2 = [Cor]^2$. In other words, the squared correlation and the R2 statistic are identical</p></blockquote><h2 id=p-value-vs-r2>p-Value v.s. $R^2$<a hidden class=anchor aria-hidden=true href=#p-value-vs-r2>#</a></h2><p>Referring answer fromn <a href=https://www.researchgate.net/profile/Faye_Anderson3>Faye Anderson</a>:</p><p>There is no established association/relationship between p-value and R-square. This all depends on the data (i.e.; contextual).</p><p>R-square value tells you <strong>how much variation is explained by your model</strong>. So $R^2=0.1$ means that your model explains 10% of variation within the data. The greater R-square the better the model.</p><p>Whereas p-value tells you about the F-statistic hypothesis testing of the &ldquo;fit of the intercept-only model and your model are equal&rdquo;. So if the p-value is less than the significance level (usually 0.05) then your model fits the data well.</p><p>Thus you have four scenarios:</p><p><strong>1) low R-square and low p-value (p-value &lt;= 0.05) :</strong> means that your model doesn&rsquo;t explain much of variation of the data but it is significant (better than not having a model)</p><p><strong>2) low R-square and high p-value (p-value > 0.05) :</strong> means that your model doesn&rsquo;t explain much of variation of the data and it is not significant (worst scenario)</p><p><strong>3) high R-square and low p-value :</strong> means your model explains a lot of variation within the data and is significant (best scenario)</p><p><strong>4) high R-square and high p-value :</strong> means that your model explains a lot of variation within the data but is not significant (model is worthless)</p><h2 id=variance--bias>Variance & Bias<a hidden class=anchor aria-hidden=true href=#variance--bias>#</a></h2><p>Consider the <strong>mean squared error</strong> of an estimator $\tilde{\theta}$ in estimating $\theta$:</p><p>$$
\begin{align}
MSE(\tilde{\theta})&= E(\tilde{\theta}-\theta)^2 \
&= E(\tilde{\theta^2}+\theta^2-2\theta\tilde{\theta}) \
&= E(\tilde{\theta^2})-E^2(\tilde{\theta})+E^2(\tilde{\theta})+E(\theta^2-2\theta\tilde{\theta})\
&= Var(\tilde{\theta})+[E(\tilde{\theta})-\theta]^2
\end{align}
$$
The ﬁrst term is the <strong>variance</strong>, while the second term is the <strong>squared bias</strong>.</p><p>The <strong>Gauss-Markov theorem</strong> implies that the <em>least squares estimator</em> has the smallest <em>mean squared error</em> of all linear estimators with <em>no bias</em>. However, there may well exist a biased estimator with smaller mean squared error. Such an estimator would trade a little bias for a larger reduction in variance.</p><p>From a more pragmatic point of view, most models are distortions of the truth, and hence are biased; picking the right model amounts to creating the right balance between bias and variance.</p><p><strong>Mean squared error</strong> is intimately related to <em>prediction accuracy</em>. Consider the prediction of the new response at input $x_0$,
$$
\begin{equation}
y_0=f(x_0)+\epsilon_0
\end{equation}
$$</p><h3 id=prediction-error--mse>Prediction error & MSE<a hidden class=anchor aria-hidden=true href=#prediction-error--mse>#</a></h3><p>The <strong>expected prediction error</strong> of an estimate $\tilde{f}(x_0)=x_0^T\tilde{\beta}$:</p><p>$$
\begin{align}
E(y_0-\tilde{f}(x_0))^2 &=E(f(x_0)+\epsilon_0-x_0^T\tilde{\beta})^2 \
&=E(\epsilon_0^2)+E(f(x_0)-x_0^T\tilde{\beta})^2-2E(\epsilon_0(f(x_0)-x_0^T\tilde{\beta})) \
&=\sigma^2+E(f(x_0)-x_0^T\tilde{\beta})^2 \
&=\sigma^2+MSE(\tilde{f}(x_0))
\end{align}
$$
Therefore, expected prediction error and mean squared error diﬀer only by the constant $\sigma^2$, representing the <em>variance of the new observation $y_0$.</em></p><h1 id=multiple-linear-regression>Multiple Linear Regression<a hidden class=anchor aria-hidden=true href=#multiple-linear-regression>#</a></h1><p>Multiple linear regression model takes the form:
$$
\begin{align}
Y=\beta_0+\beta_1X_1+,,,+\beta_pX_p+\epsilon
\end{align}
$$</p><h2 id=estimating-the-regression-coefficients>Estimating the Regression Coefficients<a hidden class=anchor aria-hidden=true href=#estimating-the-regression-coefficients>#</a></h2><p>We choose $β_0, β_1, . . . , β_p$ to minimize the sum of squared residuals
$$
\begin{align}
RSS&=\sum_{i=1}^n(y_i-\hat{y}<em>i)^2 \
&=\sum</em>{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta_1}x_{i1}-,,,-\hat{\beta_p}x_{ip})^2
\end{align}
$$<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_regression_models/15_v2_huac920a950ca9c94a669725d4e02691a9_66917_600x0_resize_box_3.png width=600 height=124><figcaption><small></small></figcaption></figure></p><p><strong>Does it make sense for the multiple regression to suggest no relationship between <em>sales</em> and <em>newspaper</em> while the simple linear regression implies the opposite?</strong></p><ul><li>Notice that the correlation between radio and newspaper is 0.35.</li><li>In markets where we spend more on radio our sales will tend to be higher, and as our correlation matrix shows, we also tend to spend more on newspaper advertising in those same markets.</li><li>Hence, in a simple linear regression which only examines sales versus newspaper, we will observe that higher values of newspaper tend to be associated with higher values of sales, even though newspaper advertising does not actually affect sales.</li></ul><h2 id=some-important-questions>Some Important Questions<a hidden class=anchor aria-hidden=true href=#some-important-questions>#</a></h2><h3 id=1-is-there-a-relationship-between-the-response-and-predictors>1. Is There a Relationship Between the Response and Predictors?<a hidden class=anchor aria-hidden=true href=#1-is-there-a-relationship-between-the-response-and-predictors>#</a></h3><h4 id=hypothesis-test><strong>Hypothesis Test</strong><a hidden class=anchor aria-hidden=true href=#hypothesis-test>#</a></h4><p>We use a hypothesis test to answer this question.</p><p>We test the <strong>null hypothesis</strong></p><pre tabindex=0><code>H_0 : β1 = β2 = · · · = βp = 0
</code></pre><p>versus the <strong>alternative</strong></p><pre tabindex=0><code>H_a : at least one βj is non-zero
</code></pre><p>This hypothesis test is performed by computing the <strong>F-statistic</strong>,
$$
\begin{align}
F=\frac{(TSS-RSS)/p}{RSS/(n-p-1)}
\end{align}
$$
where $TSS =\sum(y_i − \bar{y})^2$ and $RSS =\sum(y_i−\hat{y}_i)^2$.</p><p>If the linear model assumptions are correct, one can show that
$$
\begin{align}
E[RSS/(n-p-1)]=\sigma^2
\end{align}
$$
and that, provided $H_0$ is true,
$$
\begin{align}
E[(TSS-RSS)/p]=\sigma^2
\end{align}
$$</p><ul><li>When there is no relationship between the response and predictors, one would expect the <strong>F-statistic to take on a value close to 1.</strong></li><li>On the other hand, if $H_a$ is true, then $E[(TSS-RSS)/p]>\sigma^2$, so we expect <strong>$F$ to be</strong>
<strong>greater than 1.</strong></li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_regression_models/16_v2_hu040bc07177e688a83502536028bd6ed9_161949_600x0_resize_box_3.png width=600 height=340><figcaption><small></small></figcaption></figure><h3 id=2-how-large-does-the-f-statistic-need-to-be-before-we-can-reject-h_0-and-conclude-that-there-is-a-relationship>2. How large does the F-statistic need to be before we can reject $H_0$ and conclude that there is a relationship?<a hidden class=anchor aria-hidden=true href=#2-how-large-does-the-f-statistic-need-to-be-before-we-can-reject-h_0-and-conclude-that-there-is-a-relationship>#</a></h3><ul><li>When $n$ is large, an <em>F-statistic</em> that is just a little larger than 1 might still provide evidence against $H_0$.</li><li>In contrast, a larger F-statistic is needed to reject $H_0$ if $n$ is small.</li><li>For the advertising data, the <strong>p-value</strong> associated with the F-statistic in Table 3.6 is essentially zero, so we have extremely strong evidence that at least one of the media is associated with increased sales.</li></ul><p><strong>To test that a particular subset of $q$ of the coefficients are zero</strong></p><p>This corresponds to a null hypothesis</p><p>$$
H_0 : β_{p-q+1} = β-{p-q+2} = · · · = β_p = 0
$$
In this case we fit a second model that uses all the variables <strong>except those last $q$</strong>. Suppose that the residual sum of squares for that model is $RSS_0$. Then the appropriate F-statistic is
$$
\begin{align}
F=\frac{(RSS_0-RSS)/q}{RSS/(n-p-1)}
\end{align}
$$</p><h4 id=f-statistics-vs-t-statistics><strong>F-statistics v.s. t-statistics</strong><a hidden class=anchor aria-hidden=true href=#f-statistics-vs-t-statistics>#</a></h4><ul><li><p><strong>Equivalency</strong>: In Table 3.4, for each individual predictor a <em>t-statistic</em> and a <em>p-value</em> were reported. These provide information about <strong>whether each individual predictor is related to the response</strong>, after adjusting for the other predictors. It turns out that each of these are exactly equivalent to the <em>F-test</em> that omits that single variable from the model, leaving all the others
in—i.e. q=1 in the model. So it reports the <strong>partial effect</strong> of adding that variable to the model.</p><blockquote><p>The square of each <em>t-statistic</em> is the corresponding <em>F-statistic</em>.</p></blockquote></li><li><p><strong>$p$ is large</strong>: If any one of the <em>p-values</em> for the individual variables is very small, then <em><strong>at least one of the
predictors is related to the response</strong></em>. However, this logic is flawed, especially when the number of predictors $p$ is large.</p><ul><li>If we use the individual <em>t-statistics</em> and associated <em>p-values</em> to decide whether there is any association between the variables and the response, high chance we will incorrectly conclude there is a relationship.</li><li>However, the <em>F-statistic</em> does not suffer from this problem because <strong>it adjusts for the number of predictors</strong>.</li></ul></li><li><p><strong>$p > n$</strong>: more coefficients $β_j$ to estimate than observations from which to estimate them.</p><ul><li>cannot even fit the multiple linear regression model using least squares,</li></ul></li></ul><h3 id=3-do-all-the-predictors-help-to-explain-y--or-is-only-a-subset-of-the-predictors-useful>3. Do all the predictors help to explain Y , or is only a subset of the predictors useful?<a hidden class=anchor aria-hidden=true href=#3-do-all-the-predictors-help-to-explain-y--or-is-only-a-subset-of-the-predictors-useful>#</a></h3><h4 id=variable-selection><strong>Variable Selection</strong><a hidden class=anchor aria-hidden=true href=#variable-selection>#</a></h4><ul><li>Various statistics can be used to <em>judge the quality of a model:</em><ul><li><strong>Mallow’s Cp, Akaike informa-Mallow’s Cp tion criterion (AIC)</strong></li><li><strong>Bayesian information criterion (BIC)</strong></li><li><strong>adjusted R2</strong></li></ul></li><li>There are three classical approaches to <em>select models:</em><ul><li><strong>Forward selection</strong></li><li><strong>Backward selection</strong></li><li><strong>Mixed selection</strong></li></ul></li></ul><h3 id=4-how-well-does-the-model-fit-the-data>4. How well does the model fit the data?<a hidden class=anchor aria-hidden=true href=#4-how-well-does-the-model-fit-the-data>#</a></h3><p>Two of the most common numerical measures of model fit are the <strong>RSE</strong> and <strong>$R^2$</strong></p><h4 id=r2-statistics>$R^2$ Statistics<a hidden class=anchor aria-hidden=true href=#r2-statistics>#</a></h4><p>An $R^2$ value close to $1$ indicates that the model explains a large portion of the variance in the response variable.
$$
\begin{align}
R^2 = (TSS − RSS)/TSS= 1− RSS/TSS
\end{align}
$$
Recall that in simple regression, $R^2$ is the <em>square of the correlation</em> of the response and the variable. In multiple linear regression, it turns out that it equals $Cor(Y, \hat{Y} )^2$, the square of the correlation between the response and the fitted linear model; in fact one property of the fitted linear model is that it maximizes this correlation among all possible linear models.</p><p><strong>$R^2$ will always increase when more variables are added to the model, even if those variables are only weakly associated with the response.</strong></p><ul><li>This is due to the fact that adding another variable to the least squares equations must allow us to fit the training data (though not necessarily the testing data) more accurately.</li><li>The fact that adding <em>newspaper</em> advertising to the model containing only TV and radio
advertising leads to just a tiny increase in $R_2$ provides additional evidence that newspaper can be dropped from the model.</li></ul><h4 id=rse>RSE<a hidden class=anchor aria-hidden=true href=#rse>#</a></h4><p><strong>RSE</strong> is defined as
$$
\begin{align}
RSE=\sqrt{\frac{RSS}{n-p-1}}
\end{align}
$$
Models with more variables can have higher RSE if the decrease in RSS is small relative to the increase in $p$.</p><h4 id=graphical-summaries>Graphical summaries<a hidden class=anchor aria-hidden=true href=#graphical-summaries>#</a></h4><p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-linear_regression_models/17_v2_hu8311a094f77dc7c2c4e3fdc2bfa21bf3_64264_600x0_resize_box_3.png width=600 height=121><figcaption><small></small></figcaption></figure>It suggests a <strong>synergy</strong> or <strong>interaction</strong> effect between the advertising media, whereby combining the media together results in a bigger boost to sales than using any single medium</p><h3 id=5-given-a-set-of-predictor-values-what-response-value-should-we-predict-and-how-accurate-is-our-prediction>5. Given a set of predictor values, what response value should we predict, and how accurate is our prediction?<a hidden class=anchor aria-hidden=true href=#5-given-a-set-of-predictor-values-what-response-value-should-we-predict-and-how-accurate-is-our-prediction>#</a></h3><h4 id=uncertainty-associated-with-prediction><strong>Uncertainty associated with prediction</strong><a hidden class=anchor aria-hidden=true href=#uncertainty-associated-with-prediction>#</a></h4><ol><li>The coefficient estimates $\hat{\beta_0},\hat{\beta_1},&mldr;,\hat{\beta_p}$ are estimates for $β_0, β_1, . . . , β_p$. That is, the <strong>least squares plane</strong>
$$
\begin{align}
\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X_1+,&mldr;+\hat{\beta_p}X_p
\end{align}
$$
is only an estimate for the <strong>true population regression plane</strong>
$$
\begin{align}
f(X)=\beta_0+\beta_1X_1+,&mldr;+\beta_pX_p
\end{align}
$$</li></ol><ul><li>The inaccuracy in the coefficient estimates is related to the <strong>reducible error</strong>.</li><li>We can compute a <strong>confidence interval</strong> in order to determine how close $\hat{Y}$ will be to $f(X)$.</li></ul><ol start=2><li>In practice, assuming a linear model for $f(X)$ is almost always an approximation of reality, so there is an additional source of potentially <strong>reducible error</strong> which we call <strong>model bias</strong>.</li><li>Even if we knew $f(X)$—true values for $β_0, β_1, . . . , β_p$—the response value cannot be predicted perfectly
because of the random error $\epsilon$ &ndash;<strong>irreducible error</strong>.</li></ol><ul><li>How much will Y vary from $\hat{Y}$? &ndash; <strong>prediction intervals</strong></li></ul><h4 id=prediction-intervals><strong>Prediction intervals</strong><a hidden class=anchor aria-hidden=true href=#prediction-intervals>#</a></h4><p><strong>Prediction intervals</strong> are always wider than <strong>confidence intervals</strong></p><ul><li>Because they incorporate both <em>the error in the estimate for f(X) (the reducible error) and the uncertainty as to how much an individual point will differ from the population regression plane (the irreducible error).</em></li></ul><p>E.g.</p><ul><li><strong>confidence interval</strong> : quantify the uncertainty surrounding the average sales over a large number of cities.</li><li><strong>prediction interval</strong> : quantify the uncertainty surrounding sales for a particular city.</li></ul><h2 id=multiple-regression-from-simple-univariate-regression>Multiple Regression from Simple Univariate Regression<a hidden class=anchor aria-hidden=true href=#multiple-regression-from-simple-univariate-regression>#</a></h2><p>Gram-Schmidt正交化</p><h3 id=simple-univariate-regression>Simple Univariate Regression<a hidden class=anchor aria-hidden=true href=#simple-univariate-regression>#</a></h3><p>Suppose ﬁrst that we have a univariate model with no intercept:
$$
\begin{align}
\mathbf{Y}=\mathbf{X}\beta+\epsilon
\end{align}
$$
The least squares estimate and residuals are:
$$
\begin{align}
\hat{\beta}&=\frac{\sum_1^Nx_iy_i}{\sum_1^Nx_i^2} \
r_i&=y_i-x_i\hat{\beta}
\end{align}
$$
The least squares estimate and residuals are:
$$
\begin{align}
\hat{\beta}&=\frac{\sum_1^Nx_iy_i}{\sum_1^Nx_i^2} \
r_i&=y_i-x_i\hat{\beta}
\end{align}
$$
Let $\mathbf{y}=(y_1,&mldr;,y_N)^T$,$\mathbf{x}=(x_1,&mldr;,x_N)^T$</p><p>Define the <strong>inner product</strong> between $\mathbf{x}$ and $\mathbf{y}$:
$$
\begin{align}
&lt;\mathbf{x},\mathbf{y}>=\sum_1^Nx_iy_i=\mathbf{x}^T\mathbf{y}
\end{align}
$$
Then,
$$
\begin{align}
\hat{\beta}&=\frac{&lt;\mathbf{x},\mathbf{y}>}{&lt;\mathbf{x},\mathbf{x}>} \
\mathbf{r}&=\mathbf{y}-\mathbf{x}\hat{\beta}
\end{align}
$$
Suppose the inputs $x_1, x_2,&mldr;, x_p$ (the columns of the data matrix $\mathbf{X}$) are orthogonal; that is $&lt;\mathbf{x_k},\mathbf{x_j}>=0$. Then the multiple least squares estimates $\hat{\beta_j}$ are equal to $\frac{&lt;\mathbf{x_j},\mathbf{y}>}{&lt;\mathbf{x_j},\mathbf{x_j}>}$—the univariate estimates. In other words, when the inputs are orthogonal, they have no eﬀect on each other’s
parameter estimates in the model.</p><h3 id=orthogonalization>Orthogonalization<a hidden class=anchor aria-hidden=true href=#orthogonalization>#</a></h3><p>$$
\begin{align}
\hat{\beta}_1&=\frac{&lt;\mathbf{x}-\bar{x}\mathbf{1},\mathbf{y}>}{&lt;\mathbf{x}-\bar{x}\mathbf{1},\mathbf{x}-\bar{x}\mathbf{1}>} \
\end{align}
$$</p><ul><li>$\bar{x}=\sum_ix_i/N$;</li><li>$\mathbf{1}$, the vector of N ones;</li></ul><p><strong>Steps:</strong></p><ol><li>regress $\mathbf{x}$ on $\mathbf{1}$ to produce the residual $\mathbf{z}=\mathbf{x}-\bar{x}\mathbf{1}$;</li><li>regress $\mathbf{y}$ on the residual $\mathbf{z}$ to give the coeﬃcient $\hat{\beta}_1$</li></ol><p>Regress $\mathbf{a}$ on $\mathbf{b}$ ($\mathbf{b}$ is adjusted for $\mathbf{a}$),(or $\mathbf{b}$ is <strong>“orthogonalized”</strong> with respect to $\mathbf{a}$); a simple univariate regression of $\mathbf{b}$ on a with no intercept, producing coeﬃcient $\hat{\lambda}=\frac{&lt;\mathbf{a},\mathbf{b}>}{&lt;\mathbf{a},\mathbf{a}>}$ and residual vector $ \mathbf{b}-\hat{\lambda} \mathbf{a}$.</p><p>The orthogonalization does not change the subspace spanned by $x_1$ and $x_2$, it simply produces an <strong>orthogonal basis</strong> for representing it.</p><h3 id=gramschmidt-procedure-for-multiple-regression>Gram–Schmidt procedure for multiple regression<a hidden class=anchor aria-hidden=true href=#gramschmidt-procedure-for-multiple-regression>#</a></h3><p><strong>ALGORITHM 3.1 Regression by Successive Orthogonalization</strong></p><ol><li>Initialize $\mathbf{z_0}=\mathbf{x_0}=\mathbf{1}$.</li><li>For j=1,2,&mldr;,1,,&mldr;,p,
Regress $\mathbf{x_j}$ on $\mathbf{z_0},\mathbf{z_1},&mldr;,\mathbf{z_{j-1}}$ to produce coeﬃcients $\hat{\lambda}<em>{l,j}=\frac{&lt;\mathbf{z_l},\mathbf{x_j}>}{&lt;\mathbf{z_l},\mathbf{z_l}>}$, l=0,1,&mldr;,j-1, and residual vector $\mathbf{z_j}=\mathbf{x_j}-\sum</em>{k=0}^{j-1}\hat{\lambda_{kj}}\mathbf{z_k}$</li><li>Regress $\mathbf{y}$ on the residual $\mathbf{z_p}$ to give the estimate $\hat{\beta_p}=\frac{&lt;\mathbf{z_p},\mathbf{y}>}{&lt;\mathbf{z_p},\mathbf{z_p}>}$.</li></ol><p><strong>Note:</strong></p><ul><li>Each of the $\mathbf{x}_j$ is a linear combination of the $\mathbf{z}_k$, $k ≤ j$.</li><li>Since the $\mathbf{z}_j$ are all orthogonal, they form a basis for the column space of $\mathbf{X}$, and hence the least squares projection
onto this subspace is $\mathbf{\hat{y}}$.</li><li>By rearranging the $x_j$ , any one of them could be in the last position, and a similar results holds.</li><li>The multiple regression coeﬃcient $\mathbf{x}_j$ represents the additional contribution of $\mathbf{x}<em>j$ on $\mathbf{y}$, after $\mathbf{x}<em>j$ has been adjusted for $x_0, x_1,&mldr;, x</em>{j−1},x</em>{j+1},&mldr;, x_p$.</li></ul><h3 id=precision-of-coefficient-estimation>Precision of Coefficient Estimation<a hidden class=anchor aria-hidden=true href=#precision-of-coefficient-estimation>#</a></h3><p>If $\mathbf{x}_p$ is highly correlated with some of the other $\mathbf{x}_k$’s, the residual vector $\mathbf{z}_p$ will be close to zero, and the coeﬃcient $\mathbf{x}_j$ will be very unstable.</p><p>From $\hat{\beta_p}=\frac{&lt;\mathbf{z_p},\mathbf{y}>}{&lt;\mathbf{z_p},\mathbf{z_p}>}$, we also obtain an alternate formula for the variance estimates:</p><p>$$
\begin{align}Var(\hat{\beta}_p)=\frac{\sigma^2}{&lt;\mathbf{z_p},\mathbf{z_p}>}=\frac{\sigma^2}{||\mathbf{z_p}||^2} \end{align}
$$</p><p>The precision with which we can estimate $\hat{\beta_p}$ depends on the length of the residual vector $\mathbf{z_p}$; this represents how much of $\mathbf{x_p}$ is unexplained by the other $\mathbf{x_k}$’s</p><h3 id=qr-decomposition>QR decomposition<a hidden class=anchor aria-hidden=true href=#qr-decomposition>#</a></h3><p>We can represent step 2 of Algorithm 3.1 in matrix form:</p><p>$$
\begin{align}
\mathbf{X}=\mathbf{Z}\mathbf{Γ}
\end{align}
$$</p><ul><li>$\mathbf{Z}$ has as columns the $\mathbf{z_j}$ (in order)</li><li>$\mathbf{Γ}$ is the upper triangular matrix with entries $\hat{\lambda}_{kj}$</li></ul><p>Introducing the diagonal matrix D with jth diagonal entry $D_{jj} = ||\mathbf{z_j}||$, we get</p><p><strong>QR decomposition of X</strong>:</p><p>$$
\begin{align}
\mathbf{X}=\mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{Γ}=\mathbf{Q}\mathbf{R}
\end{align}
$$</p><ul><li>$\mathbf{Q}$ is an $N ×(p+1)$ orthogonal matrix, $Q^TQ=I$;</li><li>$\mathbf{R}$ is a $(p +1) × (p + 1) $Vupper triangular matrix.</li></ul><p><strong>Least squares solution:</strong>
$$
\begin{align}
\hat{\beta}&=R^{-1}Q^T\mathbf{y} \
\mathbf{\hat{y}}&=QQ^T\mathbf{y}
\end{align}
$$</p><h2 id=324-multiple-outputs>3.2.4 Multiple Outputs<a hidden class=anchor aria-hidden=true href=#324-multiple-outputs>#</a></h2><p>Suppose we have multiple outputs Y1,Y2,&mldr;,YK that we wish to predict from our inputs X0,X1,X2,&mldr;,Xp. We assume a linear model for each
output:</p><p>$$
\begin{align}
Y_k&=\beta_{0k}+\sum_{j=1}^pX_j\beta_{jk}+\epsilon_k \
&=f_k(X)+\epsilon_k
\end{align}
$$</p><p>With N training cases we can write the model in matrix notation:</p><ul><li><p>$$
\begin{align}
Y=XB+E
\end{align}
$$</p></li><li><p>Y: N×K response matrix</p></li><li><p>X: N×(p+1) input matrix</p></li><li><p>B: (p+1)× K matrix of parameters</p></li><li><p>E: N×K matrix of errors</p></li></ul><p>A straightforward generalization of the univariate loss function:</p><p>$$
\begin{align}
RSS(B)&=\sum_{k=1}^K\sum_{i=1}^N(y_{ik}-f_k(x_i))^2 \
&=tr[(Y-XB)^T(Y-XB)]
\end{align}
$$
The least squares estimates have exactly the same form as before:
$$
\begin{align}
\hat{B}=(X^TX)^{-1}X^Ty
\end{align}
$$
If the errors $\epsilon =(\epsilon_1,&mldr;,\epsilon_K)$ in are correlated, suppose $Cov(\epsilon)= Σ$, then the multivariate weighted criterion:
$$
\begin{align}
RSS(B;Σ)&=\sum_{i=1}^N(y_{ik}-f_k(x_i))^TΣ^{-1}(y_{ik}-f_k(x_i))
\end{align}
$$</p><h1 id=comparison-of-linear-regression-with-k-nearest-neighbors>Comparison of Linear Regression with K-Nearest Neighbors<a hidden class=anchor aria-hidden=true href=#comparison-of-linear-regression-with-k-nearest-neighbors>#</a></h1><h2 id=parametric-vs-non-parametric>Parametric v.s. Non-parametric<a hidden class=anchor aria-hidden=true href=#parametric-vs-non-parametric>#</a></h2><p><strong>Linear regression</strong> is an example of a <strong>parametric</strong> approach because it assumes a linear functional form for $f(X)$.</p><p><strong>Parametric methods</strong></p><ul><li><strong>Advantages</strong>:<ul><li><em>Easy to fit</em>, because one need estimate only a small number of coefficients.</li><li><em>Simple interpretations</em>, and tests of statistical significance can be easily performed</li></ul></li><li><strong>Disadvantage</strong>:<ul><li><em>Strong assumptions about the form of $f(X)$</em>. If the specified functional form is far from the truth, and
prediction accuracy is our goal, then the parametric method will perform poorly.</li></ul></li></ul><p><strong>Non-parametric methods</strong></p><ul><li>Do not explicitly assume a parametric form for $f(X)$, and thereby provide an alternative and more flexible
approach for performing regression.</li><li><strong>K-nearest neighbors</strong> regression (KNN regression)</li></ul><h2 id=knn-regression>KNN Regression<a hidden class=anchor aria-hidden=true href=#knn-regression>#</a></h2><p>Given a value for $K$ and a prediction point $x_0$, <strong>KNN</strong> regression first identifies the $K$ training observations that are closest to $x_0$, represented by $N_0$. It then estimates $f(x_0)$ using the average of all the training responses in $N_0$.
$$
\begin{align}
\hat{f}(x_0)=\frac{1}{K}\sum_{x_i\in N_0}y_i
\end{align}
$$</p><ul><li>The optimal value for $K$ will depend on the <strong>bias-variance trade-off</strong>.</li><li>A <strong>small value for $K$</strong> provides the <strong>most flexible</strong> fit, which will have <strong>low bias</strong> but <strong>high variance</strong>. This variance is due to the fact that the prediction in a given region is entirely dependent on just one observation.</li><li>A <strong>larger K</strong> provide a <strong>smoother and less variable</strong> fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause <strong>bias</strong> by masking some of the structure in $f(X)$</li></ul><p><strong>The parametric approach will outperform the nonparametric approach if the selected parametric form is close to the true form of $f(x)$.</strong></p><ul><li>A <strong>non-parametric</strong> approach incurs a cost in <strong>variance</strong> that is not offset by a reduction in <strong>bias</strong></li><li><strong>KNN</strong> performs slightly worse than <strong>linear regression</strong> when the <em>relationship is linear</em>, but much better than linear regression for non-linear situations.</li></ul><p><strong>The increase in dimension has only caused a small deterioration in the linear regression test set MSE, but it has caused more than a ten-fold increase in the MSE for KNN.</strong></p><ul><li>This decrease in performance as the dimension increases is a common problem for <strong>KNN</strong>, and results from the fact that in higher dimensions there is effectively a reduction in sample size$\Rightarrow$ <strong>curse of dimensionality</strong></li><li>As a general rule, <strong>parametric methods</strong> will tend to <em><strong>outperform</strong></em> <strong>non-parametric</strong> approaches when there is a small number of observations per predictor.</li></ul><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &ldquo;The elements of statistical learning: data mining, inference and prediction.&rdquo; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p><p>Rice, John A. <em>Mathematical statistics and data analysis</em>. Cengage Learning, 2006.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://nancyyanyu.github.io/tags/linear-regression/>Linear Regression</a></li><li><a href=https://nancyyanyu.github.io/tags/regression/>Regression</a></li></ul><nav class=paginav><a class=prev href=https://nancyyanyu.github.io/posts/ml-linear_discriminant_analysis/ml-linear_discriminant_analysis/><span class=title>« Prev</span><br><span>Study Note: Linear Discriminant Analysis, ROC & AUC, Confusion Matrix</span></a>
<a class=next href=https://nancyyanyu.github.io/posts/ml-logistic_regression/><span class=title>Next »</span><br><span>Study Note: Logistic Regression</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Regression Part I - Linear Regression Models on twitter" href="https://twitter.com/intent/tweet/?text=Study%20Note%3a%20Linear%20Regression%20Part%20I%20-%20Linear%20Regression%20Models&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_regression_models%2f&amp;hashtags=LinearRegression%2cRegression"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Regression Part I - Linear Regression Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_regression_models%2f&amp;title=Study%20Note%3a%20Linear%20Regression%20Part%20I%20-%20Linear%20Regression%20Models&amp;summary=Study%20Note%3a%20Linear%20Regression%20Part%20I%20-%20Linear%20Regression%20Models&amp;source=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_regression_models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Regression Part I - Linear Regression Models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_regression_models%2f&title=Study%20Note%3a%20Linear%20Regression%20Part%20I%20-%20Linear%20Regression%20Models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Regression Part I - Linear Regression Models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_regression_models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Regression Part I - Linear Regression Models on whatsapp" href="https://api.whatsapp.com/send?text=Study%20Note%3a%20Linear%20Regression%20Part%20I%20-%20Linear%20Regression%20Models%20-%20https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_regression_models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Regression Part I - Linear Regression Models on telegram" href="https://telegram.me/share/url?text=Study%20Note%3a%20Linear%20Regression%20Part%20I%20-%20Linear%20Regression%20Models&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_regression_models%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Regression Part I - Linear Regression Models on ycombinator" href="https://news.ycombinator.com/submitlink?t=Study%20Note%3a%20Linear%20Regression%20Part%20I%20-%20Linear%20Regression%20Models&u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-linear_regression_models%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://nancyyanyu.github.io/>Nancy's Notebook</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>