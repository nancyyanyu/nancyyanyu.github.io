<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Study Note: Dimension Reduction - PCA, PCR | Nancy's Notebook</title><meta name=keywords content="PCA,Dimension Reduction,Model Selection"><meta name=description content="Dimension Reduction Methods
Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.
Dimension Reduction Methods transform the predictors and then fit a least
squares model using the transformed variables.
Approach
Let $Z_1,Z_2, . . . ,Z_M$ represent $M < p$ linear combinations of our original $p$ predictors. That is,
$$
\begin{align}
Z_m=\sum_{j=1}^p\phi_{jm}X_j
\end{align}
$$"><meta name=author content="Me"><link rel=canonical href=https://nancyyanyu.github.io/posts/ml-dimension_reduction-pca/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Study Note: Dimension Reduction - PCA, PCR"><meta property="og:description" content="Dimension Reduction Methods
Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.
Dimension Reduction Methods transform the predictors and then fit a least
squares model using the transformed variables.
Approach
Let $Z_1,Z_2, . . . ,Z_M$ represent $M < p$ linear combinations of our original $p$ predictors. That is,
$$
\begin{align}
Z_m=\sum_{j=1}^p\phi_{jm}X_j
\end{align}
$$"><meta property="og:type" content="article"><meta property="og:url" content="https://nancyyanyu.github.io/posts/ml-dimension_reduction-pca/"><meta property="og:image" content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-06-14T14:49:45+00:00"><meta property="article:modified_time" content="2019-06-14T14:49:45+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Study Note: Dimension Reduction - PCA, PCR"><meta name=twitter:description content="Dimension Reduction Methods
Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.
Dimension Reduction Methods transform the predictors and then fit a least
squares model using the transformed variables.
Approach
Let $Z_1,Z_2, . . . ,Z_M$ represent $M < p$ linear combinations of our original $p$ predictors. That is,
$$
\begin{align}
Z_m=\sum_{j=1}^p\phi_{jm}X_j
\end{align}
$$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nancyyanyu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Study Note: Dimension Reduction - PCA, PCR","item":"https://nancyyanyu.github.io/posts/ml-dimension_reduction-pca/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Study Note: Dimension Reduction - PCA, PCR","name":"Study Note: Dimension Reduction - PCA, PCR","description":"Dimension Reduction Methods Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.\nDimension Reduction Methods transform the predictors and then fit a least squares model using the transformed variables.\nApproach Let $Z_1,Z_2, . . . ,Z_M$ represent $M \u0026lt; p$ linear combinations of our original $p$ predictors. That is,\n$$ \\begin{align} Z_m=\\sum_{j=1}^p\\phi_{jm}X_j \\end{align} $$\n","keywords":["PCA","Dimension Reduction","Model Selection"],"articleBody":"Dimension Reduction Methods Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.\nDimension Reduction Methods transform the predictors and then fit a least squares model using the transformed variables.\nApproach Let $Z_1,Z_2, . . . ,Z_M$ represent $M \u003c p$ linear combinations of our original $p$ predictors. That is,\n$$ \\begin{align} Z_m=\\sum_{j=1}^p\\phi_{jm}X_j \\end{align} $$\nfor some constants $φ_{1m}, φ_{2m} . . . , φ_{pm}, m = 1, . . .,M.$ We can then fit the linear regression model $$ \\begin{align} y_i=\\theta_0+\\sum_{m=1}^M\\theta_m z_{im}+\\epsilon_i \\quad i=1,2,3,4,…,n \\end{align} $$ Dimension reduction: reduces the problem of estimating the $p+1$ coefficients $β_0, β_1, . . . , β_p$ to the simpler problem of estimating the $M + 1$ coefficients $θ_0, θ_1, . . . , θ_M$, where M \u003c p. In other words, the dimension of the problem has been reduced from $p + 1$ to $M + 1$. $$ \\begin{align} \\sum_{m=1}^M\\theta_m z_{im}\u0026=\\sum_{m=1}^M\\theta_m \\sum_{j=1}^p\\phi_{jm}x_{ij}=\\sum_{m=1}^M\\sum_{j=1}^p\\theta_m \\phi_{jm}x_{ij}=\\sum_{j=1}^p \\beta_jx_{ij} \\ \\beta_j\u0026=\\sum_{m=1}^M\\theta_m \\phi_{jm} \\end{align} $$ All dimension reduction methods work in two steps:\nThe transformed predictors $Z_1,Z_2, . . . ,Z_M$are obtained. The model is fit using these $M$ predictors. However, the choice of $Z_1,Z_2, . . . ,Z_M$, or equivalently, the selection of the $φ_{jm}$’s, can be achieved in different ways. Principal Components Regression An Overview of Principal Components Analysis Principal component analysis (PCA) refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data.\nPCA also serves as a tool for data visualization (visualization of the observations or visualization of the variables). What Are Principal Components? PCA :finds a low-dimensional representation of a data set that contains as much as possible of the variation\nEach of the dimensions found by PCA is a linear combination of the $p$ features.\nThe first principal component of a set of features $X_1,X_2, . . . , X_p$ is the normalized linear combination of the features $$ \\begin{align} Z_1=\\phi_{11}X_1+\\phi_{21}X_2+,,,+\\phi_{p1}X_p \\end{align} $$ that has the largest variance.\nNormalized: $\\sum_{j=1}^p \\phi_{j1}^2=1$\nLoadings: $\\phi_{11}, . . . , \\phi_{p1}$ the loadings of the first principal component;\nTogether, the loadings make up the principal component loading vector, $\\phi_1=(\\phi_{11},\\phi_{21},…,\\phi_{p1})^T$ 1st Principal Component Interpretation 1: greatest variability The first principal component direction of the data: is that along which the observations vary the most.\nThe first principal component direction is the direction along which there is the greatest variability in the data. That is, if we projected the 100 observations onto this line (as shown in the left-hand panel of Figure 6.15), then the resulting projected observations would have the largest possible variance\nThe first principal component is given by the formula\n$$ \\begin{align} Z_1 = 0.839 × (pop − \\bar{pop}) + 0.544 × (ad − \\bar{ad}) \\end{align} $$ Here $φ_{11} = 0.839$ and $φ_{21} = 0.544$ are the principal component loadings, which define the direction referred to above.\nThe idea is that out of every possible linear combination of pop and ad such that $\\phi_{11}^2+\\phi_{21}^2=1$, this particular linear combination yields the highest variance: i.e. this is the linear combination for which $Var(φ_{11} × (pop − \\bar{pop}) + φ_{21} × (ad − \\bar{ad}))$ is maximized.\nPrincipal Component Scores\nThe values of $z_{11}, . . . , z_{n1}$ are known as the principal component scores, and can be seen in the right-hand panel of Figure 6.15. For example, $$ \\begin{align} z_{i1} = 0.839 × (pop_i − \\bar{pop}) + 0.544 × (ad_i − \\bar{ad}) \\end{align} $$\nInterpretation 2: closest to data There is also another interpretation for PCA: the first principal component vector defines the line that is as close as possible to the data.\nIn Figure 6.14, the first principal component line minimizes the sum of the squared perpendicular distances between each point and the line.\nIn the right-hand panel of Figure 6.15, the left-hand panel has been rotated so that the first principal component direction coincides with the x-axis. It is possible to show that the first principal component score for the ith observation is the distance in the $x$-direction of the ith cross from zero.\nInterpretation 3: single number summarization We can think of the values of the principal component $Z_1$ as single number summaries of the joint pop and ad budgets for each location.\nIn this example, if $z_{i1} = 0.839 × (pop_i − pop) + 0.544 × (ad_i − ad) \u003c 0$, then this indicates a city with below-average population size and belowaverage ad spending.\nFigure 6.16 displays $z_{i1}$ versus both pop and ad. The plots show a strong relationship between the first principal component and the two features. In other words, the first principal component appears to capture most of the information contained in the pop and ad predictors.\nCompute the first principal component Assume that each of the variables in $X$ has been centered to have mean zero. We then look for the linear combination of the sample feature values of the form $$ \\begin{align} z_{i1}=\\phi_{11}x_{i1}+\\phi_{21}x_{i2}+,,,+\\phi_{p1}x_{ip} \\quad \\quad i=1,2,…,n \\end{align} $$ that has largest sample variance, subject to the constraint that $\\sum_{j=1}^p \\phi_{j1}^2=1$\nThe first principal component loading vector solves the optimization problem $$ \\begin{align} \\max_{\\phi_{11},…,\\phi_{p1}} (\\frac{1}{n} \\sum_{i=1}^n \\left( \\sum_{j=1}^p \\phi_{j1}x_{ij} \\right)^2 ) , subject , to , \\sum_{j=1}^p \\phi_{j1}^2=1 \\end{align} $$\nSince $\\sum_{i=1}^nx_{ij}/n=1$, the average of the $z_{11}, . . . , z_{n1}$ will be zero as well. Hence the objective that we are maximizing is just the sample variance of the $n$ values of zi1\nScores: We refer to $z_{11}, . . . , z_{n1}$ as the scores of the first principal component.\nGeometric interpretation: for the first principal component: The loading vector $\\phi_1$ with elements $\\phi_{11},\\phi_{21},…,\\phi_{p1}$ defines a direction in feature space along which the data vary the most. If we project the n data points $x_1, . . . , x_n$ onto this direction, the projected values are the principal component scores $z_{11}, . . . , z_{n1}$ themselves.\n2nd Principal Component The second principal component $Z_2$ is a linear combination of the variables that is uncorrelated with $Z_1$, and has largest variance subject to this constraint.\nIt turns out that the zero correlation condition of $Z_1$ with $Z_2$ is equivalent to the condition that the direction must be perpendicular, or orthogonal, to the first principal component direction.\nThe second principal component is given by the formula:\n$$ \\begin{align} Z_2 = 0.544 × (pop − \\bar{pop}) − 0.839 × (ad − \\bar{ad}). \\end{align} $$ Figure 6.15. The fact that the second principal component scores are much closer to zero indicates that this component captures far less information.\nCompute the second principal component The second principal component $Z_2$: the linear combination of $X_1,X_2, . . . , X_p$ that has maximal variance out of all linear combinations that are uncorrelated with $Z_1$.\nThe second principal component scores $z_{12}, . . . , z_{n2}$ take the form $$ \\begin{align} z_{i2}=\\phi_{12}x_{i1}+\\phi_{22}x_{i2}+,,,+\\phi_{p2}x_{ip} \\quad \\quad i=1,2,…,n \\end{align} $$ where $\\phi_2$ is the second principal component loading vector, with elements $\\phi_{12},\\phi_{22},…,\\phi_{p2}$.\nIt turns out that constraining $Z_2$ to be uncorrelated with $Z_1$ is equivalent to constraining the direction $\\phi_2$ to be orthogonal (perpendicular) to the direction $\\phi_1$.\nTo find $\\phi_2$, we solve a problem similar to (10.3) with $\\phi_2$ replacing $\\phi_1$, and with the additional constraint that $\\phi_2$ is orthogonal to $\\phi_1$\nInterpretation:\n1st loading vector places approximately equal weight on Assault, Murder, and Rape, with much less weight UrbanPop. Hence this component roughly corresponds to a measure of overall rates of serious crimes. Overall, we see that the crime-related variables (Murder, Assault, and Rape) are located close to each other, and that the UrbanPop variable is far from the other three. This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high assault and rape rates—and that the UrbanPop variable is less correlated with the other three. Another Interpretation of Principal Components An alternative interpretation for principal components: principal components provide low-dimensional linear surfaces that are closest to the observations\nThe first principal component loading vector has a very special property: it is the line in p-dimensional space that is closest to the n observations (using average squared Euclidean distance as a measure of closeness).\nThe appeal of this interpretation : we seek a single dimension of the data that lies as close as possible to all of the data points, since such a line will likely provide a good summary of the data.\nThe first two principal components of a data set span the plane that is closest to the n observations, in terms of average squared Euclidean distance\nTogether the first M principal component score vectors and the first M principal component loading vectors provide the best M-dimensional approximation (in terms of Euclidean distance) to the ith observation $x_{ij}$ . $$ \\begin{align} x_{ij} \\approx \\sum_{m=1}^Mz_{im}\\phi_{jm} \\end{align} $$ (assuming the original data matrix X is column-centered).\nWhen $M = min(n − 1, p)$, then the representation is exact: $x_{ij} = \\sum_{m=1}^Mz_{im}\\phi_{jm}$\nMore on PCA Scaling the Variables Before PCA is performed, the variables should be centered to have mean zero. Furthermore, the results obtained when we perform PCA will also depend on whether the variables have been individually scaled (each multiplied by a different constant)\nUniqueness of the Principal Components **Each principal component loading vector $\\phi_1=(\\phi_{11},\\phi_{21},…,\\phi_{p1})^T$ and the score vectors $z_{11}, . . . , z_{n1}$ is unique, up to a sign flip. **\nTwo different software packages will yield the same principal component loading vectors and score vectors, although the signs of those loading vectors may differ. The signs may differ because each principal component loading vector specifies a direction in p-dimensional space: flipping the sign has no effect as the direction does not change. The Proportion of Variance Explained How much of the variance in the data is not contained in the first few principal components?\nProportion of variance explained (PVE) by each principal component:\nThe total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as $$ \\begin{align} \\sum_{j=1}^pVar(X_j)=\\sum_{j=1}^p\\frac{1}{n}\\sum_{i=1}^nx_{ij}^2 \\end{align} $$\nThe variance explained by the mth principal component is $$ \\begin{align} \\frac{1}{n}\\sum_{i=1}^nz_{im}^2=\\frac{1}{n}\\sum_{i=1}^n \\left( \\sum_{j=1}^p \\phi_{jm}x_{ij} \\right)^2 \\end{align} $$\nTherefore, the PVE of the mth principal component is given by $$ \\begin{align} \\frac{\\sum_{i=1}^n \\left( \\sum_{j=1}^p \\phi_{jm}x_{ij} \\right)^2}{\\sum_{j=1}^p\\sum_{i=1}^nx_{ij}^2} \\end{align} $$\nThe PVE of each principal component is a positive quantity. In order to compute the cumulative PVE of the first $M$ principal components, we can simply sum (10.8) over each of the first $M$ PVEs. In total, there are $min(n − 1, p)$ principal components, and their PVEs sum to one.\nDeciding How Many Principal Components to Use We would like to use the smallest number of principal components required to get a good understanding of the data.\nHow many principal components are needed?\nWe typically decide on the number of principal components required to visualize the data by examining a scree plot (Right FIGURE 10.4) We choose the smallest number of principal components that are required in order to explain a sizable amount of the variation in the data. We tend to look at the first few principal components in order to find interesting patterns in the data. If no interesting patterns are found in the first few principal components, then further principal components are unlikely to be of interest. The Principal Components Regression Approach The principal components regression (PCR) approach involves constructing the first M principal components, $Z_1,Z_2, . . . ,Z_M$, and then using these components as the predictors in a linear regression model that is fit using least squares\nThe key idea\nOften a small number of principal components suffice to explain most of the variability in the data, as well as the relationship with the response. In other words, we assume that the directions in which $X_1, . . .,X_p$ show the most variation are the directions that are associated with $Y$\nExample:\nPerforming PCR with an appropriate choice of M can result in a substantial improvement over least squares PCR does not perform as well as the two shrinkage methods Reason: The data were generated in such a way that many principal components are required in order to adequately model the response. In contrast, PCR will tend to do well in cases when the first few principal components are sufficient to capture most of the variation in the predictors as well as the relationship with the response. Note: even though PCR provides a simple way to perform regression using $M \u003c p$ predictors, it is not a feature selection method!\nThis is because each of the $M$ principal components used in the regression is a linear combination of all p of the original features. PCR is more closely related to ridge regression than to the lasso. One can even think of ridge regression as a continuous version of PCR! Cross-validation: In PCR, the number of principal components, $M$, is typically chosen by cross-validation.\nStandardisation: When performing PCR, we generally recommend standardizing each predictor, prior to generating the principal components.\nIn the absence of standardization, the high-variance variables will tend to play a larger role in the principal components obtained, and the scale on which the variables are measured will ultimately have an effect on the final PCR model. Ref:\nJames, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.\nHastie, Trevor, et al. “The elements of statistical learning: data mining, inference and prediction.” The Mathematical Intelligencer 27.2 (2005): 83-85\n","wordCount":"2235","inLanguage":"en","datePublished":"2019-06-14T14:49:45Z","dateModified":"2019-06-14T14:49:45Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nancyyanyu.github.io/posts/ml-dimension_reduction-pca/"},"publisher":{"@type":"Organization","name":"Nancy's Notebook","logo":{"@type":"ImageObject","url":"https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nancyyanyu.github.io/ accesskey=h title="Nancy's Notebook (Alt + H)"><img src=https://nancyyanyu.github.io/apple-touch-icon.png alt aria-label=logo height=35>Nancy's Notebook</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nancyyanyu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://nancyyanyu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://nancyyanyu.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nancyyanyu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nancyyanyu.github.io/posts/>Posts</a></div><h1 class=post-title>Study Note: Dimension Reduction - PCA, PCR</h1><div class=post-meta><span title='2019-06-14 14:49:45 +0000 UTC'>June 14, 2019</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;2235 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/ML-Dimension_Reduction-PCA/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h1 id=dimension-reduction-methods>Dimension Reduction Methods<a hidden class=anchor aria-hidden=true href=#dimension-reduction-methods>#</a></h1><p>Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.</p><p>Dimension Reduction Methods <em><strong>transform</strong></em> the predictors and then fit a least
squares model using the transformed variables.</p><h2 id=approach>Approach<a hidden class=anchor aria-hidden=true href=#approach>#</a></h2><p>Let $Z_1,Z_2, . . . ,Z_M$ represent $M &lt; p$ linear combinations of our original $p$ predictors. That is,</p><p>$$
\begin{align}
Z_m=\sum_{j=1}^p\phi_{jm}X_j
\end{align}
$$</p><p>for some constants $φ_{1m}, φ_{2m} . . . , φ_{pm}, m = 1, . . .,M.$ We can then fit the
linear regression model
$$
\begin{align}
y_i=\theta_0+\sum_{m=1}^M\theta_m z_{im}+\epsilon_i \quad i=1,2,3,4,&mldr;,n
\end{align}
$$
<strong>Dimension reduction</strong>: reduces the problem of estimating the $p+1$ coefficients $β_0, β_1, . . . , β_p$ to the
simpler problem of estimating the $M + 1$ coefficients $θ_0, θ_1, . . . , θ_M$, where
M &lt; p. In other words, the dimension of the problem has been reduced
from $p + 1$ to $M + 1$.
$$
\begin{align}
\sum_{m=1}^M\theta_m z_{im}&=\sum_{m=1}^M\theta_m \sum_{j=1}^p\phi_{jm}x_{ij}=\sum_{m=1}^M\sum_{j=1}^p\theta_m \phi_{jm}x_{ij}=\sum_{j=1}^p \beta_jx_{ij} \
\beta_j&=\sum_{m=1}^M\theta_m \phi_{jm}
\end{align}
$$
<strong>All dimension reduction methods work in two steps:</strong></p><ol><li>The transformed predictors $Z_1,Z_2, . . . ,Z_M$are obtained.</li><li>The model is fit using these $M$ predictors. However, the choice of $Z_1,Z_2, . . . ,Z_M$, or equivalently,
the selection of the $φ_{jm}$’s, can be achieved in different ways.</li></ol><h1 id=principal-components-regression>Principal Components Regression<a hidden class=anchor aria-hidden=true href=#principal-components-regression>#</a></h1><h2 id=an-overview-of-principal-components-analysis>An Overview of Principal Components Analysis<a hidden class=anchor aria-hidden=true href=#an-overview-of-principal-components-analysis>#</a></h2><p><strong>Principal component analysis (PCA)</strong> refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data.</p><ul><li>PCA also serves as a tool for data visualization (visualization of
the observations or visualization of the variables).</li></ul><h2 id=what-are-principal-components>What Are Principal Components?<a hidden class=anchor aria-hidden=true href=#what-are-principal-components>#</a></h2><p><strong>PCA</strong> :finds a low-dimensional representation of a data set that contains as much as possible of the <strong>variation</strong></p><p>Each of the dimensions found by PCA is a linear combination of the $p$ features.</p><p><em><strong>The first principal component</strong></em> of a set of features $X_1,X_2, . . . , X_p$ is the normalized linear combination of the features
$$
\begin{align}
Z_1=\phi_{11}X_1+\phi_{21}X_2+,,,+\phi_{p1}X_p
\end{align}
$$
that has the <strong>largest variance</strong>.</p><p><strong>Normalized</strong>: $\sum_{j=1}^p \phi_{j1}^2=1$</p><p><strong>Loadings</strong>: $\phi_{11}, . . . , \phi_{p1}$ the loadings of the first principal component;</p><ul><li>Together, the loadings make up the principal component loading vector, $\phi_1=(\phi_{11},\phi_{21},&mldr;,\phi_{p1})^T$</li></ul><h3 id=1st-principal-component>1st Principal Component<a hidden class=anchor aria-hidden=true href=#1st-principal-component>#</a></h3><h4 id=interpretation-1-greatest-variability>Interpretation 1: greatest variability<a hidden class=anchor aria-hidden=true href=#interpretation-1-greatest-variability>#</a></h4><p><strong>The first principal component</strong> direction of the data: is that along which the observations <strong>vary the most</strong>.</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-dimension_reduction-pca/7_huf69fb881672e971cf07377277933e5e4_138367_600x0_resize_box_3.png width=600 height=350><figcaption><small></small></figcaption></figure><p>The first principal component direction is the direction along which there is the greatest variability in the data. That is, if we projected the 100 observations onto this line (as shown in the left-hand panel of Figure 6.15), then the resulting projected observations would have the largest possible variance</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-dimension_reduction-pca/8_hu80081a7b1ef6e5839c82fb0a069fc495_221190_600x0_resize_box_3.png width=600 height=358><figcaption><small></small></figcaption></figure><p>The first principal component is given by the formula</p><p>$$
\begin{align}
Z_1 = 0.839 × (pop − \bar{pop}) + 0.544 × (ad − \bar{ad})
\end{align}
$$
Here $φ_{11} = 0.839$ and $φ_{21} = 0.544$ are the <strong>principal component loadings</strong>,
which define the direction referred to above.</p><blockquote><p>The idea is that out of every possible linear combination
of pop and ad such that $\phi_{11}^2+\phi_{21}^2=1$, this particular linear combination
yields the highest variance: i.e. this is the linear combination for which
$Var(φ_{11} × (pop − \bar{pop}) + φ_{21} × (ad − \bar{ad}))$ is maximized.</p></blockquote><p><strong>Principal Component Scores</strong></p><p>The values of $z_{11}, . . . , z_{n1}$ are known as the <strong>principal component scores</strong>, and
can be seen in the right-hand panel of Figure 6.15. For example,
$$
\begin{align}
z_{i1} = 0.839 × (pop_i − \bar{pop}) + 0.544 × (ad_i − \bar{ad})
\end{align}
$$</p><h4 id=interpretation-2-closest-to-data>Interpretation 2: closest to data<a hidden class=anchor aria-hidden=true href=#interpretation-2-closest-to-data>#</a></h4><p>There is also another interpretation for PCA: the first principal component
vector defines the line that is as close as possible to the data.</p><p>In Figure 6.14, the first principal component line minimizes the
sum of the squared perpendicular distances between each point and the
line.</p><p>In the right-hand panel of Figure 6.15, the left-hand panel has been
rotated so that the first principal component direction coincides with the
x-axis. It is possible to show that the <em><strong>first principal component score</strong></em> for
the ith observation is the distance in the $x$-direction of the
ith cross from zero.</p><h4 id=interpretation-3-single-number-summarization>Interpretation 3: single number summarization<a hidden class=anchor aria-hidden=true href=#interpretation-3-single-number-summarization>#</a></h4><p>We can think of the values of the principal component $Z_1$ as single number
summaries of the joint pop and ad budgets for each location.</p><p>In this example, if $z_{i1} = 0.839 × (pop_i − pop) + 0.544 × (ad_i − ad) &lt; 0$, then this indicates a city with below-average population size and belowaverage ad spending.</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-dimension_reduction-pca/9_hu1e302a7f9b1d3235a82a3e20cca15dd8_122592_700x0_resize_box_3.png width=700 height=336><figcaption><small></small></figcaption></figure><p>Figure 6.16 displays
$z_{i1}$ versus both pop and ad. The plots show a strong relationship between
the first principal component and the two features. In other words, the first
principal component appears to <em>capture most of the information</em> contained
in the pop and ad predictors.</p><h4 id=compute-the-first-principal-component>Compute the first principal component<a hidden class=anchor aria-hidden=true href=#compute-the-first-principal-component>#</a></h4><ul><li><p>Assume that each of the variables in $X$ has been centered to have mean zero. We then look for the linear combination of the sample feature values of the form
$$
\begin{align}
z_{i1}=\phi_{11}x_{i1}+\phi_{21}x_{i2}+,,,+\phi_{p1}x_{ip} \quad \quad i=1,2,&mldr;,n
\end{align}
$$
that has largest sample variance, subject to the constraint that $\sum_{j=1}^p \phi_{j1}^2=1$</p></li><li><p>The first principal component loading vector solves the optimization problem
$$
\begin{align}
\max_{\phi_{11},&mldr;,\phi_{p1}} (\frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p \phi_{j1}x_{ij} \right)^2 ) , subject , to , \sum_{j=1}^p \phi_{j1}^2=1
\end{align}
$$</p></li><li><p>Since $\sum_{i=1}^nx_{ij}/n=1$, the average of the $z_{11}, . . . , z_{n1}$ will be zero as well. Hence
the objective that we are maximizing is just the <strong>sample variance</strong> of the $n$ values of zi1</p></li><li><p><strong>Scores</strong>: We refer to $z_{11}, . . . , z_{n1}$ as the scores of the first principal component.</p></li></ul><p><strong>Geometric interpretation</strong>: for the first principal component: The loading vector $\phi_1$ with elements $\phi_{11},\phi_{21},&mldr;,\phi_{p1}$ defines a direction in
feature space along which the data <strong>vary the most</strong>. If we project the n data
points $x_1, . . . , x_n$ onto this direction, the projected values are the principal
component scores $z_{11}, . . . , z_{n1}$ themselves.</p><h3 id=2nd-principal-component>2nd Principal Component<a hidden class=anchor aria-hidden=true href=#2nd-principal-component>#</a></h3><p>The s<strong>econd principal component $Z_2$</strong> is a linear combination of the variables that is uncorrelated
with $Z_1$, and has largest variance subject to this constraint.</p><p>It turns out that the zero correlation condition of $Z_1$ with $Z_2$ is equivalent to the condition that the direction must be perpendicular, or orthogonal, to the first principal component direction.</p><p>The second principal component is given by the formula:</p><p>$$
\begin{align}
Z_2 = 0.544 × (pop − \bar{pop}) − 0.839 × (ad − \bar{ad}).
\end{align}
$$
Figure 6.15. The fact that the second principal component scores are much closer to zero indicates that
this component captures far less information.</p><h4 id=compute-the-second-principal-component>Compute the second principal component<a hidden class=anchor aria-hidden=true href=#compute-the-second-principal-component>#</a></h4><p><strong>The second principal component $Z_2$</strong>: the linear combination of $X_1,X_2, . . . , X_p$ that has maximal
variance out of all linear combinations that are <strong>uncorrelated with $Z_1$</strong>.</p><p>The second principal component scores $z_{12}, . . . , z_{n2}$ take the form
$$
\begin{align}
z_{i2}=\phi_{12}x_{i1}+\phi_{22}x_{i2}+,,,+\phi_{p2}x_{ip} \quad \quad i=1,2,&mldr;,n
\end{align}
$$
where $\phi_2$ is the second principal component <strong>loading</strong> vector, with elements
$\phi_{12},\phi_{22},&mldr;,\phi_{p2}$.</p><p>It turns out that constraining $Z_2$ to be uncorrelated with $Z_1$ is equivalent to constraining the direction $\phi_2$ to be <strong>orthogonal</strong> (perpendicular) to the direction $\phi_1$.</p><p>To find $\phi_2$, we solve a problem similar to (10.3) with $\phi_2$ replacing $\phi_1$, and with the additional constraint that $\phi_2$ is orthogonal to $\phi_1$</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-dimension_reduction-pca/1_v2_hu04dce6580fe9a0305b33ec5e3038452f_304593_700x0_resize_box_3.png width=700 height=791><figcaption><small></small></figcaption></figure><p><strong>Interpretation:</strong></p><ul><li>1st loading vector places approximately equal weight on Assault, Murder, and Rape, with much less weight UrbanPop. Hence this component roughly corresponds to a measure of overall
rates of serious crimes.</li><li>Overall, we see that the crime-related variables (Murder, Assault, and Rape)
are located close to each other, and that the UrbanPop variable is far from
the other three.</li><li>This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high
assault and rape rates—and that the UrbanPop variable is less correlated
with the other three.</li></ul><h2 id=another-interpretation-of-principal-components>Another Interpretation of Principal Components<a hidden class=anchor aria-hidden=true href=#another-interpretation-of-principal-components>#</a></h2><p><strong>An alternative interpretation for principal components</strong>: principal components provide low-dimensional linear surfaces that are closest to the observations</p><ul><li><p><strong>The first principal component loading vector has a very special property</strong>:
it is the line in p-dimensional space that is closest to the n observations
(using average squared Euclidean distance as a measure of closeness).</p></li><li><p>The appeal of this interpretation : we seek a single dimension of the data that lies as close as possible to all of the data points, since such a line will likely provide a good summary of the
data.</p></li><li><p><strong>The first two principal components</strong> of a data set <strong>span the plane</strong> that is closest to the n observations, in terms of average squared Euclidean distance</p></li><li><p>Together <strong>the first M principal component</strong> score vectors and the first M principal component loading vectors provide the best M-dimensional approximation (in terms of Euclidean distance) to the ith observation $x_{ij}$ .
$$
\begin{align}
x_{ij} \approx \sum_{m=1}^Mz_{im}\phi_{jm}
\end{align}
$$
(assuming the original data matrix X is column-centered).</p></li><li><p>When $M = min(n − 1, p)$, then the representation is exact: $x_{ij} = \sum_{m=1}^Mz_{im}\phi_{jm}$</p></li></ul><h2 id=more-on-pca>More on PCA<a hidden class=anchor aria-hidden=true href=#more-on-pca>#</a></h2><h3 id=scaling-the-variables>Scaling the Variables<a hidden class=anchor aria-hidden=true href=#scaling-the-variables>#</a></h3><p>Before PCA is performed, the variables should be <strong>centered to have mean zero</strong>. Furthermore, the results obtained when we perform PCA will also depend on whether the variables have been
<strong>individually scaled</strong> (each multiplied by a different constant)</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-dimension_reduction-pca/2_v2_hub99d7991c89559da696a6bbbe1e6ea8f_251865_600x0_resize_box_3.png width=600 height=440><figcaption><small></small></figcaption></figure><h3 id=uniqueness-of-the-principal-components>Uniqueness of the Principal Components<a hidden class=anchor aria-hidden=true href=#uniqueness-of-the-principal-components>#</a></h3><p>**Each principal component loading vector $\phi_1=(\phi_{11},\phi_{21},&mldr;,\phi_{p1})^T$ and the score vectors $z_{11}, . . . , z_{n1}$ is unique, up to a sign flip. **</p><ul><li>Two different software packages will yield the same principal
component loading vectors and score vectors, although the signs of those loading vectors
may differ.</li><li><strong>The signs may differ</strong> because each principal component loading
vector specifies a direction in p-dimensional space: flipping the sign has no
effect as the direction does not change.</li></ul><h3 id=the-proportion-of-variance-explained>The Proportion of Variance Explained<a hidden class=anchor aria-hidden=true href=#the-proportion-of-variance-explained>#</a></h3><p><strong>How much of the variance in the data is not contained in the first few principal components?</strong></p><p><strong>Proportion of variance explained (PVE)</strong> by each principal component:</p><ul><li>The total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as</li></ul><p>$$
\begin{align}
\sum_{j=1}^pVar(X_j)=\sum_{j=1}^p\frac{1}{n}\sum_{i=1}^nx_{ij}^2
\end{align}
$$</p><ul><li>The variance explained by the mth principal component is</li></ul><p>$$
\begin{align}
\frac{1}{n}\sum_{i=1}^nz_{im}^2=\frac{1}{n}\sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2
\end{align}
$$</p><ul><li>Therefore, the <strong>PVE of the mth principal component</strong> is given by</li></ul><p>$$
\begin{align}
\frac{\sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2}{\sum_{j=1}^p\sum_{i=1}^nx_{ij}^2}
\end{align}
$$</p><p>The PVE of each principal component is a positive quantity. In order to
compute the <strong>cumulative PVE</strong> of the first $M$ principal components, we
can simply sum (10.8) over each of the first $M$ PVEs. In total, there are
$min(n − 1, p)$ principal components, and their PVEs sum to one.</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-dimension_reduction-pca/3_v2_hu73d2377e8a96f08a94d710693c7a29ce_161850_600x0_resize_box_3.png width=600 height=358><figcaption><small></small></figcaption></figure><h3 id=deciding-how-many-principal-components-to-use>Deciding How Many Principal Components to Use<a hidden class=anchor aria-hidden=true href=#deciding-how-many-principal-components-to-use>#</a></h3><p>We would like to use the smallest number of principal components required to get a good understanding of the data.</p><p><strong>How many principal components are needed?</strong></p><ul><li>We typically decide on the number of principal components required to visualize the data by examining a <strong>scree plot</strong> (Right FIGURE 10.4)</li><li>We choose the smallest number of
principal components that are required in order to explain a sizable amount
of the variation in the data.</li><li>We tend to look
at the first few principal components in order to find interesting patterns
in the data. If no interesting patterns are found in the first few principal
components, then further principal components are unlikely to be of interest.</li></ul><h2 id=the-principal-components-regression-approach>The Principal Components Regression Approach<a hidden class=anchor aria-hidden=true href=#the-principal-components-regression-approach>#</a></h2><p>The principal components regression (PCR) approach involves constructing the first M principal components, $Z_1,Z_2, . . . ,Z_M$, and then using these components
as the predictors in a linear regression model that is fit
using least squares</p><p><strong>The key idea</strong></p><p>Often a small number of principal components suffice to explain most of the variability in the data, as
well as the relationship with the response. In other words, we assume that
<em><strong>the directions in which $X_1, . . .,X_p$ show the most variation are the directions
that are associated with $Y$</strong></em></p><p><strong>Example</strong>:</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-dimension_reduction-pca/10_hu357eb6b15d5481b48181a9ae1e863575_248127_600x0_resize_box_3.png width=600 height=395><figcaption><small></small></figcaption></figure><ul><li>Performing PCR with an appropriate
choice of M can result in a substantial improvement over least squares</li><li>PCR does not perform as well as the two shrinkage methods<ul><li><strong>Reason</strong>: The data were generated in such a way that many principal
components are required in order to adequately model the response.
In contrast, PCR will tend to do well in cases when the first few principal
components are sufficient to capture most of the variation in the predictors
as well as the relationship with the response.</li></ul></li></ul><p><strong>Note</strong>: even though PCR provides a simple way to perform
regression using $M &lt; p$ predictors, it is not a <em>feature selection</em> method!</p><ul><li>This is because each of the $M$ principal components used in the regression is a linear combination of all p of the original features.</li><li>PCR is more closely related to ridge regression than
to the lasso. One can even think of ridge regression as a continuous version
of PCR!</li></ul><p><strong>Cross-validation</strong>: In PCR, the number of principal components, $M$, is typically chosen by
cross-validation.</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-dimension_reduction-pca/11_hudb14d0b7180b89c7b9779299b20c17fb_169627_600x0_resize_box_3.png width=600 height=327><figcaption><small></small></figcaption></figure><p><strong>Standardisation</strong>: When performing PCR, we generally recommend standardizing each
predictor, prior to generating the principal components.</p><ul><li>In the
absence of standardization, the <em>high-variance variables</em> will tend to play a
larger role in the principal components obtained, and the scale on which
the variables are measured will ultimately have an effect on the final PCR
model.</li></ul><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &ldquo;The elements of statistical learning: data mining, inference and prediction.&rdquo; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://nancyyanyu.github.io/tags/pca/>PCA</a></li><li><a href=https://nancyyanyu.github.io/tags/dimension-reduction/>Dimension Reduction</a></li><li><a href=https://nancyyanyu.github.io/tags/model-selection/>Model Selection</a></li></ul><nav class=paginav><a class=prev href=https://nancyyanyu.github.io/posts/ml-clustering/><span class=title>« Prev</span><br><span>Study Note: Clustering</span></a>
<a class=next href=https://nancyyanyu.github.io/posts/ml-svm/><span class=title>Next »</span><br><span>Study Note: SVM</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Dimension Reduction - PCA, PCR on twitter" href="https://twitter.com/intent/tweet/?text=Study%20Note%3a%20Dimension%20Reduction%20-%20PCA%2c%20PCR&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-dimension_reduction-pca%2f&amp;hashtags=PCA%2cDimensionReduction%2cModelSelection"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Dimension Reduction - PCA, PCR on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-dimension_reduction-pca%2f&amp;title=Study%20Note%3a%20Dimension%20Reduction%20-%20PCA%2c%20PCR&amp;summary=Study%20Note%3a%20Dimension%20Reduction%20-%20PCA%2c%20PCR&amp;source=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-dimension_reduction-pca%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Dimension Reduction - PCA, PCR on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-dimension_reduction-pca%2f&title=Study%20Note%3a%20Dimension%20Reduction%20-%20PCA%2c%20PCR"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Dimension Reduction - PCA, PCR on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-dimension_reduction-pca%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Dimension Reduction - PCA, PCR on whatsapp" href="https://api.whatsapp.com/send?text=Study%20Note%3a%20Dimension%20Reduction%20-%20PCA%2c%20PCR%20-%20https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-dimension_reduction-pca%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Dimension Reduction - PCA, PCR on telegram" href="https://telegram.me/share/url?text=Study%20Note%3a%20Dimension%20Reduction%20-%20PCA%2c%20PCR&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-dimension_reduction-pca%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Dimension Reduction - PCA, PCR on ycombinator" href="https://news.ycombinator.com/submitlink?t=Study%20Note%3a%20Dimension%20Reduction%20-%20PCA%2c%20PCR&u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-dimension_reduction-pca%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://nancyyanyu.github.io/>Nancy's Notebook</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>