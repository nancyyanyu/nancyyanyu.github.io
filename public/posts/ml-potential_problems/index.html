<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Study Note: Linear Regression Part II - Potential Problems | Nancy's Notebook</title><meta name=keywords content="Linear Regression,Regression"><meta name=description content="Qualitative Predictors
Predictors with Only Two Levels
Suppose that we wish to investigate differences in credit card balance between
males and females, ignoring the other variables for the moment. If a
qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values.




  


  
  
  
    
  
  


and use this variable as a predictor in the regression equation. This results
in the model




  


  
  
  
    
  
  

"><meta name=author content="Me"><link rel=canonical href=https://nancyyanyu.github.io/posts/ml-potential_problems/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Study Note: Linear Regression Part II - Potential Problems"><meta property="og:description" content="Qualitative Predictors
Predictors with Only Two Levels
Suppose that we wish to investigate differences in credit card balance between
males and females, ignoring the other variables for the moment. If a
qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values.




  


  
  
  
    
  
  


and use this variable as a predictor in the regression equation. This results
in the model




  


  
  
  
    
  
  

"><meta property="og:type" content="article"><meta property="og:url" content="https://nancyyanyu.github.io/posts/ml-potential_problems/"><meta property="og:image" content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-06-07T00:00:13+00:00"><meta property="article:modified_time" content="2019-06-07T00:00:13+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Study Note: Linear Regression Part II - Potential Problems"><meta name=twitter:description content="Qualitative Predictors
Predictors with Only Two Levels
Suppose that we wish to investigate differences in credit card balance between
males and females, ignoring the other variables for the moment. If a
qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values.




  


  
  
  
    
  
  


and use this variable as a predictor in the regression equation. This results
in the model




  


  
  
  
    
  
  

"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nancyyanyu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Study Note: Linear Regression Part II - Potential Problems","item":"https://nancyyanyu.github.io/posts/ml-potential_problems/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Study Note: Linear Regression Part II - Potential Problems","name":"Study Note: Linear Regression Part II - Potential Problems","description":"Qualitative Predictors Predictors with Only Two Levels Suppose that we wish to investigate differences in credit card balance between males and females, ignoring the other variables for the moment. If a qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values. and use this variable as a predictor in the regression equation. This results in the model ","keywords":["Linear Regression","Regression"],"articleBody":"Qualitative Predictors Predictors with Only Two Levels Suppose that we wish to investigate differences in credit card balance between males and females, ignoring the other variables for the moment. If a qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values. and use this variable as a predictor in the regression equation. This results in the model Now β0 can be interpreted as the average credit card balance among males, β0 + β1 as the average credit card balance among females\nQualitative Predictors with More than Two Levels When a qualitative predictor has more than two levels, we can create additional dummy variables. For example, for the ethnicity variable we create two dummy variables. The first could be and the second could be Then both of these variables can be used in the regression equation, in order to obtain the model\nBaseline\nThere will always be one fewer dummy variable than the number of levels. The level with no dummy variable—African American in this example—is known as the baseline. The p-values associated with the coefficient estimates for the two dummy variables are very large, suggesting no statistical evidence of a real difference in credit card balance between the ethnicities\nThe coefficients and their p-values do depend on the choice of dummy variable coding\nRather than rely on the individual coefficients, we can use an F-test to test H0 : β1 = β2 = 0; this does not depend on the coding.\nThis F-test has a p-value of 0.96, indicating that we cannot reject the null hypothesis that there is no relationship between balance and ethnicity.\nExtensions of the Linear Model Two of the most important assumptions state that the relationship between the predictors and response are additive and linear.\nAdditive: the effect of changes in a predictor $X_j$ on the response $Y$ is independent of the values of the other predictors Linear: the change in the response $Y$ due to a one-unit change in $X_j$ is constant, regardless of the value of $X_j$ Removing the Additive Assumption Consider the standard linear regression model with two variables, $$ \\begin{align} Y = β_0 + β_1X_1 + β_2X_2 + \\epsilon \\end{align} $$\nOne way of extending this model to allow for interaction effects is to include a third predictor, called an interaction term: $$ \\begin{align} Y = β_0 + β_1X_1 + β_2X_2 + β_3X_1X_2 + \\epsilon \\end{align} $$ How does inclusion of this interaction term relax the additive assumption?\nThe model above could be written as: $$ \\begin{align} Y \u0026= β_0 + (β_1+β_3X_2)X_1 + β_2X_2 + \\epsilon \\\\ \u0026= β_0 + \\tilde{β}_1X_1 + β_2X_2 + \\epsilon \\end{align} $$ Since $\\tilde{β}_1$ changes with $X_2$, the effect of $X_1$ on $Y$ is no longer constant: adjusting $X_2$ will change the impact of $X_1$ on $Y$.\nSometimes the case that an interaction term has a very small p-value, but the associated main effects (in this case, TV and radio) do not. The hierarchical principle states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant. (If the interaction between X1 and X2 seems important, we should include both X1 and X2 in the model even if their coefficient estimates have large p-values) Concept of interactions applies on qualitative variables Adding an interaction variable, model now becomes: Non-linear Relationships Extending the linear model to accommodate non-linear relationships is known as polynomial regression, since we have included polynomial functions of the predictors in the regression model\nPotential Problems Non-linearity of the Data Assumption: The linear regression model assumes that there is a straight-line relationship between the predictors and the response.\nResidual plots: graphical tool for identifying non-linearity\nGiven a simple linear regression model, we can plot the residuals,$e_i = y_i-\\hat{y_i}$ versus the predictor $x_i$, or $\\hat{y_i}$ when there are multiple predictors Ideally, the residual plot will show no discernible pattern. If the residual plot indicates non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as $\\log{X},\\sqrt{X}, X^2$, in the regression model. Correlation of Error Terms Assumption: The error terms, $\\epsilon_1,\\epsilon_2,…,\\epsilon_n$, are uncorrelated.\nIf the errors are uncorrelated, then the fact that \u0003i is positive provides little or no information about the sign of \u0003i+1. If the error terms are correlated, we may have an unwarranted sense of confidence in our model. estimated standard errors will underestimate the true standard errors. confidence and prediction intervals will be narrower than they should be. For example, a 95% confidence interval may in reality have a much lower probability than 0.95 of containing the true value of the parameter. p-values will be lower than they should be Lead to erroneously conclude that a parameter is statistically significant. Why might correlations among the error terms occur?\nSuch correlations frequently occur in the context of time series data In many cases, observations that are obtained at adjacent time points will have positively correlated errors. Plot the residuals from our model as a function of time to identify this correlation. Tracking: If the error terms are positively correlated, then we may see tracking in the residuals—that is, adjacent residuals may have similar values. Non-constant Variance of Error Terms Assumption: the error terms have a constant variance, $Var(\\epsilon_i) = σ^2$.\nThe standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption. The variances of the error terms are non-constant.\nFor instance, the variances of the error terms may increase with the value of the response. One can identify non-constant variances in the errors, or heteroscedasticity异方差性,from the presence of a funnel shape in residual plot. Solution: transform the response Y using a concave function such as $\\log{Y}$ or $\\sqrt{Y}$ . Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity. Outliers Outlier: is a point for which $y_i$ is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.\nProblems of Outlier:\nEffect on the least squares fit, Effect on interpretation of the fit For instance, in this example, the RSE is 1.09 when the outlier is included in the regression, but it is only 0.77 when the outlier is removed. Residual Plots can be used to identify outliers\nDifficult to decide how large a residual needs to be Studentized residuals: computed by dividing each residual $e_i$ by its estimated standard error.\nObservations whose studentized residuals are greater than 3 in abso- residual lute value are possible outliers. High Leverage Points High Leverage:Observations with high leverage have an unusual value for xi\nremoving the high leverage observation has a much more substantial impact on the least squares line than removing the outlier. Leverage Statistic: quantify an observation’s leverage\nFor a simple linear regression\n$$ \\begin{align} h_i=\\frac{1}{n}+\\frac{(x_i-\\bar{x})^2}{\\sum_{i^{’}=1}^n (x_{i^{’}}-\\bar{x})^2} \\end{align} $$\n$h_i$ increases with the distance of $x_i$ from $\\bar{x}$. $h_i$ is always between 1/n and 1, and the average leverage for all the observations is always equal to $(p+1)/n$. High leverage: a leverage statistic that greatly exceeds $(p+1)/n$, high leverage. The right-hand panel of Figure 3.13 provides a plot of the studentized residuals versus $h_i$ for the data in the left-hand panel of Figure 3.13. Observation 41 stands out as having a very high leverage statistic as well as a high studentized residual. In other words, it is an outlier as well as a high leverage observation.\nCollinearity Collinearity: situation in which two or more predictor variables are closely related to one another.\nProblems of Collinearity\nDifficult to separate out the individual effects of collinear variables on the response Uncertainty in the coefficient estimates. Causes the standard error for $\\hat{β_j}$ to grow Recall that the t-statistic for each predictor is calculated by dividing $\\hat{β_j}$ by its standard error. Consequently, collinearity results in a decline in the t-statistic. As a result, in the presence of collinearity, we may fail to reject H0 : βj = 0. This means that the power of the hypothesis test—the probability of correctly detecting a non-zero coefficient—is reduced by collinearity. Detection of Collinearity\nCorrelation matrix of the predictors. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables. Situation Multicollinearity: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation Variance Inflation Factor (VIF) The ratio of the variance of $\\hat{β_j}$ when fitting the full model divided by the variance of $\\hat{β_j}$ if fit on its own. The smallest possible value for VIF is 1, which indicates the complete absence of collinearity. A VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. Solution of Collinearity\nDrop one of the problematic variables from the regression. Combine the collinear variables together into a single predicto E.g.: take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness Ref:\nJames, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.\nHastie, Trevor, et al. “The elements of statistical learning: data mining, inference and prediction.” The Mathematical Intelligencer 27.2 (2005): 83-85\n","wordCount":"1569","inLanguage":"en","datePublished":"2019-06-07T00:00:13Z","dateModified":"2019-06-07T00:00:13Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nancyyanyu.github.io/posts/ml-potential_problems/"},"publisher":{"@type":"Organization","name":"Nancy's Notebook","logo":{"@type":"ImageObject","url":"https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nancyyanyu.github.io/ accesskey=h title="Nancy's Notebook (Alt + H)"><img src=https://nancyyanyu.github.io/apple-touch-icon.png alt aria-label=logo height=35>Nancy's Notebook</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nancyyanyu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://nancyyanyu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://nancyyanyu.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nancyyanyu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nancyyanyu.github.io/posts/>Posts</a></div><h1 class=post-title>Study Note: Linear Regression Part II - Potential Problems</h1><div class=post-meta><span title='2019-06-07 00:00:13 +0000 UTC'>June 7, 2019</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1569 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/ML-Potential_Problems/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h1 id=qualitative-predictors>Qualitative Predictors<a hidden class=anchor aria-hidden=true href=#qualitative-predictors>#</a></h1><h2 id=predictors-with-only-two-levels>Predictors with Only Two Levels<a hidden class=anchor aria-hidden=true href=#predictors-with-only-two-levels>#</a></h2><p>Suppose that we wish to investigate differences in credit card balance between
males and females, ignoring the other variables for the moment. If a
qualitative predictor (also known as a <strong>factor</strong>) only has two <strong>levels</strong>, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or <strong>dummy variable</strong> that takes on two possible numerical values.<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/18_hu1d6561f500ae721a7ec01a6f000b787f_275828_300x0_resize_box_3.png width=300 height=208><figcaption><small></small></figcaption></figure></p><p>and use this variable as a predictor in the regression equation. This results
in the model<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/19_hua06852bdb939fc8b72c96de7080965a3_222927_600x0_resize_box_3.png width=600 height=551><figcaption><small></small></figcaption></figure></p><p>Now β0 can be interpreted as the average credit card balance among males, β0 + β1 as the average credit card balance among females</p><h2 id=qualitative-predictors-with-more-than-two-levels>Qualitative Predictors with More than Two Levels<a hidden class=anchor aria-hidden=true href=#qualitative-predictors-with-more-than-two-levels>#</a></h2><p>When a qualitative predictor has more than two levels, we can create additional dummy variables. For example, for the ethnicity variable we create two dummy variables. The first could be<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/10_huc4634689aa4a35d7fed073e7a2b8dcf7_24407_300x0_resize_box_3.png width=300 height=53><figcaption><small></small></figcaption></figure></p><p>and the second could be<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/11_hu1cd355710aa9f54413ff161c65ba3f63_26692_300x0_resize_box_3.png width=300 height=43><figcaption><small></small></figcaption></figure></p><p>Then both of these variables can be used in the regression equation, in order to obtain the model</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/12_hu495efb0ecfb424d778c1eb41d1a9965f_44336_600x0_resize_box_3.png width=600 height=77><figcaption><small></small></figcaption></figure><p><strong>Baseline</strong></p><ul><li>There will always be <strong>one fewer</strong> dummy variable than the number of levels. The level with no dummy variable—African American in this example—is known as the baseline.</li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/13_hu7da117e0454beb7269cf549198e709ca_120981_600x0_resize_box_3.png width=600 height=167><figcaption><small></small></figcaption></figure><p>The p-values associated with the coefficient estimates for the two dummy variables are very large, suggesting no statistical evidence of a real difference in credit card balance between the ethnicities</p><blockquote><p>The coefficients and their p-values do depend on the choice of dummy variable coding</p></blockquote><p>Rather than rely on the individual coefficients, we can use an <strong>F-test</strong> to test H0 : β1 = β2 = 0; this does not depend on the coding.</p><p>This F-test has a p-value of 0.96, indicating that we cannot reject the null hypothesis that there is no relationship between balance and ethnicity.</p><h1 id=extensions-of-the-linear-model>Extensions of the Linear Model<a hidden class=anchor aria-hidden=true href=#extensions-of-the-linear-model>#</a></h1><p>Two of the most important assumptions state that the relationship between the predictors and response are <strong>additive</strong> and <strong>linear</strong>.</p><ul><li><strong>Additive</strong>: the effect of changes in a predictor $X_j$ on the response $Y$ is independent of the values of the other predictors</li><li><strong>Linear</strong>: the change in the response $Y$ due to a one-unit change in $X_j$ is constant, regardless of the value of $X_j$</li></ul><h2 id=removing-the-additive-assumption>Removing the Additive Assumption<a hidden class=anchor aria-hidden=true href=#removing-the-additive-assumption>#</a></h2><p>Consider the standard linear regression model with two variables,
$$
\begin{align}
Y = β_0 + β_1X_1 + β_2X_2 + \epsilon
\end{align}
$$</p><p>One way of extending this model to allow for interaction effects is to include a third predictor, called an
<strong>interaction term</strong>:
$$
\begin{align}
Y = β_0 + β_1X_1 + β_2X_2 + β_3X_1X_2 + \epsilon
\end{align}
$$
<strong>How does inclusion of this interaction term relax the additive assumption?</strong></p><p>The model above could be written as:
$$
\begin{align}
Y &= β_0 + (β_1+β_3X_2)X_1 + β_2X_2 + \epsilon \\
&= β_0 + \tilde{β}_1X_1 + β_2X_2 + \epsilon
\end{align}
$$
Since $\tilde{β}_1$ changes with $X_2$, the effect of $X_1$ on $Y$ is no longer constant: adjusting $X_2$ will change the impact of $X_1$ on $Y$.</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/14_hu261f34fe8d826f41e0f94c3359d32e14_109693_600x0_resize_box_3.png width=600 height=186><figcaption><small></small></figcaption></figure><ul><li>Sometimes the case that an interaction term has a very small p-value, but the associated main effects (in this case, TV and radio) do not.</li><li>The <strong>hierarchical principle</strong> states that if we include an interaction in a model, we should also include the <strong>main effects</strong>, even if the p-values associated with
their coefficients are not significant. (If the interaction between X1 and X2 seems important, we should include both X1 and X2 in the model even if their coefficient estimates have large p-values)</li></ul><p><strong>Concept of interactions applies on qualitative variables</strong><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/15_huac920a950ca9c94a669725d4e02691a9_66917_600x0_resize_box_3.png width=600 height=124><figcaption><small></small></figcaption></figure></p><p>Adding an interaction variable, model now becomes:<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/17_hu8311a094f77dc7c2c4e3fdc2bfa21bf3_64264_600x0_resize_box_3.png width=600 height=121><figcaption><small></small></figcaption></figure><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/16_hu040bc07177e688a83502536028bd6ed9_161949_600x0_resize_box_3.png width=600 height=340><figcaption><small></small></figcaption></figure></p><h2 id=non-linear-relationships>Non-linear Relationships<a hidden class=anchor aria-hidden=true href=#non-linear-relationships>#</a></h2><p>Extending the linear model
to accommodate non-linear relationships is known as <strong>polynomial regression</strong>,
since we have included <strong>polynomial functions</strong> of the predictors in the
regression model</p><h1 id=potential-problems>Potential Problems<a hidden class=anchor aria-hidden=true href=#potential-problems>#</a></h1><h2 id=non-linearity-of-the-data>Non-linearity of the Data<a hidden class=anchor aria-hidden=true href=#non-linearity-of-the-data>#</a></h2><p><strong>Assumption</strong>: The linear regression model assumes that there is a straight-line relationship
between the predictors and the response.</p><p><strong>Residual plots</strong>: graphical tool for identifying non-linearity</p><ul><li>Given a simple linear regression model, we can plot the residuals,$e_i = y_i-\hat{y_i}$ versus the predictor $x_i$, or $\hat{y_i}$ when there are multiple predictors</li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/18_hu1d6561f500ae721a7ec01a6f000b787f_275828_600x0_resize_box_3.png width=600 height=417><figcaption><small></small></figcaption></figure><ul><li>Ideally, the residual plot will show no discernible pattern.</li><li>If the residual plot indicates non-linear associations in the
data, then a simple approach is to use <strong>non-linear transformations</strong> of the
predictors, such as $\log{X},\sqrt{X}, X^2$, in the regression model.</li></ul><h2 id=correlation-of-error-terms>Correlation of Error Terms<a hidden class=anchor aria-hidden=true href=#correlation-of-error-terms>#</a></h2><p><strong>Assumption</strong>: The error terms, $\epsilon_1,\epsilon_2,&mldr;,\epsilon_n$, are uncorrelated.</p><ul><li>If the errors are uncorrelated, then the fact that i is positive provides little or no information about the sign of i+1.</li><li>If the error terms are correlated, we may have an unwarranted sense of confidence in our model.</li><li><strong>estimated standard errors</strong> will underestimate the true standard errors.</li><li><strong>confidence and prediction intervals</strong> will be narrower than they should be. For example, a 95% confidence interval may in reality have a much lower probability than 0.95 of containing the true value of the parameter.</li><li><strong>p-values</strong> will be lower than they should be</li><li>Lead to erroneously conclude that a parameter is statistically significant.</li></ul><p><strong>Why might correlations among the error terms occur?</strong></p><ul><li>Such correlations
frequently occur in the context of <strong>time series</strong> data</li><li>In many cases, observations that are obtained at adjacent time points will
have <strong>positively correlated errors</strong>.</li><li>Plot the residuals from our model as a function of time to identify this correlation.</li><li><strong>Tracking</strong>: If the error terms are positively correlated, then
we may see <strong>tracking</strong> in the residuals—that is, adjacent residuals may have similar values.<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/19_hua06852bdb939fc8b72c96de7080965a3_222927_600x0_resize_box_3.png width=600 height=551><figcaption><small></small></figcaption></figure></li></ul><h2 id=non-constant-variance-of-error-terms>Non-constant Variance of Error Terms<a hidden class=anchor aria-hidden=true href=#non-constant-variance-of-error-terms>#</a></h2><p><strong>Assumption</strong>: the error terms have a constant variance, $Var(\epsilon_i) = σ^2$.</p><ul><li>The standard errors,
confidence intervals, and hypothesis tests associated with the linear model
rely upon this assumption.</li></ul><p>The variances of the error terms are non-constant.</p><ul><li>For instance, the variances of the error terms may increase with the value of the response.</li><li>One can identify non-constant variances in
the errors, or <strong>heteroscedasticity</strong>异方差性,from the presence of a funnel shape in residual plot.</li><li><strong>Solution</strong>: transform the response Y using a concave function such as $\log{Y}$ or $\sqrt{Y}$ . Such
a transformation results in a greater amount of shrinkage of the larger responses,
leading to a reduction in <strong>heteroscedasticity</strong>.</li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/20_hu347d5a1349247ca52c7579c8d9621327_352067_600x0_resize_box_3.png width=600 height=378><figcaption><small></small></figcaption></figure><h2 id=outliers>Outliers<a hidden class=anchor aria-hidden=true href=#outliers>#</a></h2><p><strong>Outlier</strong>: is a point for which $y_i$ is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.</p><p><strong>Problems of Outlier</strong>:</p><ul><li>Effect on the least squares fit,</li><li>Effect on interpretation of the fit</li><li>For instance, in this example, the RSE is 1.09 when the
outlier is included in the regression, but it is only 0.77 when the outlier
is removed.</li></ul><p><strong>Residual Plots</strong> can be used to identify outliers</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/21_hu2709d549fdaf40e6a6a90c05fd975c6a_267150_600x0_resize_box_3.png width=600 height=274><figcaption><small></small></figcaption></figure><ul><li>Difficult to decide how large a residual
needs to be</li></ul><p><strong>Studentized residuals</strong>: computed by dividing each residual $e_i$ by its estimated standard error.</p><ul><li>Observations whose studentized residuals are greater than 3 in abso- residual
lute value are possible outliers.</li></ul><h2 id=high-leverage-points>High Leverage Points<a hidden class=anchor aria-hidden=true href=#high-leverage-points>#</a></h2><p><strong>High Leverage</strong>:Observations with high leverage have an unusual value for xi</p><ul><li>removing the high leverage observation has a much more substantial impact on the least squares line
than removing the outlier.<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/23_hu134e44adbeedd124c793270a9c2e1dcc_303720_600x0_resize_box_3.png width=600 height=287><figcaption><small></small></figcaption></figure></li></ul><p><strong>Leverage Statistic</strong>: quantify an observation’s leverage</p><p>For a simple linear regression</p><p>$$
\begin{align}
h_i=\frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum_{i^{&rsquo;}=1}^n (x_{i^{&rsquo;}}-\bar{x})^2}
\end{align}
$$</p><ul><li>$h_i$ increases with the distance of $x_i$ from $\bar{x}$.</li><li>$h_i$ is always between 1/n and 1, and the <strong>average leverage</strong> for all the observations is
always equal to $(p+1)/n$.</li><li><strong>High leverage</strong>: a leverage statistic that greatly exceeds $(p+1)/n$, high leverage.</li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/22_hu29af4331ead678df2c54e0ed05473885_253929_600x0_resize_box_3.png width=600 height=342><figcaption><small></small></figcaption></figure><p>The right-hand panel of Figure 3.13 provides a plot of the studentized
residuals versus $h_i$ for the data in the left-hand panel of Figure 3.13. Observation
41 stands out as having a very high leverage statistic as well as a
high studentized residual. In other words, it is an outlier as well as a high
leverage observation.</p><h2 id=collinearity>Collinearity<a hidden class=anchor aria-hidden=true href=#collinearity>#</a></h2><p>Collinearity: situation in which two or more predictor variables are closely related to one another.</p><p><strong>Problems of Collinearity</strong></p><ul><li>Difficult to separate out the individual effects of collinear variables on the response</li><li>Uncertainty in the coefficient estimates.</li><li>Causes the standard error for $\hat{β_j}$ to grow</li><li>Recall that the t-statistic for each predictor is calculated by dividing $\hat{β_j}$ by its standard error. Consequently, collinearity results in a decline in the t-statistic. As a
result, in the presence of collinearity, we may fail to reject H0 : βj = 0. This
means that the <strong>power</strong> of the hypothesis test—the probability of correctly
detecting a non-zero coefficient—is reduced by collinearity.</li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-potential_problems/24_hu2720afa9b1306f20627fb01ff4fe424e_369558_600x0_resize_box_3.png width=600 height=422><figcaption><small></small></figcaption></figure><p><strong>Detection of Collinearity</strong></p><ul><li><strong>Correlation matrix</strong>
of the predictors.<ul><li>An element of this matrix that is large in absolute value indicates a pair of highly correlated variables.</li><li><strong>Situation Multicollinearity</strong>: it is possible for collinearity
to exist between three or more variables even if no pair of variables
has a particularly high correlation</li></ul></li><li><strong>Variance Inflation Factor (VIF)</strong><ul><li>The ratio of the variance of $\hat{β_j}$ when fitting the full model divided by the
variance of $\hat{β_j}$ if fit on its own. The smallest possible value for VIF is 1,
which indicates the complete absence of collinearity.</li><li>A VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.</li></ul></li></ul><p><strong>Solution of Collinearity</strong></p><ul><li>Drop one of the problematic variables from the regression.</li><li>Combine the collinear variables together into a single predicto<ul><li>E.g.: take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness</li></ul></li></ul><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &ldquo;The elements of statistical learning: data mining, inference and prediction.&rdquo; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://nancyyanyu.github.io/tags/linear-regression/>Linear Regression</a></li><li><a href=https://nancyyanyu.github.io/tags/regression/>Regression</a></li></ul><nav class=paginav><a class=prev href=https://nancyyanyu.github.io/posts/ml-logistic_regression/><span class=title>« Prev</span><br><span>Study Note: Logistic Regression</span></a>
<a class=next href=https://nancyyanyu.github.io/posts/apache_spark-advanced_topics/><span class=title>Next »</span><br><span>Apache Spark: Advanced Topics</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Regression Part II - Potential Problems on twitter" href="https://twitter.com/intent/tweet/?text=Study%20Note%3a%20Linear%20Regression%20Part%20II%20-%20Potential%20Problems&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-potential_problems%2f&amp;hashtags=LinearRegression%2cRegression"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Regression Part II - Potential Problems on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-potential_problems%2f&amp;title=Study%20Note%3a%20Linear%20Regression%20Part%20II%20-%20Potential%20Problems&amp;summary=Study%20Note%3a%20Linear%20Regression%20Part%20II%20-%20Potential%20Problems&amp;source=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-potential_problems%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Regression Part II - Potential Problems on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-potential_problems%2f&title=Study%20Note%3a%20Linear%20Regression%20Part%20II%20-%20Potential%20Problems"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Regression Part II - Potential Problems on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-potential_problems%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Regression Part II - Potential Problems on whatsapp" href="https://api.whatsapp.com/send?text=Study%20Note%3a%20Linear%20Regression%20Part%20II%20-%20Potential%20Problems%20-%20https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-potential_problems%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Regression Part II - Potential Problems on telegram" href="https://telegram.me/share/url?text=Study%20Note%3a%20Linear%20Regression%20Part%20II%20-%20Potential%20Problems&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-potential_problems%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Linear Regression Part II - Potential Problems on ycombinator" href="https://news.ycombinator.com/submitlink?t=Study%20Note%3a%20Linear%20Regression%20Part%20II%20-%20Potential%20Problems&u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-potential_problems%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://nancyyanyu.github.io/>Nancy's Notebook</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>