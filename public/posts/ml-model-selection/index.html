<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Study Note: Model Selection and Regularization (Ridge & Lasso) | Nancy's Notebook</title><meta name=keywords content="Model Selection"><meta name=description content="
Subset Selection/Adjusted $R^2$/Ridge/Lasso/SVD
"><meta name=author content="Me"><link rel=canonical href=https://nancyyanyu.github.io/posts/ml-model-selection/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!0},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script type=module>
    import renderMathInElement from "https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.mjs";
    renderMathInElement(document.body);
</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Study Note: Model Selection and Regularization (Ridge & Lasso)"><meta property="og:description" content="
Subset Selection/Adjusted $R^2$/Ridge/Lasso/SVD
"><meta property="og:type" content="article"><meta property="og:url" content="https://nancyyanyu.github.io/posts/ml-model-selection/"><meta property="og:image" content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-06-11T00:00:14+00:00"><meta property="article:modified_time" content="2019-06-11T00:00:14+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Study Note: Model Selection and Regularization (Ridge & Lasso)"><meta name=twitter:description content="
Subset Selection/Adjusted $R^2$/Ridge/Lasso/SVD
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nancyyanyu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Study Note: Model Selection and Regularization (Ridge \u0026 Lasso)","item":"https://nancyyanyu.github.io/posts/ml-model-selection/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Study Note: Model Selection and Regularization (Ridge \u0026 Lasso)","name":"Study Note: Model Selection and Regularization (Ridge \u0026 Lasso)","description":" Subset Selection/Adjusted $R^2$/Ridge/Lasso/SVD\n","keywords":["Model Selection"],"articleBody":" Subset Selection/Adjusted $R^2$/Ridge/Lasso/SVD\nIntroduction to Model Selection Setting:\nIn the regression setting, the standard linear model $Y = β_0 + β_1X_1 + · · · + β_pX_p + \\epsilon$\nIn the chapters that follow, we consider some approaches for extending the linear model framework.\nReason of using other fitting procedure than lease squares:\nPrediction Accuracy:\nProvided that the true relationship between the response and the predictors is approximately linear, the least squares estimates will have low bias. If n $\\gg$ p, least squares estimates tend to also have low variance $\\Rightarrow$ perform well on test data. If n is not much larger than p, least squares fit has large variance $\\Rightarrow$ overfitting $\\Rightarrow$ consequently poor predictions on test data If p \u003e n, no more unique least squares coefficient estimate: the variance is infinite so the method cannot be used at all By constraining or shrinking the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias.\nModel Interpretability：\nirrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables—that is, by setting the corresponding coefficient estimates to zero—we can obtain a model that is more easily interpreted. least squares is extremely unlikely to yield any coefficient estimates that are exactly zero $\\Rightarrow$ feature selection Alternatives of lease squares:\nSubset Selection Shrinkage Dimension Reduction Subset Selection Drawbacks of least squares estimates: prediction accuracy: the least squares estimates often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coeﬃcients to zero.By doing so we sacriﬁce a little bit of bias to reduce the variance of the predicted values, and hence may improve the overall prediction accuracy. interpretation: With a large number of predictors, we often would like to determine a smaller subset that exhibit the strongest eﬀects. In order to get the “big picture,” we are willing to sacriﬁce some of the small details. Best Subset Selection Best subset regression ﬁnds for each k ∈{0, 1, 2,…,p} the subset of size k that gives smallest residual sum of squares.\nWe choose the smallest model that minimizes an estimate of the expected prediction error. FIGURE 3.5.All possible subset models for the prostate cancer example. At each subset size is shown the residual sum-of-squares for each model of that size.\nApproach\nfit a separate least squares regression best subset for each possible combination of the p predictors. That is, we fit all p models selection that contain exactly one predictor, all $\\left(\\begin{array}{c}p\\ 2\\end{array}\\right)= p(p−1)/2$ models that contain exactly two predictors, and so forth. We then look at all of the resulting models, with the goal of identifying the one that is best. Note\n$RSS$ of these p + 1 models decreases monotonically, and the $R2$ increases monotonically, as the number of features included in the models increases. Therefore, if we use these statistics to select the best model, then we will always end up with a model involving all of the variables The problem of selecting the best model from among the $2^p$ possibilities considered by best subset selection is not trivial. Stepwise Selection Rather than search through all possible subsets (which becomes infeasible for p much larger than 40), we can seek a good path through them.\nForward Stepwise Selection Forward-stepwise selection starts with the intercept, and then sequentially adds into the model the predictor that most improves the ﬁt.\nForward-stepwise selection is a greedy algorithm, producing a nested sequence of models. In this sense it might seem sub-optimal compared to best-subset selection.\nAdvantages: Computational: for large p we cannot compute the best subset sequence, but we can always compute the forward stepwise sequence Statistical: a price is paid in variance for selecting the best subset of each size; forward stepwise is a more constrained search, and will have lower variance, but perhaps more bias Approach\nForward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model. Forward Stepwise Selection V.S. Best Subset Selection\nForward stepwise selection’s computational advantage over best subset selection is clear. Forward stepwise is not guaranteed to find the best possible model out of all $2^p$ models containing subsets of the p predictors. Backward Stepwise Selection Backward-stepwise selection starts with the full model, and sequentially deletes the predictor that has the least impact on the ﬁt. The candidate for dropping is the variable with the smallest Z-score\nApproach\nBackward Stepwise Selection begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time Backward Stepwise Selection V.S. Forward Stepwise Selection:\nLike forward stepwise selection, the backward selection approach searches through only 1+p(p+1)/2 models, and so can be applied in settings where p is too large to apply best subset selection. Like forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the p predictors. Backward selection requires that the number of samples n is larger than the number of variables p (so that the full model can be fit). In contrast, forward stepwise can be used even when n \u003c p, and so is the only viable subset method when p is very large. Hybrid Approaches Approach\nVariables are added to the model sequentially, in analogy to forward selection. However, after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model fit. Note\nSuch an approach attempts to more closely mimic best subset selection while retaining the computational advantages of forward and backward stepwise selection.\nChoosing the Optimal Model The training error can be a poor estimate of the test error. Therefore, RSS and R2 are not suitable for selecting the best model among a collection of models with different numbers of predictors.\n2 Methods:\nindirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting. directly estimate the test error, using either a validation set approach or a cross-validation approach $C_p$, $AIC$, $BIC$, Adjusted $R^2$ the training set MSE is generally an underestimate of the test MSE. (Recall that MSE = RSS/n.) the training error will decrease as more variables are included in the model, but the test error may not. Therefore, training set RSS and training set R2 cannot be used to select from among a set of models with different numbers of variables. $C_p$ $C_p$ estimate of test MSE: $$ \\begin{align} C_p=\\frac{1}{n}(RSS+2d\\hat{\\sigma}^2) \\end{align} $$ where $\\hat{\\sigma}^2$ is an estimate of the variance of the error $\\epsilon$\nNote:\nThe $C_p$ statistic adds a penalty of $2d\\hat{\\sigma}^2$ to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error. The penalty increases as the number of predictors in the model increases; this is intended to adjust for the corresponding decrease in training RSS. If $\\hat{\\sigma}^2$ is an unbiased estimate of $\\sigma^2$ in, then $C_p$ is an unbiased estimate of test MSE When determining which of a set of models is best, we choose the model with the lowest $C_p$ value. AIC The AIC criterion is defined for a large class of models fit by maximum likelihood. In the case of the model $Y = β_0 + β_1X_1 + · · · + β_pX_p + \\epsilon$ with Gaussian errors, maximum likelihood and least squares are the same thing.\nIn this case AIC is given by $$ \\begin{align} AIC=\\frac{1}{n\\hat{\\sigma}^2}(RSS+2d\\hat{\\sigma}^2) \\end{align} $$ For least squares models, Cp and AIC are proportional to each other\nBIC For the least squares model with d predictors, the BIC is, up to irrelevant constants, given by $$ \\begin{align} BIC=\\frac{1}{n}(RSS+\\log(n)d\\hat{\\sigma}^2) \\end{align} $$ Since log(n) \u003e 2 for any n \u003e 7, the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than Cp.\nAdjusted $R^2$ Recall: $$ \\begin{align} R^2=1 − RSS/TSS=1-\\frac{RSS}{\\sum(y_i-\\bar{y})^2} \\end{align} $$ TSS: total sum of squares for the response\nFor a least squares model with d variables, the adjusted R2 statistic is calculated as $$ Adjusted , R^2=1 − \\frac{RSS/(n-d-1)}{TSS/(n-1)} $$ Note:\na large value of adjusted R2 indicates a model with a small test error. Maximizing the adjusted R2 is equivalent to minimizing $RSS/(n−d−1)$ $RSS/(n−d−1)$ may increase or decrease, due to the presence of d in the denominator. Intuition:\nonce all of the correct variables have been included in the model, adding additional noise variables will lead to only a very small decrease in RSS Unlike the R2 statistic, the adjusted R2 statistic pays a price for the inclusion of unnecessary variables in the model Validation and Cross-Validation As an alternative to the approaches just discussed, we can compute the validation set error or the cross-validation error for each model under consideration, and then select the model for which the resulting estimated test error is smallest.\nAdvantage over $C_p, AIC, BIC$:\nDirect estimate of the test error, and makes fewer assumptions about the true underlying model. Used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom or hard to estimate the error variance σ2. One-standard-error rule: We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.\nRationale: if a set of models appear to be more or less equally good, then we might as well choose the simplest model Shrinage Methods Ridge Regression Ridge regression shrinks the regression coeﬃcients by imposing a penalty on their size.The ridge coeﬃcients minimize a penalized residual sum of squares: $$ \\begin{align} \\hat{\\beta}^{ridge}=argmin_\\beta {\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j)^2+\\lambda\\sum_{j=1}^p\\beta_j^2} \\end{align} $$\nλ ≥ 0 is a complexity parameter that controls the amount of shrinkage Writing the criterion in matrix form: $$ \\begin{align} RSS(\\lambda)=(\\mathbf{y}-\\mathbf{X}\\beta)^T(\\mathbf{y}-\\mathbf{X}\\beta)+\\lambda\\beta^T\\beta \\end{align} $$ The ridge regression solutions: $$ \\begin{align} \\hat{\\beta}^{ridge}=(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y} \\end{align} $$\n$\\mathbf{I}$ is the p×p identity matrix Note:\nthe ridge regression solution is again a linear function of $\\mathbf{y}$; The solution adds a positive constant to the diagonal of $\\mathbf{X}^T\\mathbf{X}$ before inversion, which makes the problem nonsingular. Recall least squares: $$ \\begin{align} RSS=\\sum_{i=1}^n\\left( y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j \\right)^2 \\end{align} $$ Ridge regression coefficient estimates $\\hat{\\beta}^R$ are the values that minimize $$ \\begin{align} \\sum_{i=1}^n\\left( y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j \\right)^2+\\lambda\\sum_{j=1}^p\\beta_j^2=RSS+\\lambda\\sum_{j=1}^p\\beta_j^2 \\end{align} $$\nTrade-off:\nRidge regression seeks coefficient estimates that fit the data well, by making the RSS small. shrinkage penalty $\\lambda\\sum_{j=1}^p\\beta_j^2$ is small when β1, . . . , βp are close to zero, and so it has the effect of shrinking the estimates of βj towards zero Standardization:\nscale equivariant: The standard least squares coefficient estimates are scale equivariant: multiplying Xj by a constant c simply leads to a scaling of the least squares coefficient estimates by a factor of 1/c.\n$X_{j,\\lambda}^\\beta$ will depend not only on the value of λ, but also on the scaling of the jth predictor, and the scaling of the other predictors. It is best to apply ridge regression after standardizing the predictors $$ \\begin{align} \\tilde{x}{ij}=\\frac{x{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}} \\end{align} $$ The denominator is the estimated standard deviation of the jth predictor\nRidge Regression Improves Over Least Squares bias-variance trade-off Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. At the least squares coefficient estimates, which correspond to ridge regression with λ = 0, the variance is high but there is no bias. But as λ increases, the shrinkage of the ridge coefficient estimates leads to a substantial reduction in the variance of the predictions, at the expense of a slight increase in bias. Ridge regression works best in situations where the least squares estimates have high variance\ncomputational advantages over best subset selection Singular value decomposition (SVD) The singular value decomposition (SVD) of the centered input matrix X gives us some additional insight into the nature of ridge regression. The SVD of the N × p matrix X has the form: $$ \\begin{align} X=UDV^T \\end{align} $$\nU: N×p orthogonal matrices, with the columns of U spanning the column space of X V: p×p orthogonal matrices, the columns of V spanning the row space of X D: p×p diagonal matrix, with diagonal entries d1 ≥ d2 ≥···≥ dp ≥ 0 called the singular values of X. If one or more values dj =0,X is singular least squares ﬁtted vector: $$ \\begin{align} \\mathbf{X}\\hat{\\beta}^{ls}\u0026=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\ \u0026=UDV^T (VD^TU^TUDV^T)^{-1}VD^TU^Ty \\\\ \u0026=UDV^T (VD^TDV^T)^{-1}VD^TU^Ty \\\\ \u0026=UDV^T (V^T)^{-1}D^{-1}(D^T)^{-1}V^{-1}VD^TU^Ty \\\\ \u0026=\\mathbf{U}\\mathbf{U}^T\\mathbf{y} \\end{align} $$ Note: $\\mathbf{U}^T\\mathbf{y}$ are the coordinates of y with respect to the orthonormal basis U.\nThe ridge solutions: $$ \\begin{align} \\mathbf{X}\\hat{\\beta}^{ridge}\u0026=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\ \u0026=UD(D^2+\\lambda\\mathbf{I})^{-1}D^TU^Ty \\\\ \u0026=\\sum_{j=1}^p\\mathbf{u}_j\\frac{d^2_j}{d^2_j+\\lambda}\\mathbf{u}^T_j\\mathbf{y} \\end{align} $$\n$\\mathbf{u}_j$ are the columns of U Note: ridge regression computes the coordinates of y with respect to the orthonormal basis U. It then shrinks these coordinates by the factors $\\frac{d^2_j}{d^2_j+\\lambda}$\nWhat does a small value of $d^2_j$ mean? The SVD of the centered matrix X is another way of expressing the principal components of the variables in X. The sample covariance matrix is given by $S=X^TX/N$, we have\nEigen decomposition of $X^TX$: $$ \\begin{align} \\mathbf{X}^T\\mathbf{X}=VD^TU^TUDV^T=VD^2V^T \\end{align} $$ The eigenvectors $v_j$ (columns of V) are also called the principal components (or Karhunen–Loeve) directions of X. The ﬁrst principal component direction $v_1$ has the property that $z_1=Xv_1$ has the largest sample variance amongst all normalized linear combinations of the columns of X, which is: $$ \\begin{align} Var(z_1)=Var(Xv_1)=\\frac{d^2_1}{N} \\end{align} $$ and in fact $z_1=Xv_1=u_1d_1$. The derived variable $z_1$ is called the ﬁrst principal component of X, and hence $u_1$ is the normalized ﬁrst principal component.Subsequent principal components $z_j$ have maximum variance $\\frac{d^2_j}{N}$, subject to being orthogonal to the earlier ones.\nHence the small singular values $d_j$ correspond to directions in the column space of X having small variance, and ridge regression shrinks these directions the most.\nEﬀective degrees of freedom $$ \\begin{align} df(\\lambda)\u0026=tr[\\mathbf{X}(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T] \\\\ \u0026=tr[\\mathbf{H}\\lambda] \\\\ \u0026=\\sum^p_{j=1}\\frac{d^2_j}{d^2_j+\\lambda} \\end{align} $$\nThis monotone decreasing function of λ is the eﬀective degrees of freedom of the ridge regression ﬁt. Usually in a linear-regression ﬁt with p variables,the degrees-of-freedom of the ﬁt is p, the number of free parameters.\nNote that\ndf(λ)= p as λ = 0 (no regularization)\ndf(λ) → 0 as λ →∞.\nThe Lasso The lasso coefficients, $\\hat{\\beta}\\lambda^L$, minimize the quantity $$ \\begin{align} \\sum{i=1}^n\\left( y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j \\right)^2+\\lambda\\sum_{j=1}^p|\\beta_j|=RSS+\\lambda\\sum_{j=1}^p|\\beta_j| \\end{align} $$\nThe lasso is a shrinkage method like ridge, with subtle but important differences.The lasso estimate is deﬁned by: $$ \\begin{align} \\hat{\\beta}^{lasso}\u0026=argmin_\\beta\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j)^2 \\\\ \u0026 s.t. \\sum_{j=1}^p|\\beta_j|\\leq t \\end{align} $$ Lasso problem in Lagrangian form: $$ \\begin{align} \\hat{\\beta}^{lasso}\u0026=argmin_\\beta ( \\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j)^2+\\lambda\\sum_{j=1}^p|\\beta_j| ) \\end{align} $$\nAnother Formulation for Ridge Regression and the Lasso The lasso and ridge regression coefficient estimates solve the problems $$ \\begin{align} \\min_\\beta \\left(\\sum_{i=1}^n\\left( y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j \\right)^2\\right),, subject, to , \\sum_{j=1}^p|\\beta_j|\\leq s \\\\ \\min_\\beta \\left(\\sum_{i=1}^n\\left( y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j \\right)^2\\right),, subject, to , \\sum_{j=1}^p\\beta_j^2\\leq s \\end{align} $$ When we perform the lasso we are trying to find the set of coefficient estimates that lead to the smallest RSS, subject to the constraint that there is a budget s for how large $\\sum_{j=1}^p|\\beta_j|$ can be. When s is extremely large, then this budget is not very restrictive, and so the coefficient estimates can be large\nA close connection between the lasso, ridge regression, and best subset selection:\nbest subset selection is equivelant to : $$ \\begin{align} \\min_{\\beta} \\left(\\sum_{i=1}^n\\left( y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j \\right)^2\\right),, subject, to , \\sum_{j=1}^pI(\\beta_j\\neq 0)\\leq s \\end{align} $$ Therefore, we can interpret ridge regression and the lasso as computationally feasible alternatives to best subset selection.\nThe Variable Selection Property of the Lasso The lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region.\nridge regression: circular constraint with no sharp points, so the ridge regression coefficient estimates will be exclusively non-zero.\nthe lasso: constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis.\nthe $l_1$ penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large. Hence, much like best subset selection, the lasso performs variable selection lasso yields sparse models\nComparing the Lasso and Ridge Regression SAME: Ridge \u0026 Lasso all can yield a reduction in variance at the expense of a small increase in bias, and consequently can generate more accurate predictions.\nDIFFERENCES:\nUnlike ridge regression, the lasso performs variable selection, and hence results in models that are easier to interpret. Ridge regression outperforms the lasso in terms of prediction error in this setting Suitable setting:\nLasso: perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression: perform better when the response is a function of many predictors, all with coefficients of roughly equal size. The number of predictors that is related to the response is never known a priori for real data sets. Cross-validation can be used in order to determine which approach is better on a particular data set. The L2 ridge penalty $\\sum_{j=1}^p\\beta_j^2$ is replaced by the L1 lasso penalty $\\sum_{j=1}^p|\\beta_j|$. This latter constraint makes the solutions nonlinear in the $y_i$, and there is no closed form expression as in ridge regression.\nt should be adaptively chosen to minimize an estimate of expected prediction error.\nif $t\u003et_0=\\sum_{j=1}^p|\\hat{\\beta_j^{ls}}|$, then the lasso estimates are the $\\hat{\\beta_j^{ls}}$ if $t\u003et_0/2$, the least squares coeﬃcients are shrunk by about 50% on average The standardized parameter: $s=t/\\sum_1^p|\\hat{\\beta_j}|$\ns=1.0, the lasso coeﬃcients are the least squares estimates s-\u003e0, as the lasso coeﬃcients -\u003e0 Shrinkage Methods v.s. Subset Selection: Subset selection described involve using least squares to fit a linear model that contains a subset of the predictors. are discrete process—variables are either retained or discarded—it often exhibits high variance,and so doesn’t reduce the prediction error of the full model. Shrinkage Methods fit a model containing all p predictors by constraining or regularizing the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero. are more continuous, and don’t suﬀer as much from high variability. Discussion: Subset Selection, Ridge Regression and the Lasso Ridge regression: does a proportional shrinkage Lasso: translates each coeﬃcient by a constant factor λ, truncating at zero –“soft thresholding,” Best-subset selection: drops all variables with coeﬃcients smaller than the Mth largest –“hard-thresholding.” Bayes View Consider the criterion $$ \\begin{align} \\tilde{\\beta}\u0026=argmin_\\beta ( \\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j)^2+\\lambda\\sum_{j=1}^p|\\beta_j|^q ) \\end{align} $$ for q ≥ 0. The contours of constant value of $\\sum_{j=1}^p|\\beta_j|^q$ are shown in Figure 3.12, for the case of two inputs.\nThe lasso, ridge regression and best subset selection are Bayes estimates with diﬀerent priors:Thinking of $\\sum_{j=1}^p|\\beta_j|^q$ as the log-prior density for βj , these are also the equi-contours of the prior distribution of the parameters.\nq = 0 :variable subset selection, as the penalty simply counts the number of nonzero parameters; q = 1 :the lasso, also Laplace distribution for each input, with density $\\frac{1}{2\\tau}exp(-|\\beta|/\\tau)$, where $\\tau=1/\\lambda$ q = 2 :the ridge Considerations In High Dimensions High-Dimensional Data High-dimensional: Data sets containing more features than observations are often referred to as high-dimensional.\nClassical approaches such as least squares linear regression are not appropriate in this setting What Goes Wrong in High Dimensions? When the number of features p is as large as, or \u003en, least squares cannot be performed. Reason: regardless of whether or not there truly is a relationship between the features and the response, least squares will yield a set of coefficient estimates that result in a perfect fit to the data, such that the residuals are zero.\nThis perfect fit will almost certainly lead to overfitting of the data The problem is simple: when p \u003e n or p ≈ n, a simple least squares regression line is too flexible and hence overfits the data. Examines only the R2 or the training set MSE might erroneously conclude that the model with the greatest number of variables is best. Cp, AIC, and BIC approaches are not appropriate in the high-dimensional setting, because estimating $\\hat{σ}^2$ is problematic.(For instance, the formula for $\\hat{σ}^2$ from Chapter 3 yields an estimate $\\hat{σ}^2$ = 0 in this setting.) **Adjusted $R^2$ ** in the high-dimensional setting is problematic, since one can easily obtain a model with an adjusted $R^2$ value of 1. Regression in High Dimensions Alternative approaches better-suited to the high-dimensional setting:\nForward stepwise selection Ridge regression The lasso Principal components regression Reason: These approaches avoid overfitting by using a less flexible fitting approach than least squares.\nThree important points: (1) Regularization or shrinkage plays a key role in high-dimensional problems,\n(2) Appropriate tuning parameter selection is crucial for good predictive performance, and\n(3) The test error tends to increase as the dimensionality of the problem (i.e. the number of features or predictors) increases, unless the additional features are truly associated with the response.$\\Rightarrow$ curse of dimensionality\nCurse of dimensionality Adding additional signal features that are truly associated with the response will improve the fitted model; However, adding noise features that are not truly associated with the response will lead to a deterioration in the fitted model.\nReason: This is because noise features increase the dimensionality of the problem, exacerbating the risk of overfitting (since noise features may be assigned nonzero coefficients due to chance associations with the response on the training set) without any potential upside in terms of improved test set error.\nInterpreting Results in High Dimensions Be cautious in reporting the results obtained when we perform the lasso, ridge regression, or other regression procedures in the high-dimensional setting. In the high-dimensional setting, the multicollinearity problem is extreme: any variable in the model can be written as a linear combination of all of the other variables in the model. This means that we can never know exactly which variables (if any) truly are predictive of the outcome, and we can never identify the best coefficients for use in the regression. Be cautious in reporting errors and measures of model fit in the high-dimensional setting e.g.: when p \u003e n, it is easy to obtain a useless model that has zero residuals. One should never use sum of squared errors, p-values, R2 statistics, or other traditional measures of model fit on the training data as evidence of a good model fit in the high-dimensional setting It is important to instead report results on an independent test set, or cross-validation errors. For instance, the MSE or R2 on an independent test set is a valid measure of model fit, but the MSE on the training set certainly is not. Ref:\nJames, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.\nHastie, Trevor, et al. “The elements of statistical learning: data mining, inference and prediction.” The Mathematical Intelligencer 27.2 (2005): 83-85\n","wordCount":"3817","inLanguage":"en","datePublished":"2019-06-11T00:00:14Z","dateModified":"2019-06-11T00:00:14Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nancyyanyu.github.io/posts/ml-model-selection/"},"publisher":{"@type":"Organization","name":"Nancy's Notebook","logo":{"@type":"ImageObject","url":"https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!0},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script type=module>
    import renderMathInElement from "https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.mjs";
    renderMathInElement(document.body);
</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nancyyanyu.github.io/ accesskey=h title="Nancy's Notebook (Alt + H)"><img src=https://nancyyanyu.github.io/apple-touch-icon.png alt aria-label=logo height=35>Nancy's Notebook</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nancyyanyu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://nancyyanyu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://nancyyanyu.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nancyyanyu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nancyyanyu.github.io/posts/>Posts</a></div><h1 class=post-title>Study Note: Model Selection and Regularization (Ridge & Lasso)</h1><div class=post-meta><span title='2019-06-11 00:00:14 +0000 UTC'>June 11, 2019</span>&nbsp;·&nbsp;18 min&nbsp;·&nbsp;3817 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/ML-Model%20Selection/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><blockquote><p>Subset Selection/Adjusted $R^2$/Ridge/Lasso/SVD</p></blockquote><h1 id=introduction-to-model-selection>Introduction to Model Selection<a hidden class=anchor aria-hidden=true href=#introduction-to-model-selection>#</a></h1><p><strong>Setting:</strong></p><ul><li><p>In the regression setting, the standard linear model
$Y = β_0 + β_1X_1 + · · · + β_pX_p + \epsilon$</p></li><li><p>In the chapters that follow, we consider some approaches for extending
the linear model framework.</p></li></ul><p><strong>Reason of using other fitting procedure than lease squares</strong>:</p><ul><li><p><em><strong>Prediction Accuracy:</strong></em></p><ul><li>Provided that the true relationship between the response and the predictors is approximately linear, the least squares estimates will have low bias.</li><li>If n $\gg$ p, least squares estimates tend to also have low variance $\Rightarrow$ perform well on test data.</li><li>If n is not much larger than p, least squares fit has large variance $\Rightarrow$ overfitting $\Rightarrow$ consequently poor predictions on test data</li><li>If p > n, no more unique least squares coefficient estimate: the <strong>variance is infinite</strong> so the method cannot be used at all</li></ul><p>By <strong>constraining</strong> or <strong>shrinking</strong> the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias.</p></li><li><p><em><strong>Model Interpretability</strong></em>：</p><ul><li>irrelevant variables leads to unnecessary complexity in the resulting model. By removing these
variables—that is, by setting the corresponding coefficient estimates
to zero—we can obtain a model that is more easily interpreted.</li><li>least squares is extremely unlikely to yield any coefficient estimates that are exactly zero $\Rightarrow$ <strong>feature selection</strong></li></ul></li></ul><p><strong>Alternatives of lease squares:</strong></p><ol><li>Subset Selection</li><li>Shrinkage</li><li>Dimension Reduction</li></ol><h1 id=subset-selection>Subset Selection<a hidden class=anchor aria-hidden=true href=#subset-selection>#</a></h1><h4 id=drawbacks-of-least-squares-estimates>Drawbacks of least squares estimates:<a hidden class=anchor aria-hidden=true href=#drawbacks-of-least-squares-estimates>#</a></h4><ul><li><em>prediction accuracy</em>: the least squares estimates often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coeﬃcients to zero.By doing so we sacriﬁce a little bit of bias to reduce the variance of the predicted values, and hence may improve the overall prediction accuracy.</li><li><em>interpretation</em>: With a large number of predictors, we often would like to determine a smaller subset that exhibit the strongest eﬀects. In order to get the “big picture,” we are willing to sacriﬁce some of the small details.</li></ul><h2 id=best-subset-selection>Best Subset Selection<a hidden class=anchor aria-hidden=true href=#best-subset-selection>#</a></h2><p>Best subset regression ﬁnds for each k ∈{0, 1, 2,&mldr;,p} the subset of size k that gives smallest residual sum of squares.</p><p>We choose the smallest model that minimizes an estimate of the expected prediction error.<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-model-selection/best_subset_huaa57aee0d1f89df6652d69eff48cb982_13556_600x0_resize_box_3.png width=600 height=400><figcaption><small></small></figcaption></figure></p><blockquote><p>FIGURE 3.5.All possible subset models for the prostate cancer example. At each subset size is shown the residual sum-of-squares for each model of that size.</p></blockquote><p><strong>Approach</strong></p><ol><li>fit a separate least squares regression best subset for each possible combination of the p predictors. That is, we fit all p models selection that contain exactly one predictor, all $\left(\begin{array}{c}p\ 2\end{array}\right)= p(p−1)/2$ models that contain exactly two predictors, and so forth.</li><li>We then look at all of the resulting
models, with the goal of identifying the one that is best.</li></ol><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-model-selection/1_hu6239b43d2c87dcb8b790977a6d3b9972_177372_600x0_resize_box_3.png width=600 height=273><figcaption><small></small></figcaption></figure><p><strong>Note</strong></p><ul><li>$RSS$ of these p + 1 models decreases monotonically, and the $R2$ increases
monotonically, as the number of features included in the models increases.
Therefore, if we use these statistics to select the best model, then we will
always end up with a model involving all of the variables</li><li>The problem of selecting the best model from among the $2^p$ possibilities
considered by best subset selection is not trivial.</li></ul><h2 id=stepwise-selection>Stepwise Selection<a hidden class=anchor aria-hidden=true href=#stepwise-selection>#</a></h2><blockquote><p>Rather than search through all possible subsets (which becomes infeasible for p much larger than 40), we can seek a good path through them.</p></blockquote><h3 id=forward-stepwise-selection>Forward Stepwise Selection<a hidden class=anchor aria-hidden=true href=#forward-stepwise-selection>#</a></h3><p><strong>Forward-stepwise selection</strong> starts with the intercept, and then sequentially adds into the model the predictor that most improves the ﬁt.</p><p>Forward-stepwise selection is a <em>greedy algorithm</em>, producing a nested sequence of models. In this sense it might seem sub-optimal compared to best-subset selection.</p><h4 id=advantages>Advantages:<a hidden class=anchor aria-hidden=true href=#advantages>#</a></h4><ul><li><strong>Computational</strong>: for large p we cannot compute the best subset sequence, but we can always compute the forward stepwise sequence</li><li><strong>Statistical</strong>: a price is paid in variance for selecting the best subset of each size; forward stepwise is a more constrained search, and will have lower variance, but perhaps more bias</li></ul><p><strong>Approach</strong></p><ol><li><strong>Forward stepwise selection</strong> begins with a model containing no predictors, and then adds predictors
to the model, one-at-a-time, until all of the predictors are in the model.</li><li>In particular, at each step the variable that gives the greatest additional
improvement to the fit is added to the model.</li></ol><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-model-selection/12_hu114fcb30357cb88d2a79fd63dcf818c6_162276_600x0_resize_box_3.png width=600 height=385><figcaption><small></small></figcaption></figure><p><strong>Forward Stepwise Selection V.S. Best Subset Selection</strong></p><ul><li>Forward stepwise selection’s computational advantage over best subset
selection is clear.</li><li>Forward stepwise is not guaranteed to find the best possible model out of all $2^p$ models
containing subsets of the p predictors.</li></ul><h3 id=backward-stepwise-selection>Backward Stepwise Selection<a hidden class=anchor aria-hidden=true href=#backward-stepwise-selection>#</a></h3><p><strong>Backward-stepwise selection</strong> starts with the full model, and sequentially deletes the predictor that has the least impact on the ﬁt. The candidate for dropping is the variable with the smallest Z-score</p><p><strong>Approach</strong></p><ol><li><strong>Backward Stepwise Selection</strong> begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time</li></ol><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-model-selection/3_hu99320462169c7ed7dbd4fd45b7ecb2ce_175146_600x0_resize_box_3.png width=600 height=269><figcaption><small></small></figcaption></figure><p><strong>Backward Stepwise Selection V.S. Forward Stepwise Selection</strong>:</p><ul><li>Like forward stepwise selection, the backward selection approach searches
through only 1+p(p+1)/2 models, and so can be applied in settings where
p is too large to apply best subset selection.</li><li>Like forward stepwise selection, backward stepwise selection is not guaranteed to yield the best
model containing a subset of the p predictors.</li><li>Backward selection requires that the number of samples n is larger than
the number of variables p (so that the full model can be fit). In contrast,
forward stepwise can be used even when n &lt; p, and so is the only viable
subset method when p is very large.</li></ul><h3 id=hybrid-approaches>Hybrid Approaches<a hidden class=anchor aria-hidden=true href=#hybrid-approaches>#</a></h3><p><strong>Approach</strong></p><ol><li>Variables are added to the model sequentially, in analogy to forward selection.</li><li>However, after adding each new variable, the method
may also remove any variables that no longer provide an improvement in
the model fit.</li></ol><p><strong>Note</strong></p><p>Such an approach attempts to more closely mimic best subset
selection while retaining the computational advantages of forward and
backward stepwise selection.</p><h2 id=choosing-the-optimal-model>Choosing the Optimal Model<a hidden class=anchor aria-hidden=true href=#choosing-the-optimal-model>#</a></h2><p>The training error can be a poor estimate of the test error. Therefore, RSS and R2 are not suitable for selecting the best model among a collection of models with different numbers of predictors.</p><p><strong>2 Methods</strong>:</p><ol><li><em>indirectly</em> estimate test error by making an adjustment to the
training error to account for the bias due to overfitting.</li><li><em>directly</em> estimate the test error, using either a validation set
approach or a cross-validation approach</li></ol><h3 id=c_p-aic-bic-adjusted-r2>$C_p$, $AIC$, $BIC$, Adjusted $R^2$<a hidden class=anchor aria-hidden=true href=#c_p-aic-bic-adjusted-r2>#</a></h3><ul><li>the training set MSE is generally an underestimate of the test MSE. (Recall that MSE = RSS/n.)</li><li>the training error will decrease as more variables are included in the model, but the test error may not.</li><li>Therefore, training set RSS and training set R2 cannot be used to select from among a set of models with different numbers of variables.</li></ul><h4 id=c_p>$C_p$<a hidden class=anchor aria-hidden=true href=#c_p>#</a></h4><p>$C_p$ estimate of test MSE:
$$
\begin{align}
C_p=\frac{1}{n}(RSS+2d\hat{\sigma}^2)
\end{align}
$$
where $\hat{\sigma}^2$ is an estimate of the variance of the error $\epsilon$</p><p><strong>Note</strong>:</p><ul><li>The $C_p$ statistic adds a penalty of $2d\hat{\sigma}^2$ to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error.</li><li>The penalty increases as the number of predictors in the model increases; this is intended to adjust
for the corresponding decrease in training RSS.</li><li>If $\hat{\sigma}^2$ is an unbiased estimate of $\sigma^2$ in, then $C_p$ is an unbiased estimate of test MSE</li><li>When determining which of a set of models is best, we choose the model with the lowest $C_p$ value.</li></ul><h4 id=aic>AIC<a hidden class=anchor aria-hidden=true href=#aic>#</a></h4><p>The AIC criterion is defined for a large class of models fit by maximum
likelihood. In the case of the model $Y = β_0 + β_1X_1 + · · · + β_pX_p + \epsilon$ with Gaussian errors, maximum
likelihood and least squares are the same thing.</p><p>In this case AIC is given by
$$
\begin{align}
AIC=\frac{1}{n\hat{\sigma}^2}(RSS+2d\hat{\sigma}^2)
\end{align}
$$
For least
squares models, Cp and AIC are proportional to each other</p><h4 id=bic>BIC<a hidden class=anchor aria-hidden=true href=#bic>#</a></h4><p>For the least squares model with d predictors, the
BIC is, up to irrelevant constants, given by
$$
\begin{align}
BIC=\frac{1}{n}(RSS+\log(n)d\hat{\sigma}^2)
\end{align}
$$
Since log(n) > 2 for any n > 7,
the BIC statistic generally places a heavier penalty on models with many
variables, and hence results in the selection of smaller models than Cp.</p><h4 id=adjusted-r2>Adjusted $R^2$<a hidden class=anchor aria-hidden=true href=#adjusted-r2>#</a></h4><p>Recall:
$$
\begin{align}
R^2=1 − RSS/TSS=1-\frac{RSS}{\sum(y_i-\bar{y})^2}
\end{align}
$$
<strong>TSS</strong>: total sum of squares for the response</p><p>For a least squares model with d variables,
<strong>the adjusted R2</strong> statistic is calculated as
$$
Adjusted , R^2=1 − \frac{RSS/(n-d-1)}{TSS/(n-1)}
$$
<strong>Note</strong>:</p><ul><li>a large value of adjusted R2 indicates a model with a
small test error. Maximizing the adjusted R2 is equivalent to minimizing $RSS/(n−d−1)$</li><li>$RSS/(n−d−1)$ may increase or decrease, due to the presence of d in the
denominator.</li></ul><p><strong>Intuition</strong>:</p><ul><li>once all of the correct
variables have been included in the model, adding additional noise variables will lead to only a very small decrease in RSS</li><li>Unlike the R2 statistic, the adjusted R2 statistic pays
a price for the inclusion of unnecessary variables in the model</li></ul><h3 id=validation-and-cross-validation>Validation and Cross-Validation<a hidden class=anchor aria-hidden=true href=#validation-and-cross-validation>#</a></h3><p>As an alternative to the approaches just discussed, we can compute the validation set error or the
cross-validation error for each model under consideration, and then select
the model for which the resulting estimated test error is smallest.</p><p><strong>Advantage over $C_p, AIC, BIC$</strong>:</p><ul><li>Direct estimate of the test error, and makes fewer assumptions
about the true underlying model.</li><li>Used in a wider range of
model selection tasks, even in cases where it is hard to pinpoint the model
degrees of freedom or hard to
estimate the error variance σ2.</li></ul><p><strong>One-standard-error rule</strong>: We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one
standard error of the lowest point on the curve.</p><ul><li><strong>Rationale</strong>: if a set of models appear to be more or less equally good, then we might
as well choose the simplest model</li></ul><h1 id=shrinage-methods>Shrinage Methods<a hidden class=anchor aria-hidden=true href=#shrinage-methods>#</a></h1><h2 id=ridge-regression>Ridge Regression<a hidden class=anchor aria-hidden=true href=#ridge-regression>#</a></h2><p><strong>Ridge regression</strong> shrinks the regression coeﬃcients by imposing a penalty on their size.The ridge coeﬃcients minimize a penalized residual sum of squares:
$$
\begin{align}
\hat{\beta}^{ridge}=argmin_\beta {\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p\beta_j^2}
\end{align}
$$</p><ul><li>λ ≥ 0 is a complexity parameter that controls the amount of shrinkage</li></ul><p>Writing the criterion in matrix form:
$$
\begin{align}
RSS(\lambda)=(\mathbf{y}-\mathbf{X}\beta)^T(\mathbf{y}-\mathbf{X}\beta)+\lambda\beta^T\beta
\end{align}
$$
The ridge regression solutions:
$$
\begin{align}
\hat{\beta}^{ridge}=(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}
\end{align}
$$</p><ul><li>$\mathbf{I}$ is the p×p identity matrix</li></ul><p>Note:</p><ul><li>the ridge regression solution is again a linear function of $\mathbf{y}$;</li><li>The solution adds a positive constant to the diagonal of $\mathbf{X}^T\mathbf{X}$ before inversion, which makes the problem nonsingular.</li></ul><p>Recall least squares:
$$
\begin{align}
RSS=\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2
\end{align}
$$
<strong>Ridge regression</strong> coefficient estimates $\hat{\beta}^R$ are the values that minimize
$$
\begin{align}
\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2+\lambda\sum_{j=1}^p\beta_j^2=RSS+\lambda\sum_{j=1}^p\beta_j^2
\end{align}
$$</p><p><strong>Trade-off:</strong></p><ol><li>Ridge regression seeks coefficient estimates that fit the data well, by making the RSS
small.</li><li><strong>shrinkage penalty</strong> $\lambda\sum_{j=1}^p\beta_j^2$ is small when β1, . . . , βp are close to zero, and so it has the effect of shrinking the estimates of βj towards zero</li></ol><p><strong>Standardization</strong>:</p><ul><li><p><strong>scale equivariant</strong>: The standard least squares coefficient estimates are scale equivariant: multiplying Xj by a constant c simply leads to a scaling of the least squares coefficient estimates by a factor of 1/c.</p></li><li><p>$X_{j,\lambda}^\beta$ will depend not only on the value of λ, but also on the scaling of the jth predictor, and the scaling of the other predictors. It is best to apply ridge regression after
standardizing the predictors
$$
\begin{align}
\tilde{x}<em>{ij}=\frac{x</em>{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2}}
\end{align}
$$
The denominator is the
estimated standard deviation of the jth predictor</p></li></ul><h3 id=ridge-regression-improves-over-least-squares>Ridge Regression Improves Over Least Squares<a hidden class=anchor aria-hidden=true href=#ridge-regression-improves-over-least-squares>#</a></h3><ol><li><strong>bias-variance trade-off</strong></li></ol><ul><li>Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.</li><li>At the least squares coefficient estimates, which correspond to ridge regression with λ = 0, the variance is high but there is no bias. But as λ increases, the shrinkage of the ridge coefficient estimates leads to a substantial reduction in the variance of the predictions, at the expense of a slight increase in bias.<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-model-selection/4_hu8ee528d4c3a7bc21b031cd788d4ecba6_168697_600x0_resize_box_3.png width=600 height=342><figcaption><small></small></figcaption></figure></li></ul><blockquote><p>Ridge regression works best in situations where the least squares estimates have high variance</p></blockquote><ol start=2><li><strong>computational advantages over best subset selection</strong></li></ol><h3 id=singular-value-decomposition-svd>Singular value decomposition (SVD)<a hidden class=anchor aria-hidden=true href=#singular-value-decomposition-svd>#</a></h3><p>The <strong>singular value decomposition (SVD)</strong> of the centered input matrix X gives us some additional insight into the nature of ridge regression. The SVD of the N × p matrix X has the form:
$$
\begin{align}
X=UDV^T
\end{align}
$$</p><ul><li>U: N×p orthogonal matrices, with the columns of U spanning the column space of X</li><li>V: p×p orthogonal matrices, the columns of V spanning the row space of X</li><li>D: p×p diagonal matrix, with diagonal entries d1 ≥ d2 ≥···≥ dp ≥ 0 called the singular values of X. If one or more values dj =0,X is singular</li></ul><p>least squares ﬁtted vector:
$$
\begin{align}
\mathbf{X}\hat{\beta}^{ls}&=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \\
&=UDV^T (VD^TU^TUDV^T)^{-1}VD^TU^Ty \\
&=UDV^T (VD^TDV^T)^{-1}VD^TU^Ty \\
&=UDV^T (V^T)^{-1}D^{-1}(D^T)^{-1}V^{-1}VD^TU^Ty \\
&=\mathbf{U}\mathbf{U}^T\mathbf{y}
\end{align}
$$
Note: $\mathbf{U}^T\mathbf{y}$ are the coordinates of y with respect to the orthonormal basis U.</p><p>The ridge solutions:
$$
\begin{align}
\mathbf{X}\hat{\beta}^{ridge}&=\mathbf{X}(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y} \\
&=UD(D^2+\lambda\mathbf{I})^{-1}D^TU^Ty \\
&=\sum_{j=1}^p\mathbf{u}_j\frac{d^2_j}{d^2_j+\lambda}\mathbf{u}^T_j\mathbf{y}
\end{align}
$$</p><ul><li>$\mathbf{u}_j$ are the columns of U</li></ul><p>Note: ridge regression computes the coordinates of y with respect to the orthonormal basis U. It then shrinks these coordinates by the factors $\frac{d^2_j}{d^2_j+\lambda}$</p><h4 id=what-does-a-small-value-of-d2_j-mean>What does a small value of $d^2_j$ mean?<a hidden class=anchor aria-hidden=true href=#what-does-a-small-value-of-d2_j-mean>#</a></h4><p>The SVD of the centered matrix X is another way of expressing the <strong>principal components</strong> of the variables in X. The sample covariance matrix is given by $S=X^TX/N$, we have</p><p><strong>Eigen decomposition of $X^TX$:</strong>
$$
\begin{align}
\mathbf{X}^T\mathbf{X}=VD^TU^TUDV^T=VD^2V^T
\end{align}
$$
The eigenvectors $v_j$ (columns of V) are also called the <strong>principal components</strong> (or Karhunen–Loeve) directions of X.
The ﬁrst principal component direction $v_1$ has the property that $z_1=Xv_1$ has the largest sample variance amongst all normalized linear combinations of the columns of X, which is:
$$
\begin{align}
Var(z_1)=Var(Xv_1)=\frac{d^2_1}{N}
\end{align}
$$
and in fact $z_1=Xv_1=u_1d_1$. The derived variable $z_1$ is called the ﬁrst principal component of X, and hence $u_1$ is the normalized ﬁrst principal component.Subsequent principal components $z_j$ have maximum variance $\frac{d^2_j}{N}$, subject to being orthogonal to the earlier ones.</p><p>Hence the small singular values $d_j$ correspond to directions in the column space of X having small variance, and ridge regression shrinks these directions the most.</p><h3 id=eﬀective-degrees-of-freedom>Eﬀective degrees of freedom<a hidden class=anchor aria-hidden=true href=#eﬀective-degrees-of-freedom>#</a></h3><p>$$
\begin{align}
df(\lambda)&=tr[\mathbf{X}(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T] \\
&=tr[\mathbf{H}\lambda] \\
&=\sum^p_{j=1}\frac{d^2_j}{d^2_j+\lambda}
\end{align}
$$</p><p>This monotone decreasing function of λ is the eﬀective degrees of freedom of the ridge regression ﬁt.
Usually in a linear-regression ﬁt with p variables,the degrees-of-freedom of the ﬁt is p, the number of free parameters.</p><p>Note that</p><blockquote><p>df(λ)= p as λ = 0 (no regularization)</p></blockquote><blockquote><p>df(λ) → 0 as λ →∞.</p></blockquote><h2 id=the-lasso>The Lasso<a hidden class=anchor aria-hidden=true href=#the-lasso>#</a></h2><p>The lasso coefficients, $\hat{\beta}<em>\lambda^L$, minimize the quantity
$$
\begin{align}
\sum</em>{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2+\lambda\sum_{j=1}^p|\beta_j|=RSS+\lambda\sum_{j=1}^p|\beta_j|
\end{align}
$$</p><p>The lasso is a shrinkage method like ridge, with subtle but important differences.The lasso estimate is deﬁned by:
$$
\begin{align}
\hat{\beta}^{lasso}&=argmin_\beta\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2 \\
& s.t. \sum_{j=1}^p|\beta_j|\leq t
\end{align}
$$
Lasso problem in <em>Lagrangian form</em>:
$$
\begin{align}
\hat{\beta}^{lasso}&=argmin_\beta ( \sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j| )
\end{align}
$$</p><h4 id=heading><a hidden class=anchor aria-hidden=true href=#heading>#</a></h4><h3 id=another-formulation-for-ridge-regression-and-the-lasso>Another Formulation for Ridge Regression and the Lasso<a hidden class=anchor aria-hidden=true href=#another-formulation-for-ridge-regression-and-the-lasso>#</a></h3><p>The lasso and ridge regression coefficient estimates solve
the problems
$$
\begin{align}
\min_\beta \left(\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\right),, subject, to , \sum_{j=1}^p|\beta_j|\leq s \\
\min_\beta \left(\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\right),, subject, to , \sum_{j=1}^p\beta_j^2\leq s
\end{align}
$$
When we perform the lasso we are trying
to find the set of coefficient estimates that lead to the smallest RSS, subject
to the constraint that there is a budget s for how large $\sum_{j=1}^p|\beta_j|$ can be.
When s is extremely large, then this budget is not very restrictive, and so
the coefficient estimates can be large</p><p><strong>A close connection between the lasso, ridge regression, and best subset selection</strong>:</p><p>best subset selection is equivelant to :
$$
\begin{align}
\min_{\beta} \left(\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\right),, subject, to , \sum_{j=1}^pI(\beta_j\neq 0)\leq s
\end{align}
$$
Therefore, we can interpret <strong>ridge regression</strong> and <strong>the
lasso</strong> as computationally feasible alternatives to <strong>best subset selection</strong>.</p><h3 id=the-variable-selection-property-of-the-lasso>The Variable Selection Property of the Lasso<a hidden class=anchor aria-hidden=true href=#the-variable-selection-property-of-the-lasso>#</a></h3><p>The lasso and ridge regression coefficient estimates are given by the
first point at which an ellipse contacts the constraint region.</p><p><strong>ridge regression</strong>: <strong>circular</strong> constraint with no sharp points, so the ridge regression coefficient
estimates will be exclusively non-zero.</p><p><strong>the lasso</strong>: constraint has <strong>corners</strong> at each of the axes, and so the ellipse will often intersect the constraint
region at an axis.</p><ul><li>the $l_1$ penalty has the effect
of forcing some of the coefficient estimates to be exactly equal to zero when
the tuning parameter λ is sufficiently large.</li><li>Hence, much like best subset selection,
the lasso performs <strong>variable selection</strong></li></ul><blockquote><p>lasso yields <strong>sparse</strong> models</p></blockquote><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-model-selection/5_huec0b8a822063cddc94bd6d02d00d0ec3_166967_600x0_resize_box_3.png width=600 height=412><figcaption><small></small></figcaption></figure><h3 id=comparing-the-lasso-and-ridge-regression>Comparing the Lasso and Ridge Regression<a hidden class=anchor aria-hidden=true href=#comparing-the-lasso-and-ridge-regression>#</a></h3><p><strong>SAME</strong>: Ridge & Lasso all can yield a reduction in variance at the expense of a small increase in bias, and consequently can generate more accurate predictions.</p><p><strong>DIFFERENCES</strong>:</p><ul><li>Unlike ridge regression, the <strong>lasso performs variable selection</strong>, and hence results in models that are easier to interpret.</li><li>Ridge regression outperforms the lasso in terms of prediction error in this setting</li></ul><p><strong>Suitable setting</strong>:</p><ul><li><strong>Lasso</strong>: perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero.</li><li><strong>Ridge regression</strong>: perform better when the response is a function of many predictors, all with coefficients of roughly equal size.</li><li>The number of predictors that is related to the response is never known a <strong>priori</strong> for real data sets. Cross-validation can be used in order to determine which approach is better on a particular data set.</li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-model-selection/6_hu1642e8abb1519726a0ff2c8851cd82fa_197472_600x0_resize_box_3.png width=600 height=343><figcaption><small></small></figcaption></figure><p>The L2 ridge penalty $\sum_{j=1}^p\beta_j^2$ is replaced by the L1 lasso penalty $\sum_{j=1}^p|\beta_j|$. This latter constraint makes the solutions nonlinear in the $y_i$, and there is no closed form expression as in ridge regression.</p><blockquote><p>t should be adaptively chosen to minimize an estimate of expected prediction error.</p></blockquote><ul><li>if $t>t_0=\sum_{j=1}^p|\hat{\beta_j^{ls}}|$, then the lasso estimates are the $\hat{\beta_j^{ls}}$</li><li>if $t>t_0/2$, the least squares coeﬃcients are shrunk by about 50% on average</li></ul><p>The standardized parameter: $s=t/\sum_1^p|\hat{\beta_j}|$</p><ul><li>s=1.0, the lasso coeﬃcients are the least squares estimates</li><li>s->0, as the lasso coeﬃcients ->0</li></ul><h2 id=shrinkage-methods-vs-subset-selection><strong>Shrinkage Methods v.s. Subset Selection</strong>:<a hidden class=anchor aria-hidden=true href=#shrinkage-methods-vs-subset-selection>#</a></h2><ul><li><strong>Subset selection</strong><ul><li>described involve using least squares to fit a linear model that contains a subset of the predictors.</li><li>are discrete process—variables are either retained or discarded—it often exhibits high variance,and so doesn’t reduce the prediction error of the full model.</li></ul></li><li><strong>Shrinkage Methods</strong><ul><li>fit a model containing all p predictors by constraining or regularizing the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.</li><li>are more continuous, and don’t suﬀer as much from high variability.</li></ul></li></ul><h2 id=discussion-subset-selection-ridge-regression-and-the-lasso>Discussion: Subset Selection, Ridge Regression and the Lasso<a hidden class=anchor aria-hidden=true href=#discussion-subset-selection-ridge-regression-and-the-lasso>#</a></h2><ul><li>Ridge regression: does a proportional shrinkage</li><li>Lasso: translates each coeﬃcient by a constant factor λ, truncating at zero &ndash;“soft thresholding,”</li><li>Best-subset selection: drops all variables with coeﬃcients smaller than the Mth largest &ndash;“hard-thresholding.”<figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-model-selection/lass_ridge_hu96f9bcd5a0e56b2b472ec1839bd5c946_107508_600x0_resize_box_3.png width=600 height=413><figcaption><small></small></figcaption></figure></li></ul><h3 id=bayes-view>Bayes View<a hidden class=anchor aria-hidden=true href=#bayes-view>#</a></h3><p>Consider the criterion
$$
\begin{align}
\tilde{\beta}&=argmin_\beta ( \sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j|^q )
\end{align}
$$
for q ≥ 0. The contours of constant value of $\sum_{j=1}^p|\beta_j|^q$ are shown in Figure 3.12, for the case of two inputs.</p><p>The lasso, ridge regression and best subset selection are Bayes estimates with diﬀerent priors:Thinking of $\sum_{j=1}^p|\beta_j|^q$ as the log-prior density for βj , these are also the equi-contours of the prior distribution of the parameters.</p><ul><li>q = 0 :variable subset selection, as the penalty simply counts the number of nonzero parameters;</li><li>q = 1 :the lasso, also Laplace distribution for each input, with density $\frac{1}{2\tau}exp(-|\beta|/\tau)$, where $\tau=1/\lambda$</li><li>q = 2 :the ridge</li></ul><h1 id=considerations-in-high-dimensions>Considerations In High Dimensions<a hidden class=anchor aria-hidden=true href=#considerations-in-high-dimensions>#</a></h1><h2 id=high-dimensional-data>High-Dimensional Data<a hidden class=anchor aria-hidden=true href=#high-dimensional-data>#</a></h2><p><strong>High-dimensional</strong>: Data sets containing more features than observations are often referred
to as high-dimensional.</p><ul><li>Classical approaches such as least squares linear regression are not appropriate in this setting</li></ul><h2 id=what-goes-wrong-in-high-dimensions>What Goes Wrong in High Dimensions?<a hidden class=anchor aria-hidden=true href=#what-goes-wrong-in-high-dimensions>#</a></h2><ol><li>When the number of features p is as large as, or >n, least squares cannot be performed.</li></ol><p><strong>Reason</strong>: regardless of whether or not there truly is a relationship between the features and the response,
least squares will yield a set of coefficient estimates that result in a perfect fit to the data, such that the residuals are zero.</p><ul><li>This perfect fit will almost certainly lead to overfitting of the data</li><li>The problem is simple: when p > n or p ≈ n, a simple least squares regression line is too <em><strong>flexible</strong></em> and hence overfits the data.</li></ul><ol start=2><li>Examines only the R2 or the training set MSE might erroneously conclude that the model with the greatest number of variables is best.</li></ol><ul><li><strong>Cp, AIC, and BIC</strong> approaches are not appropriate in the high-dimensional setting, because estimating $\hat{σ}^2$ is problematic.(For instance, the formula for $\hat{σ}^2$ from Chapter 3 yields an
estimate $\hat{σ}^2$ = 0 in this setting.)</li><li>**Adjusted $R^2$ ** in the high-dimensional setting is problematic, since one can easily obtain
a model with an adjusted $R^2$ value of 1.</li></ul><h2 id=regression-in-high-dimensions>Regression in High Dimensions<a hidden class=anchor aria-hidden=true href=#regression-in-high-dimensions>#</a></h2><p><strong>Alternative approaches better-suited to the high-dimensional setting:</strong></p><ul><li>Forward stepwise selection</li><li>Ridge regression</li><li>The lasso</li><li>Principal components regression</li></ul><p><strong>Reason:</strong>
These approaches avoid overfitting by using a less flexible fitting approach than least squares.</p><p><strong>Three important points:</strong>
(1) Regularization or shrinkage plays a key role in high-dimensional problems,</p><p>(2) Appropriate tuning parameter selection is crucial for good predictive performance, and</p><p>(3) The test error tends to increase as the dimensionality of the problem
(i.e. the number of features or predictors) increases, unless the additional
features are truly associated with the response.$\Rightarrow$ <strong>curse of dimensionality</strong></p><h3 id=curse-of-dimensionality>Curse of dimensionality<a hidden class=anchor aria-hidden=true href=#curse-of-dimensionality>#</a></h3><p>Adding additional signal features
that are truly associated with the response will improve the fitted model; However, adding
noise features that are not truly associated with the response will lead
to a deterioration in the fitted model.</p><p><strong>Reason</strong>: This is because noise features increase the dimensionality of the problem, exacerbating the risk of overfitting (since noise features may be
assigned nonzero coefficients due to chance associations with the response
on the training set) without any potential upside in terms of improved test
set error.</p><h2 id=interpreting-results-in-high-dimensions>Interpreting Results in High Dimensions<a hidden class=anchor aria-hidden=true href=#interpreting-results-in-high-dimensions>#</a></h2><ol><li>Be cautious in reporting the results obtained when we perform the lasso, ridge regression, or other regression procedures in the high-dimensional setting.</li></ol><ul><li>In the high-dimensional setting, the <strong>multicollinearity</strong>
problem is extreme: any variable in the model can be written as a linear
combination of all of the other variables in the model. This
means that we can never know exactly which variables (if any) truly are
predictive of the outcome, and we can never identify the best coefficients
for use in the regression.</li></ul><ol start=2><li>Be cautious in reporting errors and measures of model fit in the high-dimensional setting</li></ol><ul><li>e.g.: when p > n, it is easy to obtain a useless model that has zero residuals.</li><li><strong>One should never use sum of squared errors, p-values, R2 statistics, or other traditional measures of model fit on the training data as
evidence of a good model fit in the high-dimensional setting</strong></li><li>It is important to
instead report results on an independent test set, or cross-validation errors.
For instance, the MSE or R2 on an independent test set is a valid measure
of model fit, but the MSE on the training set certainly is not.</li></ul><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &ldquo;The elements of statistical learning: data mining, inference and prediction.&rdquo; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://nancyyanyu.github.io/tags/model-selection/>Model Selection</a></li></ul><nav class=paginav><a class=prev href=https://nancyyanyu.github.io/posts/ml-resampling-methods-cross-validation-bootstrap/><span class=title>« Prev</span><br><span>Study Note: Resampling Methods - Cross Validation, Bootstrap</span></a>
<a class=next href=https://nancyyanyu.github.io/posts/ml-comparing-logistic-regression-lda-qda-and-knn/><span class=title>Next »</span><br><span>Study Note: Comparing Logistic Regression, LDA, QDA, and KNN</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Model Selection and Regularization (Ridge & Lasso) on twitter" href="https://twitter.com/intent/tweet/?text=Study%20Note%3a%20Model%20Selection%20and%20Regularization%20%28Ridge%20%26%20Lasso%29&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-model-selection%2f&amp;hashtags=ModelSelection"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Model Selection and Regularization (Ridge & Lasso) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-model-selection%2f&amp;title=Study%20Note%3a%20Model%20Selection%20and%20Regularization%20%28Ridge%20%26%20Lasso%29&amp;summary=Study%20Note%3a%20Model%20Selection%20and%20Regularization%20%28Ridge%20%26%20Lasso%29&amp;source=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-model-selection%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Model Selection and Regularization (Ridge & Lasso) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-model-selection%2f&title=Study%20Note%3a%20Model%20Selection%20and%20Regularization%20%28Ridge%20%26%20Lasso%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Model Selection and Regularization (Ridge & Lasso) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-model-selection%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Model Selection and Regularization (Ridge & Lasso) on whatsapp" href="https://api.whatsapp.com/send?text=Study%20Note%3a%20Model%20Selection%20and%20Regularization%20%28Ridge%20%26%20Lasso%29%20-%20https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-model-selection%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Model Selection and Regularization (Ridge & Lasso) on telegram" href="https://telegram.me/share/url?text=Study%20Note%3a%20Model%20Selection%20and%20Regularization%20%28Ridge%20%26%20Lasso%29&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-model-selection%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Model Selection and Regularization (Ridge & Lasso) on ycombinator" href="https://news.ycombinator.com/submitlink?t=Study%20Note%3a%20Model%20Selection%20and%20Regularization%20%28Ridge%20%26%20Lasso%29&u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-model-selection%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://nancyyanyu.github.io/>Nancy's Notebook</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>