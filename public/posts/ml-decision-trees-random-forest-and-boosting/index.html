<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Study Note: Decision Trees, Random Forest, and Boosting | Nancy's Notebook</title><meta name=keywords content="Trees"><meta name=description content="
Descision Tree/Recursive Binary Splitting/Classification Trees/Gini index/Bagging/Boosting/Random Forest
"><meta name=author content="Me"><link rel=canonical href=https://nancyyanyu.github.io/posts/ml-decision-trees-random-forest-and-boosting/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!0},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script type=module>
    import renderMathInElement from "https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.mjs";
    renderMathInElement(document.body);
</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Study Note: Decision Trees, Random Forest, and Boosting"><meta property="og:description" content="
Descision Tree/Recursive Binary Splitting/Classification Trees/Gini index/Bagging/Boosting/Random Forest
"><meta property="og:type" content="article"><meta property="og:url" content="https://nancyyanyu.github.io/posts/ml-decision-trees-random-forest-and-boosting/"><meta property="og:image" content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-06-15T14:49:45+00:00"><meta property="article:modified_time" content="2019-06-15T14:49:45+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Study Note: Decision Trees, Random Forest, and Boosting"><meta name=twitter:description content="
Descision Tree/Recursive Binary Splitting/Classification Trees/Gini index/Bagging/Boosting/Random Forest
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nancyyanyu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Study Note: Decision Trees, Random Forest, and Boosting","item":"https://nancyyanyu.github.io/posts/ml-decision-trees-random-forest-and-boosting/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Study Note: Decision Trees, Random Forest, and Boosting","name":"Study Note: Decision Trees, Random Forest, and Boosting","description":" Descision Tree/Recursive Binary Splitting/Classification Trees/Gini index/Bagging/Boosting/Random Forest\n","keywords":["Trees"],"articleBody":" Descision Tree/Recursive Binary Splitting/Classification Trees/Gini index/Bagging/Boosting/Random Forest\nIntroduction to Descision Tree Regression Trees Predicting Baseball Players’ Salaries Using Regression Trees Terminal nodes: The regions R1, R2, and R3 are known as terminal nodes or leaves of the tree.\nInternal nodes: The points along the tree where the predictor space is split are referred to as internal nodes.\nBranches: The segments of the trees that connect the nodes as branches\nPrediction via Stratification of the Feature Space Process of building a regression tree\nStep 1: We divide the predictor space—that is, the set of possible values for X1,X2, . . .,Xp—into J distinct and non-overlapping regions, R1,R2, . . . , RJ .\nStep 2: For every observation that falls into the region Rj, we make the same prediction, which is simply the mean of the response values for the training observations in Rj .\nStep 1 How do we construct the regions R1, . . .,RJ?\nWe choose to divide the predictor space into high-dimensional rectangles, or boxes, for ease of interpretation of the resulting predictive model.\nThe goal is to find boxes R1, . . . , RJ that minimize the RSS, given by $$ \\begin{align} \\sum_{j=1}^J\\sum_{i \\in R_j} (y_i-\\hat{y}_{R_j})^2 \\end{align} $$\nwhere $\\hat{y}_{R_j}$ is the mean response for the training observations within the jth box.\nRecursive Binary Splitting: a top-down, greedy approach\nTop-down: begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. Greedy: at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step. Methods:\nSelect the predictor $X_j$ and the cutpoint $s$ such that splitting the predictor space into the regions ${X|X_j \u003c s}$ and ${X|X_j ≥ s}$ leads to the greatest possible reduction in RSS In greater detail, for any $j$ and $s$, we define the pair of half-planes $$ \\begin{align} R_1(j, s) = {X|X_j \u003c s} ,\\quad R_2(j, s) = {X|X_j ≥ s} \\end{align} $$ and we seek the value of $j$ and $s$ that minimize the equation $$ \\begin{align} \\sum_{:x_i \\in R_1(j,s)}(y_i-\\hat{y}{R_1})^2+\\sum{:x_i \\in R_2(j,s)}(y_i-\\hat{y}{R_2})^2 \\end{align} $$ where $\\hat{y}{R_1}$is the mean response for the training observations in $R_1(j, s)$, where $\\hat{y}_{R_1}$is the mean response for the training observations in $R_1(j, s)$,\nand we seek the value of $j$ and $s$ that minimize the equation $$ \\begin{align} \\sum_{:x_i \\in R_1(j,s)}(y_i-\\hat{y}{R_1})^2+\\sum{:x_i \\in R_2(j,s)}(y_i-\\hat{y}{R_2})^2 \\end{align} $$ where $\\hat{y}{R_1}$is the mean response for the training observations in $R_1(j, s)$,\nwhere $\\hat{y}_{R_1}$is the mean response for the training observations in $R_1(j, s)$,\nRepeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions. However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions.\nWe now have three regions. Again, we look to split one of these three regions further, so as to minimize the RSS.\nThe process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations. Step 2 Predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.\nTree Pruning A better strategy is to grow a very large tree $T_0$, and then prune it back in order to obtain a subtree\nCost complexity pruning a.k.a.: weakest link pruning\nConsider a sequence of trees indexed by a nonnegative tuning parameter α\nFor each value of α there corresponds a subtree $T ⊂ T_0$ such that\n$$ \\begin{align} \\sum_{m=1}^T\\sum_{i:x_i \\in R_m}(y_i − \\hat{y}_{R_m})^2 + \\alpha|T| \\quad \\quad (8.4) \\end{align} $$ is as small as possible.\n$|T|$: the number of terminal nodes of the tree T , $R_m$: the rectangle (i.e. the subset of predictor space) corresponding to the m-th terminal node, $\\hat{y}_{R_m}$: the predicted response associated with $R_m$—that is, the mean of the training observations in $R_m$. The tuning parameter $α$ controls a trade-off between the subtree’s complexity and its fit to the training data. When α = 0, then the subtree T will simply equal T0, because then (8.4) just measures the training error. However, as α increases, there is a price to pay for having a tree with many terminal nodes, and so the quantity (8.4) will tend to be minimized for a smaller subtree.\nEquation 8.4 is reminiscent of the lasso, in which a similar formulation was used in order to control the complexity of a linear model.\nClassification Trees For a classification tree,\nWe predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs. RSS cannot be used as a criterion for making the binary splits $\\Rightarrow$ classification error rate. Classification Error Rate Since we plan to assign an observation in a given region to the most commonly occurring class of training observations in that region, the classification error rate is simply the fraction of the training observations in that region that do not belong to the most common class: $$ \\begin{align} E=1-\\max_k(\\hat{p}_{mk}) \\end{align} $$\n$\\hat{p}_{mk}$ : the proportion of training observations in the mth region that are from the kth class. classification error is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable: Gini index, cross-entropy. Gini index $$ \\begin{align} G=\\sum_{k=1}^K\\hat{p}{mk}(1-\\hat{p}{mk}) \\end{align} $$\nA measure of total variance across the K classes. It is not hard to see that the Gini index takes on a small value if all of the $\\hat{p}_{mk}$’s are close to zero or one. For this reason the Gini index is referred to as a measure of node purity—a small value indicates that a node contains predominantly observations from a single class. Cross-Entropy $$ \\begin{align} D=-\\sum_{k=1}^K\\hat{p}{mk}\\log{\\hat{p}{mk}} \\end{align} $$\nSince 0 ≤ $\\hat{p}{mk}$ ≤ 1, it follows that $0 ≤ −\\hat{p}{mk}\\log{\\hat{p}_{mk}}$. Cross-entropy will take on a value near zero if the $\\hat{p}_{mk}$’s are all near zero or near one. Therefore, like the Gini index, the cross-entropy will take on a small value if the mth node is pure. Cross-Entropy v.s. Gini index v.s. Classification Error Rate\nWhen building a classification tree, either the Gini index or the crossentropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate. Any of these three approaches might be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal. A surprising characteristic: some of the splits yield two terminal nodes that have the same predicted value. Why is the split performed at all? The split is performed because it leads to increased node purity. Why is node purity important? Suppose that we have a test observation that belongs to the region given by that right-hand leaf. Then we can be pretty certain that its response value is Yes. In contrast, if a test observation belongs to the region given by the left-hand leaf, then its response value is probably Yes, but we are much less certain. Even though the split RestECG\u003c1 does not reduce the classification error, it improves the Gini index and the cross-entropy, which are more sensitive to node purity. Trees Versus Linear Models Linear regression assumes a model of the form $$ \\begin{align} f(X)=\\beta_0+\\sum_{i=1}^p\\beta_iX_i \\end{align} $$ Regression trees assume a model of the form $$ \\begin{align} f(X)=\\sum_{m=1}^Mc_m \\cdot I_{X \\in R_m} \\end{align} $$ where R1, . . .,RM represent a partition of feature space\nwhere R1, . . .,RM represent a partition of feature space\nLinear regression works better: If the relationship between the features and the response is well approximated by a linear model; regression tree does not exploit this linear structure.\nRegression tree works better: If instead there is a highly non-linear and complex relationship between the features and the response.\nAdvantages and Disadvantages of Trees Advantages of decision trees for regression and classification:\n▲ Interpretation: Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!\n▲ Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches.\n▲ Visualization: Trees can be displayed graphically, and are easily interpreted even by a non-expert.\n▲ Trees can easily handle qualitative predictors without the need to create dummy variables.\nDisadvantages of decision trees for regression and classification:\n▼ Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.\nBagging Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method, frequently used in the context of decision trees.\nAveraging a set of observations reduces variance: Recall that given a set of n independent observations Z1, . . . , Zn, each with variance $σ^2$, the variance of the mean $\\bar{Z}$ of the observations is given by $σ^2/n$.\nA natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. Bootstrap taking repeated samples from the (single) training data set\nBagging\nGenerate B different bootstrapped training data sets. Train our method on the bth bootstrapped training set in order to get $\\hat{f}^{*b}(x)$ Finally average all the predictions, to obtain $$ \\begin{align} \\hat{f}{bag}(x)=\\frac{1}{B}\\sum{b=1}^B\\hat{f}^{*b}(x) \\end{align} $$\nApply bagging to regression trees\nConstruct B regression trees using B bootstrapped training sets Average the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these B trees reduces the variance. Bagging on Classification Tree\nFor a given test observation, we can record the class predicted by each of the B trees, and take a majority vote: the overall prediction is the most commonly occurring class among the B predictions. B\nIn practice weuse a value of B sufficiently large that the error has settled down, like B=100. Out-of-Bag Error Estimation Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around 2/3 of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations.\nWe can predict the response for the ith observation using each of the trees inwhich that observation was OOB.\nThis will yield around B/3 predictions for the ith observation. To obtain a single prediction for the ith observation, we can average these predicted responses (regression) or can take a majority vote (classification). This leads to a single OOB prediction for the ith observation. The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.\nVariable Importance Measures Bagging improves prediction accuracy at the expense of interpretability\nWhen we bag a large number of trees, it is no longer possible to represent the resulting statistical learning procedure using a single tree, and it is no longer clear which variables are most important to the procedure Variable Importance\nOne can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees).\nBagging regression trees: Record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all B trees. A large value indicates an important predictor. $$ \\begin{align} RSS=\\sum_{j=1}^J\\sum_{i \\in R_j} (y_i-\\hat{y}_{R_j})^2 \\end{align} $$\nBagging classification trees: Add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all B trees.\nRandom Forest Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees.\nAs in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors.\nThe split is allowed to use only one of those m predictors. A fresh sample of m predictors is taken at each split, and typically we choose $m ≈\\sqrt{p}$\nRationale:\nSuppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. Decorrelating the trees: Random forests forces each split to consider only a subset of the predictors, making the average of the resulting trees less variable and hence more reliable.\nBoosting Boosting: another approach for improving the predictions resulting from a decision tree.\nTrees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set. Idea behind this procedure\nUnlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead learns slowly. Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome Y , as the response. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter d in the algorithm. By fitting small trees to the residuals, we slowly improve $\\hat{f}$ in areas where it does not perform well. The shrinkage parameter λ slows the process down even further, allowing more and different shaped trees to attack the residuals. Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown.\nBoosting has three tuning parameters:\nThe number of trees $B$. The shrinkage parameter $λ$, a small positive number. This controls the rate at which boosting learns. The number $d$ of splits in each tree, which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump, consisting of a single split. In this case, the boosted ensemble is fitting an additive model, since each term involves only a single variable. More generally $d$ is the interaction depth, and controls the interaction order of the boosted model, since $d$ splits can involve at most d variables. Boosting V.S. Random forests:\nIn boosting, because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient. Using smaller trees can aid in interpretability as well; for instance, using stumps leads to an additive model. Ref:\nJames, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.\nHastie, Trevor, et al. “The elements of statistical learning: data mining, inference and prediction.” The Mathematical Intelligencer 27.2 (2005): 83-85\n","wordCount":"2621","inLanguage":"en","datePublished":"2019-06-15T14:49:45Z","dateModified":"2019-06-15T14:49:45Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nancyyanyu.github.io/posts/ml-decision-trees-random-forest-and-boosting/"},"publisher":{"@type":"Organization","name":"Nancy's Notebook","logo":{"@type":"ImageObject","url":"https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!0},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script type=module>
    import renderMathInElement from "https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.mjs";
    renderMathInElement(document.body);
</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nancyyanyu.github.io/ accesskey=h title="Nancy's Notebook (Alt + H)"><img src=https://nancyyanyu.github.io/apple-touch-icon.png alt aria-label=logo height=35>Nancy's Notebook</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nancyyanyu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://nancyyanyu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://nancyyanyu.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nancyyanyu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nancyyanyu.github.io/posts/>Posts</a></div><h1 class=post-title>Study Note: Decision Trees, Random Forest, and Boosting<sup><span class=entry-isdraft>&nbsp;&nbsp;[draft]</span></sup></h1><div class=post-meta><span title='2019-06-15 14:49:45 +0000 UTC'>June 15, 2019</span>&nbsp;·&nbsp;13 min&nbsp;·&nbsp;2621 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/ML-Decision%20Trees,%20Random%20Forest,%20and%20Boosting/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><blockquote><p>Descision Tree/Recursive Binary Splitting/Classification Trees/Gini index/Bagging/Boosting/Random Forest</p></blockquote><h1 id=introduction-to-descision-tree>Introduction to Descision Tree<a hidden class=anchor aria-hidden=true href=#introduction-to-descision-tree>#</a></h1><h2 id=regression-trees>Regression Trees<a hidden class=anchor aria-hidden=true href=#regression-trees>#</a></h2><h3 id=predicting-baseball-players-salaries-using-regression-trees>Predicting Baseball Players’ Salaries Using Regression Trees<a hidden class=anchor aria-hidden=true href=#predicting-baseball-players-salaries-using-regression-trees>#</a></h3><p><strong>Terminal nodes</strong>: The regions R1, R2, and R3 are known
as terminal nodes or leaves of the tree.</p><p><strong>Internal nodes</strong>: The points along the tree where the predictor space is split are referred to as internal nodes.</p><p><strong>Branches</strong>: The segments of the trees that connect the nodes as branches</p><p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-decision-trees-random-forest-and-boosting/12_hue2cb6e687dd49c70aa424a8fa93dd6a5_184591_500x0_resize_box_3.png width=500 height=387><figcaption><small></small></figcaption></figure><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-decision-trees-random-forest-and-boosting/1_hubf31f2da66f9c48e5bb2e53e924b64ba_112420_500x0_resize_box_3.png width=500 height=314><figcaption><small></small></figcaption></figure></p><h3 id=prediction-via-stratification-of-the-feature-space>Prediction via Stratification of the Feature Space<a hidden class=anchor aria-hidden=true href=#prediction-via-stratification-of-the-feature-space>#</a></h3><p><strong>Process of building a regression tree</strong></p><p><strong>Step 1</strong>: We divide the predictor space—that is, the set of possible values for
X1,X2, . . .,Xp—into J distinct and non-overlapping regions,
R1,R2, . . . , RJ .</p><p><strong>Step 2</strong>: For every observation that falls into the region Rj, we make the same
prediction, which is simply the <em>mean of the response values</em> for the
training observations in Rj .</p><h4 id=step-1>Step 1<a hidden class=anchor aria-hidden=true href=#step-1>#</a></h4><p><strong>How do we construct the regions R1, . . .,RJ?</strong></p><ul><li><p>We choose to divide the predictor space into high-dimensional rectangles, or
<strong>boxes</strong>, for ease of interpretation of the resulting predictive
model.</p></li><li><p>The goal is to find boxes R1, . . . , RJ that <strong>minimize the RSS</strong>,
given by
$$
\begin{align}
\sum_{j=1}^J\sum_{i \in R_j} (y_i-\hat{y}_{R_j})^2
\end{align}
$$</p><p>where $\hat{y}_{R_j}$ is the mean response for the training observations within the jth box.</p></li></ul><p><strong>Recursive Binary Splitting</strong>: a <em>top-down, greedy</em> approach</p><ul><li><strong>Top-down</strong>: begins at the top of the tree (at which point
all observations belong to a single region) and then successively splits the
predictor space; each split is indicated via two new branches further down
on the tree.</li><li><strong>Greedy</strong>: at each step of the tree-building process,
the best split is made at that particular step, rather than looking ahead
and picking a split that will lead to a better tree in some future step.</li></ul><p><strong>Methods</strong>:</p><ol><li>Select the predictor $X_j$ and the cutpoint $s$ such that splitting the predictor space into
the regions ${X|X_j &lt; s}$ and ${X|X_j ≥ s}$ leads to the greatest possible
reduction in RSS</li></ol><ul><li>In greater detail, for any $j$ and $s$, we define the pair of half-planes
$$
\begin{align}
R_1(j, s) = {X|X_j &lt; s} ,\quad R_2(j, s) = {X|X_j ≥ s}
\end{align}
$$
and we seek the value of $j$ and $s$ that <strong>minimize</strong> the equation
$$
\begin{align}
\sum_{:x_i \in R_1(j,s)}(y_i-\hat{y}<em>{R_1})^2+\sum</em>{:x_i \in R_2(j,s)}(y_i-\hat{y}<em>{R_2})^2
\end{align}
$$
where $\hat{y}</em>{R_1}$is the mean response for the training observations in $R_1(j, s)$,</li></ul><p>where $\hat{y}_{R_1}$is the mean response for the training observations in $R_1(j, s)$,</p><p>and we seek the value of $j$ and $s$ that <strong>minimize</strong> the equation
$$
\begin{align}
\sum_{:x_i \in R_1(j,s)}(y_i-\hat{y}<em>{R_1})^2+\sum</em>{:x_i \in R_2(j,s)}(y_i-\hat{y}<em>{R_2})^2
\end{align}
$$
where $\hat{y}</em>{R_1}$is the mean response for the training observations in $R_1(j, s)$,</p><p>where $\hat{y}_{R_1}$is the mean response for the training observations in $R_1(j, s)$,</p><ol start=2><li>Repeat the process, looking for the best predictor and best
cutpoint in order to split the data further so as to minimize the RSS within
each of the resulting regions.</li></ol><ul><li><p>However, this time, instead of splitting the
entire predictor space, we split one of the two previously identified regions.</p></li><li><p>We now have three regions. Again, we look to split one of these three regions
further, so as to minimize the RSS.</p></li></ul><ol start=3><li>The process continues until a stopping
criterion is reached; for instance, we may continue until no region contains
more than five observations.</li></ol><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-decision-trees-random-forest-and-boosting/3_huc8103dbb80cb75e0f949a0ac61f5e04d_327853_500x0_resize_box_3.png width=500 height=546><figcaption><small></small></figcaption></figure><h4 id=step-2>Step 2<a hidden class=anchor aria-hidden=true href=#step-2>#</a></h4><p>Predict the response
for a given test observation using the mean of the training observations in
the region to which that test observation belongs.</p><h3 id=tree-pruning>Tree Pruning<a hidden class=anchor aria-hidden=true href=#tree-pruning>#</a></h3><p>A better strategy is to grow a very large tree $T_0$, and then
<strong>prune</strong> it back in order to obtain a <strong>subtree</strong></p><h4 id=cost-complexity-pruning>Cost complexity pruning<a hidden class=anchor aria-hidden=true href=#cost-complexity-pruning>#</a></h4><p>a.k.a.: <strong>weakest link pruning</strong></p><p>Consider a sequence of trees indexed by a nonnegative tuning parameter α</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-decision-trees-random-forest-and-boosting/4_hu5c29cec875728ba8c489c301e588013b_202814_600x0_resize_box_3.png width=600 height=420><figcaption><small></small></figcaption></figure><p>For each value of α there corresponds a subtree $T ⊂ T_0$ such that</p><p>$$
\begin{align}
\sum_{m=1}^T\sum_{i:x_i \in R_m}(y_i − \hat{y}_{R_m})^2 + \alpha|T| \quad \quad (8.4)
\end{align}
$$
is as small as possible.</p><ul><li>$|T|$: the number of terminal nodes of the tree T ,</li><li>$R_m$: the rectangle (i.e. the subset of predictor space) corresponding
to the m-th <strong>terminal node</strong>,</li><li>$\hat{y}_{R_m}$: the predicted response associated with $R_m$—that is, the mean of the training observations in $R_m$.</li></ul><p>The tuning parameter $α$ controls a <em>trade-off</em> between the subtree’s <strong>complexity</strong>
and its <strong>fit to the training data</strong>. When α = 0, then the subtree T
will simply equal T0, because then (8.4) just measures the training error.
However, as α increases, there is a price to pay for having a tree with
many terminal nodes, and so the quantity (8.4) will tend to be minimized
for a smaller subtree.</p><p>Equation 8.4 is reminiscent of the lasso, in which a similar formulation was used in order to control the
complexity of a linear model.</p><p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-decision-trees-random-forest-and-boosting/5_hu194bf40d10aabdfbbf7fbed0f2f09464_127965_600x0_resize_box_3.png width=600 height=560><figcaption><small></small></figcaption></figure><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-decision-trees-random-forest-and-boosting/6_hu8e54b1040ace5ab0a9a543541a67a111_136558_600x0_resize_box_3.png width=600 height=381><figcaption><small></small></figcaption></figure></p><h2 id=classification-trees>Classification Trees<a hidden class=anchor aria-hidden=true href=#classification-trees>#</a></h2><p>For a classification tree,</p><ul><li>We predict that
each observation belongs to the <strong>most commonly occurring class</strong> of training
observations in the region to which it belongs.</li><li>RSS cannot be used as a criterion for making the binary splits $\Rightarrow$ <strong>classification error rate</strong>.</li></ul><h3 id=classification-error-rate>Classification Error Rate<a hidden class=anchor aria-hidden=true href=#classification-error-rate>#</a></h3><ul><li>Since we plan to assign an observation in a given region to the most commonly occurring class of training observations in that region, the classification error rate is
simply the <strong>fraction of the training observations in that region that do not
belong to the most common class</strong>:</li></ul><p>$$
\begin{align}
E=1-\max_k(\hat{p}_{mk})
\end{align}
$$</p><ul><li>$\hat{p}_{mk}$ : the proportion of training observations in the mth
region that are from the kth class.</li><li>classification
error is not sufficiently sensitive for tree-growing, and in practice two other
measures are preferable: <strong>Gini index, cross-entropy.</strong></li></ul><h3 id=gini-index>Gini index<a hidden class=anchor aria-hidden=true href=#gini-index>#</a></h3><p>$$
\begin{align}
G=\sum_{k=1}^K\hat{p}<em>{mk}(1-\hat{p}</em>{mk})
\end{align}
$$</p><ul><li>A measure of total variance across the K classes. It is not hard to see
that the Gini index takes on a small value if all of the $\hat{p}_{mk}$’s are close to
zero or one.</li><li>For this reason the Gini index is referred to as a measure of
node <strong>purity</strong>—a small value indicates that a node contains predominantly
observations from a single class.</li></ul><h3 id=cross-entropy>Cross-Entropy<a hidden class=anchor aria-hidden=true href=#cross-entropy>#</a></h3><p>$$
\begin{align}
D=-\sum_{k=1}^K\hat{p}<em>{mk}\log{\hat{p}</em>{mk}}
\end{align}
$$</p><ul><li>Since 0 ≤ $\hat{p}<em>{mk}$ ≤ 1, it follows that $0 ≤ −\hat{p}</em>{mk}\log{\hat{p}_{mk}}$.</li><li>Cross-entropy will take on a value near zero if the $\hat{p}_{mk}$’s are all near
zero or near one. Therefore, like the Gini index, the cross-entropy will take
on a small value if the mth node is <strong>pure</strong>.</li></ul><hr><p><strong>Cross-Entropy v.s. Gini index v.s. Classification Error Rate</strong></p><ul><li>When building a classification tree, either the Gini index or the crossentropy
are typically used to evaluate the quality of a particular split,
since these two approaches are more sensitive to node purity than is the
classification error rate. Any of these three approaches might be used when
pruning the tree, but the classification error rate is preferable if prediction
accuracy of the final pruned tree is the goal.</li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-decision-trees-random-forest-and-boosting/9_hufcd7e689dc88c0090ffb8c4e81e0468b_187226_800x0_resize_box_3.png width=800 height=954><figcaption><small></small></figcaption></figure><ul><li><strong>A surprising characteristic</strong>: some of the splits yield two
terminal nodes that have the same predicted value.</li><li><strong>Why is the split performed at all?</strong> The split is performed because it leads to <strong>increased node purity.</strong></li><li><strong>Why is node purity important?</strong> Suppose that we have a test observation
that belongs to the region given by that right-hand leaf. Then we
can be pretty certain that its response value is Yes. In contrast, if a test
observation belongs to the region given by the left-hand leaf, then its response
value is probably Yes, but we are much less certain. Even though
the split RestECG&lt;1 does not reduce the classification error, it improves the
<strong>Gini index and the cross-entropy</strong>, which are more sensitive to node purity.</li></ul><h2 id=trees-versus-linear-models>Trees Versus Linear Models<a hidden class=anchor aria-hidden=true href=#trees-versus-linear-models>#</a></h2><p>Linear regression assumes a model of the form
$$
\begin{align}
f(X)=\beta_0+\sum_{i=1}^p\beta_iX_i
\end{align}
$$
Regression trees assume a model of the form
$$
\begin{align}
f(X)=\sum_{m=1}^Mc_m \cdot I_{X \in R_m}
\end{align}
$$
where R1, . . .,RM represent a partition of feature space</p><p>where R1, . . .,RM represent a partition of feature space</p><p><strong>Linear regression works better</strong>: If the
relationship between the features and the response is well approximated
by a linear model; regression
tree does not exploit this linear structure.</p><p><strong>Regression tree works better</strong>: If instead there is a highly
non-linear and complex relationship between the features and the response.</p><h2 id=advantages-and-disadvantages-of-trees>Advantages and Disadvantages of Trees<a hidden class=anchor aria-hidden=true href=#advantages-and-disadvantages-of-trees>#</a></h2><p><strong>Advantages of decision trees for regression and classification:</strong></p><p>▲ <strong>Interpretation</strong>: Trees are very <strong>easy to explain</strong> to people. In fact, they are even easier
to explain than linear regression!</p><p>▲ Some people believe that decision trees more closely <strong>mirror human
decision-making</strong> than do the regression and classification approaches.</p><p>▲ <strong>Visualization</strong>: Trees can be <strong>displayed graphically</strong>, and are easily interpreted even by
a non-expert.</p><p>▲ Trees can easily handle qualitative predictors without the need to
create dummy variables.</p><p><strong>Disadvantages of decision trees for regression and classification:</strong></p><p>▼ Unfortunately, trees generally do not have the same level of predictive
accuracy as some of the other regression and classification approaches
seen in this book.</p><h1 id=bagging>Bagging<a hidden class=anchor aria-hidden=true href=#bagging>#</a></h1><p><strong>Bootstrap aggregation</strong>, or <strong>bagging</strong>, is a general-purpose procedure for reducing the
variance of a statistical learning method, frequently used in the context of decision trees.</p><p><strong>Averaging a set of observations reduces variance</strong>: Recall that given a set of n independent observations Z1, . . . , Zn, each
with variance $σ^2$, the variance of the mean $\bar{Z}$ of the observations is given
by $σ^2/n$.</p><ul><li>A natural way to reduce the variance and hence increase the prediction
accuracy of a statistical learning method is to <strong>take many training sets
from the population</strong>, build a separate prediction model using each training
set, and average the resulting predictions.</li></ul><p><strong>Bootstrap</strong> taking repeated samples from the (single) training data set</p><p><strong>Bagging</strong></p><ul><li>Generate B different bootstrapped training data sets.</li><li>Train our method on
the bth bootstrapped training set in order to get $\hat{f}^{*b}(x)$</li><li>Finally average
all the predictions, to obtain</li></ul><p>$$
\begin{align}
\hat{f}<em>{bag}(x)=\frac{1}{B}\sum</em>{b=1}^B\hat{f}^{*b}(x)
\end{align}
$$</p><p><strong>Apply bagging to regression trees</strong></p><ul><li>Construct B regression trees using B bootstrapped training
sets</li><li>Average the resulting predictions. These trees are grown deep,
and are not pruned. Hence each individual tree has high variance, but
low bias. Averaging these B trees reduces the variance.</li></ul><p><strong>Bagging on Classification Tree</strong></p><ul><li>For a given test observation, we can record the class predicted by each of the B trees, and
take a <strong>majority vote</strong>: the overall prediction is the most commonly occurring
class among the B predictions.</li></ul><p><strong>B</strong></p><ul><li>In practice weuse a value of B sufficiently large that the error has settled down, like B=100.</li></ul><h2 id=out-of-bag-error-estimation>Out-of-Bag Error Estimation<a hidden class=anchor aria-hidden=true href=#out-of-bag-error-estimation>#</a></h2><p>Recall that the key to bagging is that trees are
repeatedly fit to bootstrapped subsets of the observations. One can show
that on average, each bagged tree makes use of around 2/3 of the
observations. The remaining one-third of the observations not used to fit a
given bagged tree are referred to as the <strong>out-of-bag (OOB)</strong> observations.</p><blockquote><p>We can predict the response for the ith observation using each of the trees inwhich that observation was OOB.</p></blockquote><ul><li>This will yield around B/3 predictions for the ith observation.</li><li>To obtain a single prediction for the ith observation, we can <strong>average</strong> these predicted responses (regression) or can take a <strong>majority vote</strong> (classification).</li><li>This leads to a single OOB prediction for the ith observation.</li></ul><p>The OOB approach for estimating
the test error is particularly convenient when performing bagging on large
data sets for which <strong>cross-validation</strong> would be computationally onerous.</p><h2 id=variable-importance-measures>Variable Importance Measures<a hidden class=anchor aria-hidden=true href=#variable-importance-measures>#</a></h2><p><strong>Bagging improves prediction accuracy at the expense of interpretability</strong></p><ul><li>When we bag a large number of trees, it is no longer possible to represent the resulting statistical learning procedure using a single tree, and it is no longer clear which variables are most important to the procedure</li></ul><p><strong>Variable Importance</strong></p><ul><li><p>One can obtain an overall summary of the importance of
each predictor using the RSS (for bagging regression trees) or the Gini index
(for bagging classification trees).</p></li><li><p><strong>Bagging regression trees</strong>: Record the total amount that the RSS is decreased due to splits
over a given predictor, averaged over all B trees. A large value indicates
an important predictor.
$$
\begin{align}
RSS=\sum_{j=1}^J\sum_{i \in R_j} (y_i-\hat{y}_{R_j})^2
\end{align}
$$</p></li><li><p><strong>Bagging classification
trees</strong>: Add up the total amount that the <strong>Gini index</strong> is decreased
by splits over a given predictor, averaged over all B trees.</p></li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-decision-trees-random-forest-and-boosting/11_hu5a0817855fd7476dba5333fc7f0ccd8d_124969_600x0_resize_box_3.png width=600 height=441><figcaption><small></small></figcaption></figure><h1 id=random-forest>Random Forest<a hidden class=anchor aria-hidden=true href=#random-forest>#</a></h1><p><strong>Random forests</strong> provide an improvement over bagged trees by way of a small tweak that <strong>decorrelates</strong> the trees.</p><p>As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, <em>a random sample of m predictors is chosen as split candidates</em> from the full set of p predictors.</p><p><strong>The split is allowed to use only one of those m predictors.</strong> A fresh sample of m predictors is taken at each split, and typically we choose $m ≈\sqrt{p}$</p><p><strong>Rationale</strong>:</p><ul><li>Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, <em>all of the bagged trees will look quite similar to each other.</em></li><li>Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities.</li></ul><p><strong>Decorrelating</strong> the trees: Random forests forces each split to consider only a subset of the predictors, making the average of the resulting trees less variable and hence more reliable.</p><h1 id=boosting>Boosting<a hidden class=anchor aria-hidden=true href=#boosting>#</a></h1><p><strong>Boosting</strong>: another approach for improving the predictions resulting from a decision tree.</p><ul><li>Trees are grown <strong>sequentially</strong>: each tree is grown using information from previously
grown trees.</li><li>Boosting does not involve bootstrap sampling; instead each
tree is fit on a modified version of the original data set.</li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/ml-decision-trees-random-forest-and-boosting/12_hue2cb6e687dd49c70aa424a8fa93dd6a5_184591_600x0_resize_box_3.png width=600 height=464><figcaption><small></small></figcaption></figure><p><strong>Idea behind this procedure</strong></p><ul><li>Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead <strong>learns slowly</strong>.</li><li>Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome Y , as the response.</li><li>We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter <strong>d</strong> in the algorithm.</li><li>By fitting small trees to the residuals, we slowly improve $\hat{f}$ in areas where it does not perform well.</li><li>The shrinkage parameter <strong>λ</strong> slows the process down even further, allowing more and different shaped trees to attack the residuals.</li></ul><blockquote><p>Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown.</p></blockquote><p><strong>Boosting has three tuning parameters:</strong></p><ol><li>The number of trees $B$.</li><li>The shrinkage parameter $λ$, a small positive number. This controls the rate at which boosting learns.</li><li>The number $d$ of splits in each tree, which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a <strong>stump</strong>, consisting of a single split. In this case, the boosted ensemble is fitting an <strong>additive model</strong>, since each term involves only a single variable. More generally $d$ is the <strong>interaction depth</strong>, and controls the interaction order of the boosted model, since $d$ splits can involve at most d variables.</li></ol><p><strong>Boosting V.S. Random forests:</strong></p><ul><li>In boosting, because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient.</li><li>Using smaller trees can aid in interpretability as well; for instance, using <strong>stumps</strong> leads to an additive model.</li></ul><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &ldquo;The elements of statistical learning: data mining, inference and prediction.&rdquo; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://nancyyanyu.github.io/tags/trees/>Trees</a></li></ul><nav class=paginav><a class=prev href=https://nancyyanyu.github.io/posts/ml-clustering/><span class=title>« Prev</span><br><span>Study Note: Clustering</span></a>
<a class=next href=https://nancyyanyu.github.io/posts/ml-dimension_reduction-pca/><span class=title>Next »</span><br><span>Study Note: Dimension Reduction - PCA, PCR</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Decision Trees, Random Forest, and Boosting on twitter" href="https://twitter.com/intent/tweet/?text=Study%20Note%3a%20Decision%20Trees%2c%20Random%20Forest%2c%20and%20Boosting&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-decision-trees-random-forest-and-boosting%2f&amp;hashtags=Trees"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Decision Trees, Random Forest, and Boosting on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-decision-trees-random-forest-and-boosting%2f&amp;title=Study%20Note%3a%20Decision%20Trees%2c%20Random%20Forest%2c%20and%20Boosting&amp;summary=Study%20Note%3a%20Decision%20Trees%2c%20Random%20Forest%2c%20and%20Boosting&amp;source=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-decision-trees-random-forest-and-boosting%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Decision Trees, Random Forest, and Boosting on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-decision-trees-random-forest-and-boosting%2f&title=Study%20Note%3a%20Decision%20Trees%2c%20Random%20Forest%2c%20and%20Boosting"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Decision Trees, Random Forest, and Boosting on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-decision-trees-random-forest-and-boosting%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Decision Trees, Random Forest, and Boosting on whatsapp" href="https://api.whatsapp.com/send?text=Study%20Note%3a%20Decision%20Trees%2c%20Random%20Forest%2c%20and%20Boosting%20-%20https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-decision-trees-random-forest-and-boosting%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Decision Trees, Random Forest, and Boosting on telegram" href="https://telegram.me/share/url?text=Study%20Note%3a%20Decision%20Trees%2c%20Random%20Forest%2c%20and%20Boosting&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-decision-trees-random-forest-and-boosting%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Study Note: Decision Trees, Random Forest, and Boosting on ycombinator" href="https://news.ycombinator.com/submitlink?t=Study%20Note%3a%20Decision%20Trees%2c%20Random%20Forest%2c%20and%20Boosting&u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fml-decision-trees-random-forest-and-boosting%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://nancyyanyu.github.io/>Nancy's Notebook</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>