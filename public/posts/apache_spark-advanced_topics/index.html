<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Apache Spark: Advanced Topics | Nancy's Notebook</title><meta name=keywords content="Spark"><meta name=description content="
Study note of Big Data Essentials: HDFS, MapReduce and Spark RDD
"><meta name=author content="Me"><link rel=canonical href=https://nancyyanyu.github.io/posts/apache_spark-advanced_topics/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Apache Spark: Advanced Topics"><meta property="og:description" content="
Study note of Big Data Essentials: HDFS, MapReduce and Spark RDD
"><meta property="og:type" content="article"><meta property="og:url" content="https://nancyyanyu.github.io/posts/apache_spark-advanced_topics/"><meta property="og:image" content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Apache Spark: Advanced Topics"><meta name=twitter:description content="
Study note of Big Data Essentials: HDFS, MapReduce and Spark RDD
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nancyyanyu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Apache Spark: Advanced Topics","item":"https://nancyyanyu.github.io/posts/apache_spark-advanced_topics/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Apache Spark: Advanced Topics","name":"Apache Spark: Advanced Topics","description":" Study note of Big Data Essentials: HDFS, MapReduce and Spark RDD\n","keywords":["Spark"],"articleBody":" Study note of Big Data Essentials: HDFS, MapReduce and Spark RDD\nExecution \u0026 Scheduling SparkContext\nWhen creating a Spark application, the first thing you do is create a SparkContext object, which tells Sparks how to access a cluster. The context, living in your driver program, coordinates sets of processes on the cluster to run your application. The SparkContext object communicates the Cluster Manager to allocate executors. The Cluster Manager is an external service for acquiring resources on a cluster. For example, YARN, Mesos or a standalone Spark cluster. once the context has allocated the executors, it communicates directly with them and schedules tasks to be done. Jobs, stages, tasks Task is a unit of work to be done Tasks are created by a job scheduler during the scheduling of a job for every job stage. And every task belongs to the job stage. Job is spawned in response to a Spark action Job is divided in smaller sets of tasks called stages Example Z = X .map(lambda x: (x % 10, x / 10)) .reduceByKey(lambda x, y: x + y) .collect()\nWhenever you invoke an action, the job gets spawned in the driver program. Then the driver runs a job scheduler to divide the job into smaller stages. Then tasks are created for every job stage.\ntasks are delegated to the executors, which perform the actual work. bash All this machinery exists within the SparkContext object. It keeps track of the executors, it spawns jobs, and it runs the scheduler. Difference between job stage and task:\nJob stage is a pipelined computation spanning between materialization boundaries job stages are defined on RDD level, thus not immediately executable Task is a job stage bound to particular partitions bound to a particular partitions, thus immediately executable Materialization happens when reading, shuffling or passing data to an action narrow dependencies allow pipelining wide dependencies forbid it SparkContext – other functions:\nTracks liveness of the executors by sending heartbeat messages periodically. required to provide fault-tolerance Schedules multiple concurrent jobs to control the resource allocation within the application Performs dynamic resource allocation if the cluster manager permits. increases cluster utilization in shared environments by proper scheduling of multiple applications according to their resource demands Summary The SparkContext is the core of your application allows your application to connect to a cluster and allocate resources and executors. whenever you invoke an action, the SparkContext spawns a job and runs the job scheduler to divide it into stages–\u003epipelineable tasks are created for every job stage and scheduled to the executors. The driver communicates directly with the executors Execution goes as follows: Action -\u003e Job -\u003e Job Stages -\u003e Tasks Transformations with narrow dependencies allow pipelining Caching \u0026 Persistence RDDs are partitioned Execution is build around the partitions Each task processes a small number of partitions at a time, and the shuffle globally redistributes data items between the partitions, when required. Spark transfers data over the network and the IO unit here is not a partition but a block. Block is a unit of input and output in Spark Example Motivating example: load a wikipedia dump from HDFS and see how many articles there contain the words Spark\nYou need to create the RDD, apply the filter transformation, and invoke the count action. Motivating example: among those articles with the Spark word, you would like to see how many of them contain the word, Hadoop and how many the word MapReduce.\nPerform worse!\nReason: after completing the computation, Spark disposes intermediate data and those intermediate RDDs. That means, it will reload the Wikipedia dump two more times incurring extra input and output operations.\nA better strategy:\ncache the preloaded dump in the memory and reuse it until you end your session. Spark allows you to hint which RDDs are better to be kept in memory or even on the disk. Spark does so by ***caching the blocks comprising your dataset. ***\nControlling persistence level Cache: mark the data set as cached by invoking a cache method on it\nThe cache method is just a shortcut for the memory-only persistence. Persist: allows you to set RDDs storage to persist across operations after the first time it is computed.\nparameterized by a storage level Best practices When running an interactive shell, cache your dataset after you’ve done all the necessary preprocessing.\nby keeping your work inside in the memory, you would get a more responsive experience. When running a batch computation, cache dictionaries that you join with your data.\nJoin dictionaries are often reshuffled, so it would be helpful to speed up their read times. When running an iterative computation, cache static data like dictionaries or input datasets\navoid reloading the data from the ground up on every iteration. The static data tends to get evicted due to the memory pressure from the intermediate data. Summary Performance may be improved by persisting data across operations in interactive sessions, iterative computations and hot datasets You can control the persistence of a dataset whether to store in the memory or on the disk how many replicas to create Broadcast Variable shared memory is a powerful abstraction, but often misused.\nIt can make the developer’s life easier It can make the application performance deteriorate because of extra synchronization. This is why in spark there are restricted forms of the shared memory. Broadcast variable is a read-only variable that is efficiently shared among tasks\none to many communication: When it captures a variable into the closure, it is sent to an executor together with a task specification.\nmany to many communication protocol: torrent\nDistribution is done by a torrent-like protocol (extremely fast!) Distributed efficiently compared to captured variables Example Motivating example: resolve IP addresses to countries from 1 terabyte access log for your website\nIdea: map-side join–distribute the database to every mapper and query it locally.\nDistributing the database via a broadcast variable, we take slightly more than 1 gigabyte of outgoing traffic at the driver node\nMotivating example:\nsetup a transformation graph to compute a dictionary invoke the collect action to load it into the driver’s memory put it into the broadcast variable to use in further computations. Idea: upload computations to spark executors and use the driver program as the coordinator.\nSummary Broadcast variables are read-only shared variables with effective sharing mechanism Useful to share dictionaries, models Accumulator Variable Accumulator variable is a read-write variable that is shared among tasks\nWrites are restricted to increments! i. e.: var += delta addition may be replaced by any associate, commutative operation Restricting the right operations allows the framework to avoid complex synchronization thus making the accumulators efficient. Accumulator variable could be read only by the driver program and not by the executors. cannot read the accumulated value from within a task Example Guarantees on the updates Updates generated in actions: guaranteed to be applied only once to the accumulator. This is because successful actions are never re-executed and Spark can conditionally apply the update. Updates generated in transformations: no guarantees when they accumulate updates. - - Transformations can be recomputed on a failure, on the memory pressure, or in another unspecified codes like a preemption. Spark provides no guarantees on how many times transformation code maybe re-executed. Use cases Performance counters number of processed records, total elapsed time, total error and so on and so forth Simple control flow conditionals: stop on reaching a threshold for corrupted records loops: decide whether to run the next iteration of an algorithm or not Monitoring export values to the monitoring system Profiling \u0026 debugging Summary Accumulators are shared read-write variables with de-coupled read and write sides could be updated from actions and transformations by using an increment. can use custom associative, commutative operation for the updates can read the total value only in the driver Useful for the control flow, monitoring, profiling \u0026 debugging ","wordCount":"1307","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nancyyanyu.github.io/posts/apache_spark-advanced_topics/"},"publisher":{"@type":"Organization","name":"Nancy's Notebook","logo":{"@type":"ImageObject","url":"https://nancyyanyu.github.io/%3Clink%20/%20abs%20url%3E"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nancyyanyu.github.io/ accesskey=h title="Nancy's Notebook (Alt + H)"><img src=https://nancyyanyu.github.io/apple-touch-icon.png alt aria-label=logo height=35>Nancy's Notebook</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nancyyanyu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://nancyyanyu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://nancyyanyu.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nancyyanyu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nancyyanyu.github.io/posts/>Posts</a></div><h1 class=post-title>Apache Spark: Advanced Topics</h1><div class=post-meta>7 min&nbsp;·&nbsp;1307 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/Apache_Spark-Advanced_Topics/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><blockquote><p>Study note of <a href=https://www.coursera.org/learn/big-data-essentials>Big Data Essentials: HDFS, MapReduce and Spark RDD</a></p></blockquote><h1 id=execution--scheduling>Execution & Scheduling<a hidden class=anchor aria-hidden=true href=#execution--scheduling>#</a></h1><p><strong>SparkContext</strong></p><ul><li>When creating a Spark application, the first thing you do is create a SparkContext object, which tells Sparks how to access a cluster.</li><li>The context, living in your driver program, coordinates sets of processes on the cluster to run your application.</li></ul><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-advanced_topics/exe1_hu9079e9dc2d09d0f148efabfb75e293cb_121282_600x0_resize_box_3.png width=600 height=260><figcaption><small></small></figcaption></figure><ul><li>The SparkContext object communicates the Cluster Manager to allocate executors.</li><li>The Cluster Manager is an external service for acquiring resources on a cluster. For example, YARN, Mesos or a standalone Spark cluster.</li><li>once the context has allocated the executors, it communicates directly with them and schedules tasks to be done.</li></ul><h2 id=jobs-stages-tasks>Jobs, stages, tasks<a hidden class=anchor aria-hidden=true href=#jobs-stages-tasks>#</a></h2><ul><li><strong>Task</strong> is a unit of work to be done</li><li><strong>Tasks</strong> are created by a <strong>job scheduler</strong> during the scheduling of a job for every job stage. And every task belongs to the job stage.</li><li><strong>Job</strong> is spawned in response to a Spark action</li><li><strong>Job</strong> is divided in smaller sets of tasks called <strong>stages</strong></li></ul><h3 id=example>Example<a hidden class=anchor aria-hidden=true href=#example>#</a></h3><p>Z = X
.map(lambda x: (x % 10, x / 10))
.reduceByKey(lambda x, y: x + y)
.collect()</p><ol><li><p>Whenever you invoke an action, the job gets spawned in the driver program.<figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-advanced_topics/exe3_hu7c5d58649d597b0f87a096e7886264f1_36926_600x0_resize_box_3.png width=600 height=74><figcaption><small></small></figcaption></figure></p></li><li><p>Then the driver runs a job scheduler to divide the job into smaller stages.<figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-advanced_topics/exe3_hu7c5d58649d597b0f87a096e7886264f1_36926_600x0_resize_box_3.png width=600 height=74><figcaption><small></small></figcaption></figure></p></li><li><p>Then tasks are created for every job stage.</p></li><li><p>tasks are delegated to the executors, which perform the actual work.<figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-advanced_topics/exe4_hu0046bc776d872f7b360f2ad4a5a64dd6_73703_600x0_resize_box_3.png width=600 height=180><figcaption><small></small></figcaption></figure></p></li></ol><pre tabindex=0><code>bash
All this machinery exists within the SparkContext object. It keeps track of the executors, it spawns jobs, and it runs the scheduler.
</code></pre><p><strong>Difference between job stage and task:</strong></p><ul><li><strong>Job stage</strong> is a pipelined computation spanning between materialization boundaries</li><li>job stages are defined on RDD level, thus not immediately executable</li><li><strong>Task</strong> is a job stage bound to particular partitions</li><li>bound to a particular partitions, thus immediately executable</li><li><strong>Materialization</strong> happens when reading, shuffling or passing data to an action</li><li>narrow dependencies allow pipelining</li><li>wide dependencies forbid it</li></ul><p><strong>SparkContext – other functions</strong>:</p><ul><li>Tracks liveness of the executors by sending heartbeat messages periodically.</li><li>required to provide fault-tolerance</li><li>Schedules multiple concurrent jobs</li><li>to control the resource allocation within the application</li><li>Performs dynamic resource allocation if the cluster manager permits.</li><li>increases cluster utilization in shared environments by proper scheduling of multiple applications according to their resource demands</li></ul><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><ol><li>The SparkContext is the core of your application</li></ol><ul><li>allows your application to connect to a cluster and allocate resources and executors.</li><li>whenever you invoke an action, the SparkContext spawns a job and runs the job scheduler to divide it into stages&ndash;><strong>pipelineable</strong></li><li>tasks are created for every job stage and scheduled to the executors.</li></ul><ol start=2><li>The driver communicates directly with the executors</li><li>Execution goes as follows:</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Action -&gt; Job -&gt; Job Stages -&gt; Tasks
</span></span></code></pre></div><ol start=4><li>Transformations with narrow dependencies allow pipelining</li></ol><h1 id=caching--persistence>Caching & Persistence<a hidden class=anchor aria-hidden=true href=#caching--persistence>#</a></h1><ul><li>RDDs are partitioned</li><li>Execution is build around the partitions</li><li>Each task processes a small number of partitions at a time, and the shuffle globally redistributes data items between the partitions, when required.</li><li>Spark transfers data over the network and the IO unit here is not a partition but a block.</li><li>Block is a unit of input and output in Spark</li></ul><h2 id=example-1>Example<a hidden class=anchor aria-hidden=true href=#example-1>#</a></h2><blockquote><p>Motivating example: load a wikipedia dump from HDFS and see how many articles there contain the words Spark</p></blockquote><p>You need to create the RDD, apply the filter transformation, and invoke the count action.<figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-advanced_topics/exe5_hu5a6c873b21e3be895ba34164aa21770c_94778_600x0_resize_box_3.png width=600 height=197><figcaption><small></small></figcaption></figure></p><blockquote><p>Motivating example: among those articles with the Spark word, you would like to see how many of them contain the word, Hadoop and how many the word MapReduce.</p></blockquote><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-advanced_topics/exe6_hu7557cabe1c39e12556725439bd3a390d_200407_600x0_resize_box_3.png width=600 height=286><figcaption><small></small></figcaption></figure><p><em><strong>Perform worse!</strong></em></p><ul><li><p>Reason: after completing the computation, Spark disposes intermediate data and those intermediate RDDs. That means, it will reload the Wikipedia dump two more times incurring extra input and output operations.</p></li><li><p>A better strategy:</p></li></ul><pre tabindex=0><code>cache the preloaded dump in the memory and reuse it until you end your session.
</code></pre><p>Spark allows you to hint which RDDs are better to be kept in memory or even on the disk. Spark does so by ***caching the blocks comprising your dataset. ***</p><h2 id=controlling-persistence-level>Controlling persistence level<a hidden class=anchor aria-hidden=true href=#controlling-persistence-level>#</a></h2><p>Cache: mark the data set as cached by invoking a cache method on it</p><ul><li>The cache method is just a shortcut for the memory-only persistence.</li></ul><p>Persist: allows you to set RDDs storage to persist across operations after the first time it is computed.</p><ul><li>parameterized by a storage level<figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-advanced_topics/exe7_hu61d535c2b793fdbb04cb134dd5227d47_231998_600x0_resize_box_3.png width=600 height=335><figcaption><small></small></figcaption></figure></li></ul><h2 id=best-practices>Best practices<a hidden class=anchor aria-hidden=true href=#best-practices>#</a></h2><p>When running an <strong>interactive shell</strong>, cache your dataset after you&rsquo;ve done all the necessary preprocessing.</p><ul><li>by keeping your work inside in the memory, you would get a more responsive experience.</li></ul><p>When running a <strong>batch computation</strong>, cache dictionaries that you join with your data.</p><ul><li>Join dictionaries are often reshuffled, so it would be helpful to speed up their read times.</li></ul><p>When running an <strong>iterative computation</strong>, cache static data like dictionaries or input datasets</p><ul><li>avoid reloading the data from the ground up on every iteration.</li><li>The static data tends to get evicted due to the memory pressure from the intermediate data.</li></ul><h2 id=summary-1>Summary<a hidden class=anchor aria-hidden=true href=#summary-1>#</a></h2><ul><li>Performance may be improved by persisting data across operations</li><li>in interactive sessions, iterative computations and hot datasets</li><li>You can control the persistence of a dataset</li><li>whether to store in the memory or on the disk</li><li>how many replicas to create</li></ul><h1 id=broadcast-variable>Broadcast Variable<a hidden class=anchor aria-hidden=true href=#broadcast-variable>#</a></h1><p><strong>shared memory</strong> is a powerful abstraction, but often misused.</p><ul><li>It can make the developer&rsquo;s life easier</li><li>It can make the application performance deteriorate because of extra synchronization.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>This is why in spark there are restricted forms of the shared memory. 
</span></span></code></pre></div><p><strong>Broadcast variable</strong> is a read-only variable that is efficiently shared among tasks</p><p><strong>one to many communication:</strong> When it captures a variable into the closure, it is sent to an executor together with a task specification.</p><p><strong>many to many communication protocol</strong>: torrent</p><ul><li>Distribution is done by a torrent-like protocol (extremely fast!)</li><li>Distributed efficiently compared to captured variables</li></ul><h2 id=example-2>Example<a hidden class=anchor aria-hidden=true href=#example-2>#</a></h2><blockquote><p>Motivating example: resolve IP addresses to countries from 1 terabyte access log for your website</p></blockquote><p>Idea: map-side join&ndash;distribute the database to every mapper and query it locally.</p><p>Distributing the database via a broadcast variable, we take slightly more than 1 gigabyte of outgoing traffic at the driver node</p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-advanced_topics/br1_hu1303ca721a17ed85cc0e04e82c2c20d6_166329_600x0_resize_box_3.png width=600 height=249><figcaption><small></small></figcaption></figure><blockquote><p>Motivating example:</p></blockquote><ol><li>setup a transformation graph to compute a dictionary</li><li>invoke the <em>collect</em> action to load it into the driver&rsquo;s memory</li><li>put it into the broadcast variable to use in further computations.</li></ol><p>Idea: upload computations to spark executors and use the driver program as the coordinator.</p><h2 id=summary-2>Summary<a hidden class=anchor aria-hidden=true href=#summary-2>#</a></h2><ul><li>Broadcast variables are read-only shared variables with effective sharing mechanism</li><li>Useful to share dictionaries, models</li></ul><h1 id=accumulator-variable>Accumulator Variable<a hidden class=anchor aria-hidden=true href=#accumulator-variable>#</a></h1><p><strong>Accumulator variable</strong> is a read-write variable that is shared among tasks</p><ul><li>Writes are restricted to increments!</li><li>i. e.: var += delta</li><li>addition may be replaced by any associate, commutative operation</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Restricting the right operations allows the framework to avoid complex synchronization thus making the accumulators efficient. 
</span></span></code></pre></div><ul><li>Accumulator variable could be read only by the <em><strong>driver</strong></em> program and not by the executors.</li><li>cannot read the accumulated value from within a task</li></ul><h2 id=example-3>Example<a hidden class=anchor aria-hidden=true href=#example-3>#</a></h2><p><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-advanced_topics/acc1_hu2b51b603049751d141e546dcb8676888_100224_500x0_resize_box_3.png width=500 height=213><figcaption><small></small></figcaption></figure><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-advanced_topics/acc2_hu1bf45cff6dbabad87bb05ed2db979642_55343_500x0_resize_box_3.png width=500 height=365><figcaption><small></small></figcaption></figure><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-advanced_topics/acc3_hu2d817ae87fcdcc3fccd851a41d32b1ba_53723_500x0_resize_box_3.png width=500 height=361><figcaption><small></small></figcaption></figure><figure><img style=max-width:100%;width:auto;height:auto src=/posts/apache_spark-advanced_topics/acc4_hu950a7f1963ee146b267fa98e04abe80f_49023_500x0_resize_box_3.png width=500 height=384><figcaption><small></small></figcaption></figure></p><h2 id=guarantees-on-the-updates>Guarantees on the updates<a hidden class=anchor aria-hidden=true href=#guarantees-on-the-updates>#</a></h2><ul><li>Updates generated in actions: guaranteed to be applied only once to the accumulator.</li><li>This is because successful actions are never re-executed and Spark can conditionally apply the update.</li><li>Updates generated in transformations: no guarantees when they accumulate updates. - - Transformations can be recomputed on a failure, on the memory pressure, or in another unspecified codes like a preemption.</li><li>Spark provides no guarantees on how many times transformation code maybe re-executed.</li></ul><h2 id=use-cases>Use cases<a hidden class=anchor aria-hidden=true href=#use-cases>#</a></h2><ol><li>Performance counters</li></ol><ul><li>number of processed records, total elapsed time, total error and so on and so forth</li></ul><ol start=2><li>Simple control flow</li></ol><ul><li>conditionals: stop on reaching a threshold for corrupted records</li><li>loops: decide whether to run the next iteration of an algorithm or not</li></ul><ol start=3><li>Monitoring</li></ol><ul><li>export values to the monitoring system</li></ul><ol start=4><li>Profiling & debugging</li></ol><h2 id=summary-3>Summary<a hidden class=anchor aria-hidden=true href=#summary-3>#</a></h2><ul><li>Accumulators are shared read-write variables with de-coupled read and write sides</li><li>could be updated from actions and transformations by using an increment.</li><li>can use custom associative, commutative operation for the updates</li><li>can read the total value only in the driver</li><li>Useful for the control flow, monitoring, profiling & debugging</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://nancyyanyu.github.io/tags/spark/>Spark</a></li></ul><nav class=paginav><a class=prev href=https://nancyyanyu.github.io/posts/ml-potential_problems/><span class=title>« Prev</span><br><span>Study Note: Linear Regression Part II - Potential Problems</span></a>
<a class=next href=https://nancyyanyu.github.io/posts/apache_spark-basic_concepts/><span class=title>Next »</span><br><span>Apache Spark: Basic Concepts</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark: Advanced Topics on twitter" href="https://twitter.com/intent/tweet/?text=Apache%20Spark%3a%20Advanced%20Topics&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-advanced_topics%2f&amp;hashtags=Spark"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark: Advanced Topics on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-advanced_topics%2f&amp;title=Apache%20Spark%3a%20Advanced%20Topics&amp;summary=Apache%20Spark%3a%20Advanced%20Topics&amp;source=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-advanced_topics%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark: Advanced Topics on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-advanced_topics%2f&title=Apache%20Spark%3a%20Advanced%20Topics"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark: Advanced Topics on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-advanced_topics%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark: Advanced Topics on whatsapp" href="https://api.whatsapp.com/send?text=Apache%20Spark%3a%20Advanced%20Topics%20-%20https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-advanced_topics%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark: Advanced Topics on telegram" href="https://telegram.me/share/url?text=Apache%20Spark%3a%20Advanced%20Topics&amp;url=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-advanced_topics%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark: Advanced Topics on ycombinator" href="https://news.ycombinator.com/submitlink?t=Apache%20Spark%3a%20Advanced%20Topics&u=https%3a%2f%2fnancyyanyu.github.io%2fposts%2fapache_spark-advanced_topics%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://nancyyanyu.github.io/>Nancy's Notebook</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>