<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Paper on Nancy&#39;s Notebook</title>
    <link>https://nancyyanyu.github.io/tags/paper/</link>
    <description>Recent content in Paper on Nancy&#39;s Notebook</description>
    <image>
      <title>Nancy&#39;s Notebook</title>
      <url>https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 15 Aug 2023 22:36:17 +0000</lastBuildDate><atom:link href="https://nancyyanyu.github.io/tags/paper/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Paper Note: CLIP</title>
      <link>https://nancyyanyu.github.io/posts/paper-clip/</link>
      <pubDate>Tue, 15 Aug 2023 22:36:17 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/paper-clip/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;The simple pre-training task of &lt;strong&gt;predicting which caption goes with which image&lt;/strong&gt; is an efficient and scalable way to learn SOTA image representations from scratch. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling &lt;strong&gt;zero-shot transfer&lt;/strong&gt; of the model to downstream tasks.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper Note: Swin Transformer</title>
      <link>https://nancyyanyu.github.io/posts/paper-swintransformer/</link>
      <pubDate>Tue, 15 Aug 2023 22:20:38 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/paper-swintransformer/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;A new ViT &lt;em&gt;whose representation is computed with &lt;strong&gt;S&lt;/strong&gt;hifted &lt;strong&gt;win&lt;/strong&gt;dows&lt;/em&gt;*!***&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper Note: Masked autoencoders(MAE) (very short)</title>
      <link>https://nancyyanyu.github.io/posts/paper-mae/</link>
      <pubDate>Tue, 15 Aug 2023 22:19:31 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/paper-mae/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Masked autoencoders (MAE)&lt;/strong&gt; are scalable &lt;strong&gt;self-supervised&lt;/strong&gt; learners for &lt;strong&gt;computer vision&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper Note: ViT</title>
      <link>https://nancyyanyu.github.io/posts/paper-vit/</link>
      <pubDate>Tue, 15 Aug 2023 22:08:51 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/paper-vit/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;ViT &lt;em&gt;&lt;strong&gt;applies a standard Transformer directly to images&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper Note: Attention is All You Need</title>
      <link>https://nancyyanyu.github.io/posts/paper-transformer/</link>
      <pubDate>Tue, 15 Aug 2023 21:54:35 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/paper-transformer/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;The Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Paper Note: BERT</title>
      <link>https://nancyyanyu.github.io/posts/paper-bert/</link>
      <pubDate>Tue, 15 Aug 2023 21:39:56 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/paper-bert/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;BERT is designed to &lt;strong&gt;pre-train deep bidirectional representations from unlabeled text&lt;/strong&gt; by &lt;strong&gt;jointly conditioning on both left and right context in all layers&lt;/strong&gt;. As a result, the &lt;strong&gt;pre-trained BERT model&lt;/strong&gt; can be &lt;strong&gt;fine-tuned with just one additional output layer&lt;/strong&gt; to create state-of-the-art models for a wide range of tasks.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
  </channel>
</rss>
