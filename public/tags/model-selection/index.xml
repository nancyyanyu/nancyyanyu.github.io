<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Model Selection on Nancy&#39;s Notebook</title>
    <link>https://nancyyanyu.github.io/tags/model-selection/</link>
    <description>Recent content in Model Selection on Nancy&#39;s Notebook</description>
    <image>
      <title>Nancy&#39;s Notebook</title>
      <url>https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://nancyyanyu.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language><atom:link href="https://nancyyanyu.github.io/tags/model-selection/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Study Note: Bias, Variance and Model Complexity</title>
      <link>https://nancyyanyu.github.io/posts/ml-bias-variance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/ml-bias-variance/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Study Note: Dimension Reduction - PCA, PCR</title>
      <link>https://nancyyanyu.github.io/posts/ml-dimension_reduction-pca/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/ml-dimension_reduction-pca/</guid>
      <description>&lt;h1 id=&#34;dimension-reduction-methods&#34;&gt;Dimension Reduction Methods&lt;/h1&gt;
&lt;p&gt;Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.&lt;/p&gt;
&lt;p&gt;Dimension Reduction Methods &lt;em&gt;&lt;strong&gt;transform&lt;/strong&gt;&lt;/em&gt; the predictors and then fit a least
squares model using the transformed variables.&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;p&gt;Let $Z_1,Z_2, . . . ,Z_M$ represent $M &amp;lt; p$ linear combinations of our original $p$ predictors. That is,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
Z_m=\sum_{j=1}^p\phi_{jm}X_j
\end{align}
$$&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Study Note: Model Selection and Regularization (Ridge &amp; Lasso)</title>
      <link>https://nancyyanyu.github.io/posts/ml-model-selection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/ml-model-selection/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Subset Selection/Adjusted $R^2$/Ridge/Lasso/SVD&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Study Note: Resampling Methods - Cross Validation, Bootstrap</title>
      <link>https://nancyyanyu.github.io/posts/ml-resampling-methods-cross-validation-bootstrap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nancyyanyu.github.io/posts/ml-resampling-methods-cross-validation-bootstrap/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Resampling methods&lt;/strong&gt;:involve repeatedly drawing samples from a training set and refitting a mode of interest on each sample in order to obtain additional information about the fitted model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;model assessment&lt;/strong&gt;： The process of evaluating a model’s performance&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;model selection&lt;/strong&gt;：The process of selecting the proper level of flexibility for a model&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;cross-validation&lt;/strong&gt;: can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;bootstrap&lt;/strong&gt;:provide a measure of accuracy of a parameter estimate or of a given selection statistical learning method.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
